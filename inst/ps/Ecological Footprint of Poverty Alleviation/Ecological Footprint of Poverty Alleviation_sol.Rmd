# Problem Set Ecological Footprint of Poverty Alleviation

Author:  Katharina Kaufmann  
Date:    30.10.2016

#< ignore
Adapt the working directory below and press Ctrl-Alt-R (run all chunks). This creates the problem set files and the sample solution from this solution file.

```{r "create_ps"}
library(RTutor)
library(yaml)
setwd("~/Studium_Master/MASTERARBEIT/Arbeit/R/Problemsets/RTutorEcologicalFootprintOfPovertyAlleviationPS2")
ps.name = "Ecological Footprint of Poverty Alleviation"; sol.file = paste0(ps.name,"_sol.Rmd")
libs = c("car", "ggplot2", "dplyr", "dplyrExtras", "deldir", "gridExtra", "regtools", "stargazer", "lmtest", "AER", "lfe", "dplyrExtras", "yaml") # character vector of all packages you load in the problem set
create.ps(sol.file=sol.file, ps.name=ps.name, user.name=NULL,libs=libs, stop.when.finished=FALSE, var.txt.file = "variable_names.txt", extra.code.file="functions_own.r", addons="quiz")
show.shiny.ps(ps.name, load.sav=FALSE,  sample.solution=FALSE, is.solved=FALSE, catch.errors=TRUE, launch.browser=TRUE)
stop.without.error()
```
#>
## Exercise Overview

Welcome to the problem set **"Ecological Footprint of Poverty Alleviation"** which will take you on a journey through the economic paper "The Ecological Footprint of Poverty Alleviation: Evidence from Mexico's Oportunidades Program" by J. Alix-Garcia et al. (2013). You will get to know the economic findings along with the according analytic steps, as well as explanations of the economic theory behind it and useful R commands in this context - all in an interactive way.

The intensity of this journey is on you and of course it can be adapted to the level of your prior knowledge: There are *'info' boxes* explaining economic theory or R commands more deeply, giving background information or just comments and there is also one *optional exercises* marked by square brackets in the according tab. From time to time you will find a *quiz* to check your knowledge.

### What is this problem set about?
The paper studies the effect of the Mexican poverty alleviation program **"Oportunidades"** on deforestation. Apart from the **correlation between treatment with the program and deforestation**, the authors also consider **changes in the production and consumption behavior** as probable explanation of the effect on the household level as well as the **mediation of the effect through infrastructure**.  
In the *Exercises 2* and *3* of this problem set you will reproduce the results step by step. The first exercise will help you to get familiar with the contents and the data and the last exercise provides you with a summary and conclusion.  
The structure is therefore as follows:

**1. Introduction**  
The first exercise gives background information about the **"Oportunidades"**" program and deforestation and makes you familiar with the first data set.  
**2. Treatment effect**  
The central question in the second exercise is: "Is there a correlation or causal effect between the participation in the program and deforestation?". The Tobit model and the concept of the RD design are introduced here.  
**3. Household level explanation & Role of infrastructure quality**  
The third exercise deals with a possible explanation of the findings in *Exercise 2* studying the changes in consumption and production behavior of households and makes use of a second data set. Here, the difference-in-difference estimator is used.  
As soon as treated localities are linked with other localities through roads, increases in consumption can be sourced from neighboring localities and lead to environmental impacts there. The exercise thus also studies the extent to which the effect is mediated and draws conclusions about the measurability of the treatment effect. 
*Unfortunately, we cannot reproduce the results of the spatial contours of the effect itself due to a lack of data. But I will sum up the authors' results on this to give you a complete picture of their arguing and final interpretation.*  
**4. Summary and conclusion:**  
The last exercise will sum up the results, draw conclusions from them and give an outlook on what could be studied in the future.


### How to proceed:
A code chunk is always solved in the same way. Just click on the *'edit' button*, type in your answer and click on the *'check' button* to run the code and have your solution checked. If you need help, feel free to use the *'hint' button*. In case you are only interested in the economic content, you can also directly use the *'solution' button* which shows you the sample solution and the *'check' button* to run the code and see the results.  
The exercises can be solved independently from each other. Nevertheless, I recommend working through the problem set in the given order.  
If you want to have a look at a data frame, you can use the *Data Explorer* where you can also get by clicking on the *'data' button* above a code chunk.


### ...If you are mainly interested in learning econometric tools and associated R commands...
Here is a short overview on the major topics treated and where to find them in this problem set:
* **Regression Discontinuity (RD) Design:** This tool allows to measure the effect of treatment with a program for people with very similar and only continuously changing characteristics. The central condition for being able to apply this method is that the data used shows a discontinuity in treatment or in the probability of treatment. The RD Design plays an essential role in the paper and you will use it both in *Exercises 2* and *3*. The method is introduced to you in the *Exercises 2.3, 2.5* and *2.6*.  
* **Censored regression model (Tobit):** This model should be used if the values of a dependent variable in a regression are limited, because the usual OLS regression would be biased in this case. It is introduced in *Exercise 2.1* and used in both *Exercises 2* and *3* because the variable describing the percentage of an area deforested cannot be negative.  
* **Difference-in-difference estimator:** This estimator can be used to measure a treatment effect, if the data consists of values for two points in time and for two groups: the treatment and the control group. In this paper, it is used for the analysis of the changes in production and consumption behavior due to treatment. This estimator is introduced in *Exercise 3.2* and used only in *Exercise 3* of this problem set.

## Exercise 1.1 -- "Oportunidades" - a conditional cash transfer program

**"Oportunidades"** is a Mexican poverty alleviation program which started in 1995 and still exists today - even though its name has changed several times until now. At the beginning it was called **"Progresa"** which is an abbreviation for its Spanish name meaning *"program for education, health and nutrition"*. It changed its name to **"Oportunidades"** (in English "chances" or "opportunities") in the course of a new government in 2000. In the paper, the authors work on data from the start of the program until 2002, so we will focus on the beginning of the program keeping in mind that there were some changes in the methodology after 2000.  
The original idea was to provide monthly cash transfers to poor families conditioned on the school attendance of their children and regular health checkups. This way the program should help to improve both education and health. One monthly payment was about one fifth of a family's consumption costs prior to the start of the program and all payments in a year increased a family's income on average by about one third. As it was quite successful, it has served as an example for many other poverty alleviation programs.  
Originally, only very poor rural households could participate in the program - after 2000, urban households could be treated as well. Apart from that, the selection into eligible and noneligible households was realized in two steps:

**1. Choice of eligible localities:** A locality is eligible, if its marginality index describing the degree of poverty exceeds a certain value.  
**2. Choice of eligible households:** A survey among the households in an eligible locality serves to detect the households "poor enough" to be eligible.

*(Cf. paper; Skoufias (2005); website of the program* <https://www.prospera.gob.mx/swb/swb/PROSPERA2015/> *)*


Throughout this problem set we will use two data sets: The first one contains data on the program and on deforestation on the locality level; The second one contains data from the test phase of the program which allows to study the consumption and production behavior on the household level for a restricted number of localities. We will focus on the first data set in this exercise, leaving the other one to *Exercise 3*.

### a) Overview: Data on the locality level
In the following code chunk read in the data set "data_deforestation.rds" using the command *readRDS("filename")* with *"filename"* being the name of the file in quotation marks and save it in the variable *data_deforest*.
  
```{r "1 a) 1"}
#< task
#Read in the data and save it as data_deforest
#>
data_deforest=readRDS("data_deforestation.rds")
#< hint
display("Saving a data set D in a variable x is done by the command 'x = D'.", "For all other needed commands see description of the exercise - and don't forget the quotation marks for the argument of 'readRDS'.", sep = "\n")
#>

```
The authors used the following external data to create the data set:  
* Information on when which locality was enrolled (until the end of 2003) was taken from the "Oportunidades office" *(paper, p.421)*.  
* The spatial coordinates as well as the marginality index from 1995 and the population number from the same year for every locality were sourced from the National Institute of Geography and Statistics in Mexico **INEGI** which is part of the national council of population **CONAPO**.  
* Finally, data on deforestation for each locality were taken from the Mexican National Forestry Commission **CONAFOR**. To detect villages with ten or more hectares of forested land and exclude them from the data set and thus from the analysis, the data from the National Forest Inventory **INF** in 2000 was used.

*(Cf. paper, section III.B)*

If you are interested in the institutions and the National Forest Inventory, have a look at the websites given in the *'info' box*:  
#< info "Links to interesting websites"
**INEGI:** <http://www.inegi.org.mx/>  
**CONAPO:** <http://www.gob.mx/conapo>  
**CONAFOR:** <http://www.conafor.gob.mx/web/>  
Some introductory information about the **National Forest Inventory** is given here: <http://aplicaciones.semarnat.gob.mx/estadisticas/compendio2010/10.100.13.5_8080/ibi_apps/WFServlet3652.html>  
Please note that all websites are only available in Spanish.
#>
To get an idea of the data set, have a look at the first six rows of the data frame. You can do so by using the command *head(x,n)* showing the first *n* rows of *x*. Please note that the default argument *n* is already six.

```{r "1 a) 2", fig.height=3}
#< task
#Show the first six rows of data_deforest
#>
head(data_deforest)
```

#< info "Variable descriptions"
The most important variables of the current data set will be introduced in this exercise and in the following one. If you want to look up the meaning of a variable, you can always click on one of the buttons *'Data Explorer'* and *'data'*. In the tab *Description* you can then find descriptions of the basic variables. In the tab *Data* the whole data set is provided.
#>

Each observation in the data set corresponds to one community with all the observations restricted to rural communities (at most $2500$ inhabitants) with a marginality index from 1995 smaller than $3.0$ and at least ten hectares of forest in 2000. *(Cf. paper, pp.420-421; data set)* 
You can find the following information in this data set:

* **Characteristics of the communities:** e.g. population numbers (*lnpop*), infrastructure quality (*dens_road*)
* **Treatment with "Oportunidades":** e.g. marginality index (*pov_ind_95*) as determinant of eligibility (*eligible*), treatment (*treat*)
* **Variables on deforestation:** See *Exercise 1.2*

#< info "Comment on this data set"
Please note that this data ("data_deforestation.rds") set deviates from the original data set provided by the authors ("ecological_footprint_restat_deforestation_data.dta") in two aspects: First, the column describing the **percentage of area deforested** was **multiplied by** $100$ for the numbers to be interpreted in percentage and not as simple proportion. This makes the results consistent with those printed in the paper. Second, the **variable names** were changed into **more intuitive** ones using abbreviations based on the English language.
#>
### b) Poverty, eligibility and treatment
The **marginality index** plays a very important role in this analysis. It is the base for determining the eligibility of a locality for the program and an important factor in the question if a locality was treated or not. It was computed considering aspects like illiteracy, access to running water, dwelling and electricity, number of people living in a room and proportion of people working in the agricultural sector.  
The **poorer** a community, the **higher** its marginality index. One number to remember is the value $-1.22$: It is the cutoff between low and medium levels of poverty and at the same time the cutoff for being eligible for "Oportunidades".  
*(Cf. Hyun Hwa Son (2011, p.95) and paper, pp.420,422,423)*
#< info "Remark on eligibility"
Please note that in *Exercise 1* and *Exercise 2* the term "eligibility" always refers to the eligibility of a village. This is because in these exercises, the data set containing data on the locality level is used.
#>
The following code chunk creates a blue histogram with bin width $0.1$ and with one of the bin centers at $-1.17$ such that no bin is placed across the eligibility cutoff of $-1.22$. *More details on why this is important will be given in Exercise 2.* This histogram shows the density of the marginality index *pov_ind_95* and plots a red vertical line at the eligibility cutoff. Just click on the *'check' button* to run the code. The *'info' box* below gives you a short introduction to the package *'ggplot2'* used for this plot. The command *library(name of package)* loads a package.

```{r "1.1 b) 1", fig.height=3}
#< task
#Load the package 'ggplot2'
library(ggplot2)
#Define the data set used for this plot and its aesthetics
#then plot a histogram and a red vertical line
hist_pov_ind = ggplot(data_deforest, aes(x=pov_ind_95)) + 
  geom_histogram(binwidth=0.1, center=-1.17, aes(y=..density..), col="blue") +
  geom_vline(xintercept = -1.22, col="red")
#Show the plot
hist_pov_ind
#>
```

#< info "Plotting with ggplot2"
The package *'ggplot2'* is a very powerful R tool as it offers a variety of plotting options. The basic idea is to create the final plot by defining the components of the plot and fitting them together with a simple *+*. The first component always initializes the plot and defines the data frame to use for the plot as well as the aesthetics of the plot: *ggplot(data, aes(x=..., y=...))* with *x* and *y* the variables in the data frame *data* which are to be plotted. Further components can draw points, lines, regression fits etc. Usually those commands start by *geom_* , going on with an intuitive name like *histogram* for a histogram, *point* for points or *vline* for a vertical line.
#>

As you can see from the histogram, the marginality index is roughly normally distributed among the localities in the data set. This means that the majority of the localities shown here were at least of medium levels of poverty and therefore eligible for "Oportunidades".

If a locality was actually **treated** or not is not solely determined by eligibility.  
To visualize this, we will compute the mean of the treatment dummy *treat* for bins of the marginality index *pov_ind_95* of width $0.1$ - which is nothing else than computing the proportion of localities treated in a bin - and plot this over the marginality index. *Note that as this data set is quite large, the proportion treated can be regarded as an approximation of the probability of treatment.* As eligibility jumps from $0$ to $1$ at a marginality index of $-1.22$, we should choose the bins such that there is no bin across the cutoff. *(See Exercise 2.3)* For a comment on how this is implemented see the following *'info' box*.  
#< info "Creating the bins of the plot - and shifting them"
Each observation is assigned a bin of the marginality index in order to define over which observations averages are to be computed. A very easy way to define such bins is to round the marginality index to a certain number $n$ of digits. The width of the bin is then given by $10^{-n}$. Rounding can be done with the command *round(x, digits)* in R where *digits* can also be a negative value.  
The simple rounding strategy creates bins which contain the values between numbers with the last digit being a $5$. In order to get bins such that one bin starts at a marginality index of $-1.22$ and ends at $-0.72$, we first subtract $0.03$ from the marginality index and then round it to $1$ digit. We can then use this auxiliary variable to determine over which observations the average of *treat* has to be calculated. In the final plot, these average values are plotted over the "true bins" - the auxiliary bins increased by $0.03$.

*Of course, this procedure can be applied to any pair of variables $x$ and $y$ instead of the marginality index and the dummy variable indicating treatment. The shifting strategy can be adapted to arbitrary starting points and different numbers of digits.*
#>
The data preparation is already given in the next two code chunks. For information on the used commands *mutate*, *group_by* and *summarize* have a look at the *'info' box* below. At first, the column *aux_bins_povind* is added to the existing data frame indicating the according auxiliary bins of the marginality index. Click on the *'check' button* to run the code. Then have a look at the shown six rows of the changed data set to see what *mutate* is actually doing.

```{r "1.1 b) 2"}
#< task
#Load the package 'dplyr'
library(dplyr)
#Add the column aux_bins_povind
data_deforest = mutate(data_deforest, 
                       aux_bins_povind = round(pov_ind_95 - 0.03, digits = 1))
#Show the first six rows of data_deforest
head(data_deforest)
#>
```

Further, the mean of the variable *treat* for each value of the variable *aux_bins_povind* - so for each auxiliary bin - is calculated and called *prop_treated*. This is done by the command *summarize* in combination with *group_by* which finally returns a data frame containing only the grouping variable *bins_povind* and the newly calculated column *prop_treated*. After having run the code, have a look at the shown first six rows of the data frame to get an idea about what the command has done.

```{r "1.1 b) 3"}
#< task
#Add the column prop_treated to data_deforest calculating the mean of treat
#for each value of aux_bins_povind
dat = summarize(group_by(data_deforest, aux_bins_povind), 
                prop_treated = mean(treat))
#Show the first six rows of dat
head(dat)
#>
```
#< info "Data manipulation with 'dplyr' - mutate, summarize and group_by"
The package *'dplyr'* is a very useful package for easy and quick data preparation in R. Throughout this problem set you will get to know several commands out of it.
* The command *mutate(data=..., column=...)* returns the original data frame with a new column added or an existing column changed - depending on the column indicated as argument.
* The command *summarize(data=..., column=...)* does the same as *mutate* with the difference that it only returns the newly computed values. If combined with *group_by* the resulting data frame contains as many elements as there are unique values of the variable used for grouping and also shows this grouping variable.
* The command *group_by(data=..., variable=...)* "groups" the data according to the variable given as argument such that the surrounding command - in this case *mutate()* is computed for all observations with the same value of this variable.
#>

Now it is your turn to create a plot following the example before. It shall show the proportion of localities treated in each bin of the marginality index as blue points. Thus you should use the data set *dat*, the aesthetics *aux_bins_povind + 0.03* (as $x$ variable, *for more information have a look at the 'info' box above*) and *prop_treated* (as $y$ variable) and add a graph of blue points. Furthermore, a red vertical line shall be added at the intercept $-1.22$. The command for the vertical line is *geom_vline(xintercept = ...)* with *xintercept* the x axis intercept. The empty commands as well as the labeling of the x axis with the command *xlab("name")* are already given. Just fill in the missing parts and uncomment the code.  
*This corresponds to figure 3 on p.422 in the paper.*
```{r "1.1 b) 4", fig.height=6}
#< task
#Create the plot "prop_treated over bins_povind" and save it in p_proptreated
#p_proptreated = ggplot(data=..., aes(...)) +
#  geom_point(col=...) +
#  geom_vline(xintercept=..., col=...) +
#  xlab("bins marginality index")
#>
#Create the plot "prop_treated over bins_povind" and save it in p_proptreated
p_proptreated = ggplot(data=dat, aes(x=aux_bins_povind +0.03, y=prop_treated))+
  geom_point(col="blue") +
  geom_vline(xintercept = -1.22, col="red") +
  xlab("bins marginality index")

```
Now show the plot by just showing the variable *p_proptreated*.
```{r "1.1 b) 5", fig.height=6}
#< task
#Show p_proptreated
#>
p_proptreated
```
The red line visualizes the eligibility cutoff: All localities on the right side of the cutoff (marginality index greater than $-1.22$) are eligible, the others aren't.  
If eligibility was the only determinant of treatment, we would expect the proportion of localities treated to jump from $0$ to $1$ at the cutoff of $-1.22$. Actually, it starts to increase quickly at $-1.22$, but not from $0$ to $1$ at once. We will discuss this issue and its implications in *Exercise 2*.

## Exercise 1.2 -- Deforestation at the time of the program

In order to measure the impact of treatment on deforestation, Alix-Garcia et al. use data from **CONAFOR** on deforestation between 2000 and 2003 and match the deforestation numbers to the localities described in *Exercise 1.1*. As the localities originally were only determined by spatial coordinates, a special method is needed to assign a piece of land to each locality. *(Cf. paper, section III.B)* As a consequence, further information on ecological and geographical aspects possibly affecting deforestation can be added as well.  
If you are interested in the scientific background of how to **measure deforestation**, have a look at the *'info' box* below.

#< info "Measurement of deforestation"
The most common measurement of vegetation is the so-called *Normalized Difference Vegetation Index* (NDVI). It relies on the fact that green leaves absorb visible light and reflect near-infrared light. Sensors on satellites can measure the intensity of radiation of different wavelengths. The NDVI is then calculated as 

$$NDVI = \dfrac{NIR - VR}{NIR + VR}$$
              
where *NIR* and *VR* stand for the proportion of near-infrared light and visible light measured respectively.  
The higher the index, the greater the proportion of near-infrared light reflected compared to the visible light and though the higher the density of green leaves. If this information on the vegetation is collected at two points in time, one can determine locations of deforestation by comparing the two resulting pictures.  
*(Cf. http://earthobservatory.nasa.gov/Features/MeasuringVegetation/measuring_vegetation_1.php)*
#>


### a) Matching deforestation numbers to localities
The information on deforestation was originally available for the whole country of Mexico - but the selected localities in the data set used by the authors only are assigned spatial coordinate points known from the 1995 census. To match areas and localities unambiguously and completely, the authors used **Thiessen polygons**. *(Cf. paper, section III.B)* These polygons assign each point on the map to the locality closest to it. You can find some background information on Thiessen polygons in *Rhynsburger (1973)*.  
To give you an idea on what this looks like, we will do the same with some imaginary points. In R, this can be done with the package *'deldir'*. One possible input is a data frame of the two-dimensional coordinates named *x* and *y*. For this purpose, create ten random numbers drawn from a uniform distribution on [0,30] for the values of *x* and *y* each. This can be done with the command *runif(n=..., min=..., max=...)* with *n* the number of drawings, *min* the lower bound and *max* the upper bound.

```{r "1.2 a) 1", fig.height=3}
#< task
set.seed(12345)
#Draw ten random numbers from the uniform distribution on [0,30]
#and assign them to x
#Show x
#>
x = runif(10, 0, 30)
x
#< task
set.seed(23456)
#Draw ten random numbers from the uniform distribution on [0,30]
#and assign them to y
#Show y
#>
y = runif(10, 0, 30)
y
```

The following code chunk creates a data frame with the coordinates *x* and *y* and calculates and plots Thiessen polygons for the corresponding points. 

```{r "1.2 a) 2", fig.height=4}
#< task
#Create a data frame of the coordinates
input_deldir = data.frame(x=x, y=y)

#Load package 'deldir'
library(deldir)
#Calculate and plot Thiessen polygons
polygons = tile.list(deldir(input_deldir))
pol_colors = topo.colors(length(input_deldir$x)) 
plot(polygons, fillcol=pol_colors, close=TRUE)
#>
```

To return to the data set we are working with: Imagine the small circles to be the spatial coordinate points of the localities. The whole area then corresponds to the map with information on deforestation for each point on it. A colored polygon is just the area which is treated as belonging to the locality inside it.

### b) Statistics on deforestation
Let's have a look at the data on deforestation. Click on the *'check' button* to load the data set *"data_deforestation.rds"* you've already got to know in *Exercise 1.1*. If you want to have a look at it again, don't hesitate to use the *'Data Explorer'*.

```{r "1.2 b) 1"}
#< task
#Read in the data set and save it in the variable data_deforest
data_deforest = readRDS("data_deforestation.rds")
#>

```

The variable *pct_deforested* describes the percentage deforested between 2000 and 2003 of the polygon belonging to the locality. If it is positive, the dummy variable *d* equals $1$. The variable *forest_prior* describes the area within the polygon covered by forests in 2000. Unfortunately, I don't have information on the measuring unit of *forest_prior*.  
*(Cf. original data set "ecological_footprint_restat_deforestation_data.dta")*

In *Exercise 1.1* we plotted treatment over the marginality index. In the following, we will create a plot of deforestation over the marginality index. In general, there are two interesting variables to illustrate in this context:
* probability of deforestation
* percentage of area deforested  

Let's start with the illustration of the **probability of deforestation**.  
In *Exercise 1.1b* we divided the marginality index into bins and calculated the mean of the dummy variable *treat* for each bin. This way, we got the proportion of localities treated. In the following code chunks, we will do the same with the dummy variable *d* indicating positive deforestation. We will call the resulting variable *prop_posdefor* as it equals the proportion of localities with positive deforestation in a bin.   
The variable *aux_bins_povind* which we already know from *Exercise 1.1b* is created in the first code chunk. Just click on the *'check' button* to run the code which also shows the head of the changed data set.

```{r "1.2 b) 2"}
#< task
#Load package 'dplyr'
library(dplyr)
#Add the column aux_bins_povind to data_deforest
data_deforest = mutate(data_deforest, 
                       aux_bins_povind = round(pov_ind_95 - 0.03, digits = 1))
#Show the first six rows of data_deforest
head(data_deforest)
#>

```

Now use the command *summarize(group_by())* from the package *'dplyr'* to create a data frame with two columns: One containing the grouping variable *aux_bins_povind* and one containing the mean of the variable *d* for all localities with the same value of *aux_bins_povind*. If you need some help for this task, have a look at *Exercise 1.1b* again. Call the newly calculated variable *prop_posdefor* and the summarized data set *dat_d*. Finally, show the first six rows of the data set.

```{r "1.2 b) 3"}
#< task
#Calculate the mean of d for each value of aux_bins_povind
#>
dat_d = summarize(group_by(data_deforest, aux_bins_povind), 
                  prop_posdefor = mean(d))
#< task
#Show the first six rows of dat_d
#>
head(dat_d)
```

For bins with lots of observations *prop_posdefor* is approximately equal to the probability of deforestation. Use the package *'ggplot2'* to plot this variable as blue points over the bins of the marginality index *aux_bins_povind + 0.03* and add a red vertical line at the bin of the eligibility cutoff of $-1.22$. Add the label "bins marginality index" to the x axis. Call the plot *p_prop_posdefor*. Don't show the plot yet.
```{r "1.2 b) 4"}
#< task
#Load package 'ggplot2'
library(ggplot2)
#Create the plot "prop_posdefor over bins_povind"
#and save it in p_prop_posdefor
#>
p_prop_posdefor = ggplot(data=dat_d, 
                         aes(x=aux_bins_povind+0.03, y=prop_posdefor)) +
  geom_point(col="blue") + 
  geom_vline(xintercept = -1.22, col="red")+
  xlab("bins marginality index")

```

In a second step we consider the illustration of the **percentage of polygon deforested**.  
This works analogous to the illustration of the probability of deforestation: The variable of interest of which we will calculate the mean in each bin is now *pct_deforested* instead of *d*. To shorten it, the code for the respective data preparation and plotting is already given. Besides, the head of the data frame *dat_pct* is shown.  
*This corresponds to figure 3 on p.422 in the paper.*
```{r "1.2 b) 5"}
#< task
#Add the column mean_pctdefor to data_deforest calculating the mean of d
#for each value of aux_bins_povind
dat_pct = summarize(group_by(data_deforest, aux_bins_povind), 
                    mean_pctdefor = mean(pct_deforested))
#Show the first six rows of dat_pct
head(dat_pct)
#Create the plot "mean_pctdeforested over bins_povind"
#and save it in p_pctdefor
p_pctdefor = ggplot(data=dat_pct, 
                    aes(x=aux_bins_povind+0.03, y=mean_pctdefor)) +
  geom_point(col="darkgreen") + 
  geom_vline(xintercept = -1.22, col="red")+
  xlab("bins marginality index")
#>
```
To show multiple figures together one can use the command *grid.arrange(plot_1, plot_2, ..., plot_n)* from the package *'gridExtra'* with *plot_...* the plots which are to be shown. Use it to show *p_prop_posdefor* and *p_pctdefor*.
```{r "1.2 b) 6", fig.height=6}
#< task
#Load package 'gridExtra'
library(gridExtra)
#Show both p_prop_posdefor and p_pctdefor together
#>
grid.arrange(p_prop_posdefor, p_pctdefor)

```
Both the probability of deforestation (approximated by *prop_posdefor*) and the mean percentage of the area deforested per bin follow roughly an inverse-U relationship. This is consistent with the theory of the **Kuznets curve**. It says that deforestation is quite low in very poor populations, higher in less poor populations and lower again for even more wealthy populations. *(Cf. paper, p.423)*  
At first sight, one cannot detect any special behavior concerning the probability and the percentage of deforestation in the environment of the eligibility cutoff (red line). One needs to have a closer look at those graphics to draw conclusions from them. This is what we will do in *Exercise 2*.

## Exercise [Q1] -- QUIZ on Exercises 1.1 and 1.2
In the following quiz, you can check your knowledge of the contents and the R commands you have got to know in the *Exercises 1.1* and *1.2*. Just solve the multiple-choice quiz and click on the *'check' button* to see which of your answers are already correct. If there are some right answers missing, you can try again and check again... Have fun!

- - -
#< quiz "Test"
parts:
  - question: 1. Oportunidades is a conditional cash transfer program.
    choices:
        - correct*
        - wrong
    multiple: FALSE
    success: Great! This is correct! Conditional on regular health check\-ups and school attendance of their children the treated families receive money.
    failure: Try again. It might also help you to have a look at the introductory part of *Exercise 1.1* again.
  - question: 2. A locality is eligible for the program if its marginality index lies below some cutoff value because a lower marginality index indicates that the locality is poorer.
    choices:
        - correct
        - wrong*
    multiple: FALSE
    success: Great! This is correct! The statement above has to be negated because the higher the marginality index the poorer the locality is. Thus a locality is eligible if its marginality index lies below the cutoff.
    failure: Try again. It might also help you to have a look at *Exercise 1.1 b)* again.
  - question: 3. A locality is treated with the program if it is eligible.
    choices:
        - correct
        - wrong*
    multiple: FALSE
    success: Great! This is correct! Eligibility does not determine treatment perfectly.
    failure: Try again. It might also help you to have a look at *Exercise 1.1 b)* again.
  - question: 4. Both the histogram showing the proportion (probability) of deforestation in a bin of the marginality index and the histogram showing the average percentage of area deforested in a bin of the marginality index have the shape of an inverse U. This is consistent with a special curve. How is it called?
    answer: Kuznets curve
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 1.2 b)* again.
  - question: 5. Plotting in R can be done with the package 'ggplot2'. What's the command to initialize a plot? Please don't use brackets.
    answer: ggplot
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at the according *'info' box* in *Exercise 1.1 b)* again.
  - question: 6. After having initialized the plot, one can add various commands for plotting lines, points etc. What is the usual prefix of those commands?
    answer: geom_
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at the according *'info' box* in *Exercise 1.1 b)* again.
  - question: 7. The command *???(group_by(data, x), test = mean(x))* groups the data frame *data* according to the column *x*, calculates the mean of *x* for each group and returns the original data frame extended by the column *test* containing these values. Which command has to be inserted in the place of the question marks?
    choices:
        - mutate*
        - summarize
    multiple: FALSE
    success: Great! This is correct! The command *summarize* only returns the newly computed column (*test*) and the column which is used for grouping (*x*).
    failure: Try again. It might also help you to have a look at the according *'info' box* in *Exercise 1.1 b)* again.
#>

#< award "Proper start!"
You have understood the main points of the contents  
and already know some new R features.  
This is a good starting point for the first main part  
of your journey through this problem set: Exercise 2!
#>



## Exercise 2.1 -- OLS versus Tobit Regressions

The goal of *Exercise 2* is to find out **whether and how treatment with "Oportunidades" affects deforestation**. More precisely, two aspects of deforestation will be regarded: the area deforested (in % of the polygon size) and the probability of deforestation in the polygon.  
Note that due to the data used, *treatment* means being enrolled in the program between 1997 and 2003 and *deforestation* refers to the period between 2000 and 2003. *(Cf. paper, sections III.A, III.B, III.D)*  
In order to be able to work on the data, click on the *'check' button* to load the data set *"data_deforest.rds"* again.

```{r "2.1 1"}
#< task
#Load the data set "data_deforest.rds" and save it in data_deforest
data_deforest = readRDS("data_deforestation.rds")
#>
```

### a) Amount of deforestation - OLS approach
Let's start with the question how treatment with **"Oportunidades"** affects the **relative amount of deforestation**. The intuitive way to solve this question is to use simple OLS and regress the variable describing deforestation - in this case *pct_deforested* - on the dummy variable *treat* indicating treatment. You find an introduction to OLS regressions in the *'info' box* below.

#< info "Ordinary Least Squares"
**Ordinary least squares** is the classical method to estimate a model of the form
\[y = \vec x ' \vec \beta + \epsilon\]
with $y$ the *dependent variable*, $\vec x$ a vector of *independent/ explanatory variables* and $\epsilon$ the error term.  
For a given data set $\{\vec y, X\}$ with $X = \left(1,\vec{x_1},..., \vec{x_{k+1}}\right)$ and $1$ indicating a vector consisting of ones, the estimator $\vec{\hat{\beta}}_{OLS}$ minimizes the sum of squared residuals  
\[\vec{\hat{\beta}}_{OLS} = argmin \sum_{i=1}^N \hat{\epsilon}_i^2 = argmin \left\{\sum_{i=1}^N \left(y_i - \vec x_i * \vec\beta\right)^2\right\} = (X'X)^{-1} X'\vec y\]
The OLS estimator is **unbiased, consistent and BLUE (best linear unbiased estimator)**, if the following **assumptions** are fulfilled:  
**1.** **Linearity:** The dependent variable is a linear function of the explanatory variables and the error term.  
**2.** **Exogeneity:** $E[\epsilon_i | X] = 0 \thinspace \forall i$; Weak exogeneity is given if $Cor(x_j,\epsilon)=0 \thinspace \forall j$  
**3.** **Homoscedasticity, no autocorrelation:** $Var(\epsilon_i)=\sigma^2 \thinspace \forall i, \thinspace Cov(\epsilon_i,\epsilon_j)=0 \thinspace \forall i \neq j$  
**4.** **Generation of $X$:** The data in $X$ are either random variables or constants.  
**5.** **Full rank:** There are more observations than independent variables and the latter are not perfectly correlated.  
The model described by those five assumptions is called *Classical linear regression model*. If one assumption is added, it turns into the *Classical normal linear regression model*:  
**6.** **Normally distributed disturbances:** $\epsilon_i \sim \mathcal{N}(0,\sigma^2) \thinspace \forall i$  
If one or more assumptions are violated, other estimators might be better suited.  
*(Cf. Kennedy (2008, pp.40-44) and Greene (2008, p.11))*
#>

In R, linear regressions are calculated by the command *lm(formula, data)* with *formula* representing the regression equation. The code showing the regression results is already given. The according command *showreg* from the package *'regtools'* is introduced in the *'info' box*. Have a look at the following code chunk and run it.

#< info "Displaying regression results: showreg"
The command *showreg(list(models), custom.model.names, digits, ...)* from the package *regtools* displays the results of one or more regressions in the style of economic papers. One can define among other things the number of digits displayed (argument *digits*) or the kind of standard errors. For the latter just set the argument *robust* equal to *TRUE* and indicate the type of standard error in the argument *robust.type*.
#>

```{r "2.1 a) 1"}
#< task
#OLS regression of percentage of area deforested on treatment
ols_pct_treat = lm(pct_deforested ~ treat, data=data_deforest)
#Load package 'regtools'
library(regtools)
#Show regression results
showreg(list(ols_pct_treat), custom.model.names=c("% area deforested"), 
        digits=3, stars=c(0.01,0.05,0.1))
#>
```

#< info "General technical remark"
Please note that the significance levels are indicated in the following way throughout the whole problem set:  
* ***significant on $1\%$ level  
* ** significant on $5\%$ level  
* * significant on $10\%$ level

This is analogous to the way the authors denote the significance levels in their paper.
#>
According to this model, the OLS estimates indicate that without treatment, in expectation $0.152\%$ of the area is deforested. Treatment with Oportunidades decreases this level of deforestation by $0.022$ percentage points, so by about one seventh, on average. This is thus the **marginal effect of treatment on the conditional expectation of the dependent variable**. For an introduction to marginal effects have a look at the *'info' box*. Both coefficients are statistically significant.  
#< info "Marginal Effects"
In a simple OLS regression, the estimated marginal effect on $E\left[y|\vec{x}\right]$ just equals the corresponding coefficient estimate - both for continuous and binary variables.  
In general one has to distinguish between different types of variables $x$ in the calculation of the marginal effect of a variable $x$ on a quantity $\mu$: The marginal effect of a **continuous explanatory variable** $x_c$ on $\mu$ for example is calculated as the partial derivative of the quantity with respect to $x_c$: $$\frac{\partial{\mu}}{\partial{x_c}}$$
The marginal effect of a **binary variable** $x_b$ on $\mu$ can be obtained by calculating the difference between $\mu$ given $x_b=1$ and $\mu$ given $x_b=0$.  
What is the meaning of those marginal effects? In the second case, the marginal effect describes the change in $\mu$ if the dummy variable changes from $0$ to $1$. In the first case, the interpretation is less tangible. Approximately, it shows the change in $\mu$ if $x_c$ increases by one unit. This is because for very small changes in $x_c$ the change in the quantity is approximately given by the product of the marginal effect of $x_c$ on the quantity and the change in $x_c$: $\Delta \mu = \frac{\partial{\mu}}{\partial{x_c}}\cdot \Delta x_c$  
*(Cf. Wooldridge (2010, Chapter 2.2.2))*
#>
Nevertheless, the model used is not well suited: $R^2 = 0$ means that the variation of $y$ is not explained at all by the variation of *treat*. Furthermore, it is unclear whether the estimate of the coefficient of *treat* describes a **causal effect**. In order to make sure that this is plausible one has to check whether the assumptions are fulfilled.  
At first sight, one problem becomes very clear: In the model described by the OLS assumptions the dependent variable $y$ can take on every real number value - be it a positive or a negative one. In the following code chunk, calculate the minimum and maximum of the dependent variable *pct_deforested*.

```{r "2.1 a)2"}
#< task
#Calculate the minimum and the maximum of pct_deforested
#and call them min_defor and max_defor, respectively
#>
min_defor = min(data_deforest$pct_deforested)
max_defor = max(data_deforest$pct_deforested)
#< task
#Show both values
#>
min_defor
max_defor
```

As you can see, the variable *pct_deforested* measuring the proportion of the area deforested does not show values below zero. The reason for this is that the data only contains information on "areas of new deforestation" *(paper, p.422)* and not on afforestation. This does not suit the classical linear regression model. In general, if the dependent variable is limited, OLS estimates are usually (also asymptotically) biased. *(Cf. Kennedy (2008, p.262))* The authors speak of a "potential censoring problem" *(paper, p.422)*
So we need a different approach to measure the effect of treatment on the percentage polygon deforested.

## b) Probability of deforestation - OLS approach
What about the probability of deforestation? In order to find out how this quantity changes with treatment, a very common approach is to consider a **Linear Probability Model (LPM)** which is to be estimated then by *OLS*. If you are not used to this model, have a look at the *'info' box* below.
#< info "Linear Probability Model"
If the dependent variable $y$ can only take on two values - namely $1$, if some condition is fulfilled, and $0$, else - it is a so-called binary response. Statistically, this means that $y$ is Bernoulli distributed conditioned on a vector of explanatory variables $\vec x$ possibly influencing the value the variable $y$ takes on. Given such a variable $y$ and the according $\vec x$, the Linear Probability Model is defined as follows:
$$P(y=1|\vec x) = \beta_0 + \beta_1 * x_1 + ... + \beta_N * x_N$$
Under the assumption of a random sample, this model can be estimated by OLS. This becomes clear regarding the conditional expectation of the Bernoulli distributed random variable *y* on $\vec x$:
$$E[y|\vec x] = P(y=1|\vec x) * 1 + P(y=0|\vec x) * 0 = \beta_0 + \beta_1 * x_1 + ... + \beta_N * x_N$$
The variance is given by
$$Var(y|\vec x) = P(y=1|\vec x) * (1-P(y=1|\vec x)) = \vec x * \vec \beta (1 - \vec x * \vec \beta)$$
As this conditional variance changes for different values of x, there is heteroskedasticity and one should use robust standard errors.  
*(Cf. Wooldridge (2010, chapters 15.1, 15.2))*
#>
In our case, the dependent variable is the dummy variable *d* indicating if there was positive deforestation between 2000 and 2003. *(Cf. original data set "ecological_footprint_restat_deforetation_data.dta") For the beginning we consider the dummy variable *treat* as sole explanatory variable.  
In the following code chunk, we are going to estimate the LPM of *d* on the dummy variable *treat* by OLS. As we need to account for heteroskedasticity, we have to use robust standard errors. Alix-Garcia et al. use the kind of robust standard errors calculated by Stata: Huber-White standard errors (HC1). *(For further information on this, see the according help pages of STATA: <http://www.stata.com/manuals13/rregress.pdf> and <http://www.stata.com/manuals13/u20.pdf> (Chapter 20.21))* We can quite easily replicate them using the arguments *robust=TRUE* and *robust.type="H1"* in the *showreg* command.  
Analogous to the last code chunk, fill in the code for this regression and save the regression in the variable *ols_d_treat*. The code for showing the regression results is already given.
```{r "2.1 b)"}
#< task
#OLS regression ols_d_treat of dummy variable indicating deforestation
#on dummy variable indicating treatment
#>
ols_d_treat = lm(d ~ treat, data=data_deforest)
#< task
#Show regression results
showreg(list(ols_d_treat), robust=TRUE, robust.type="HC1", 
        custom.model.names=c("P(d>0)"), digits=3, stars=c(0.01,0.05,0.1))
#>
```
According to these results, the expected probability of deforestation in a polygon - $P\left(d=1|treat\right)$ - is $0.097$ if the according locality is not treated with "Oportunidades" and the coefficient of treatment is not significant at all (p-value=$0.175$).

What about the suitability of the LPM in this context?  
As Wooldridge *(2010, p.563)* explains, the LPM has some disadvantages in general: As the dependent variable is described by a linear function of explanatory variables and an error term, the predicted values can also take on negative values or values greater than $1$ which contradicts the definition of a probability. Furthermore, the partial effects describing how the probability changes when the explaining variable increases by one unit (or changes from $0$ to $1$) are equal to the constant coefficients $\beta_j$ which is not sensible in the context of a probability either. Nevertheless, if one wants to "approximate the partial effects [...], averaged across the distribution of $\vec x$" *(Wooldridge (2010), p.563)*, the model often performs quite well.  
In our specific case, the very low $R^2$ indicates that the exact formulation of the model is not adequate at all.

### c) Tobit Regressions
A model which accounts for the limited value range of the dependent variable *pct_deforested* and which makes it possible not only to estimate the effect of treatment on this variable (the amount of deforestation) but also on the probability of this variable taking on a value within this range (probability of positive deforestation) is the **Tobit model:**
- - -
In the **Tobit model** (or **censored regression model**), the dependent variable $y$ is described by a latent variable $y^*$ which is transformed to $0$ if it takes on values equal to or smaller than $0$. So for each observation $i$ it holds that 
$$ y_i = y_i^* \cdot 1_{y_i^*>0}$$
The latent variable itself can be described as a linear function of explanatory variables $\vec{x}$ with constant coefficients $\vec{\beta}$ and a normally distributed error term $\epsilon$ with mean $0$ and variance $\sigma^2$. On the level of each observation $i$ it holds thus that
$$ y_i^* = \vec{x_i}' \vec\beta + \epsilon_i \text{,     }\epsilon_i \sim N\left(0,\sigma^2\right)$$
*(Cf. Greene (2008, Chapter 24.3.2))*

This model is **applicable** to two different kinds of situations:  
**1.** The **variable of interest** is the **latent variable $y^*$** and one wants to find out estimates of the coefficients $\vec\beta$ as they are equal to the marginal effects of the explanatory variables on $E\left[y^*|\vec x\right]$. If $y_i^*$ was observable for all subjects $i$ one could just apply OLS on a classical regression model, but as it can only be observed for those subjects $i$ for which $y_i^*$ is greater than $0$, one has to estimate the according Tobit model to obtain the coefficient estimates $\widehat{\beta}$.  
**2.** The **variable of interest** is the **observable variable $y$** which by definition can only take on values greater than or equal to zero. The coefficients $\vec \beta$ don't contain the complete information on the marginal effects of the explanatory variables on the quantities of interest here. Instead, some calculations are necessary to obtain for example the marginal effects on $E\left[y|\vec x\right]$, $E\left[y|y>0, \vec x \right]$ or $P\left(y>0|\vec x \right)$.  
*(Cf. Wooldridge (2002, Chapter 16.1))*
- - -
In our case the dependent variable $y$, limited on positive values or $0$, is the percentage area deforested $\Delta f_i$ (given by *pct_deforested*) and the vector of explanatory variables $\vec{x}$ is for the moment only the dummy variable $T$ describing treatment (given by *treat*).  
As we are interested in the percentage of area deforested ($y$) itself our situation falls into the second category of applications. Like Alix-Garcia et al. we will study both the estimated coefficients and two kinds of marginal effects: The marginal effects on the probability of positive deforestation $P\left(y>0|\vec x\right)$ and on the expected amount of deforestation $E\left[y|y>0, \vec x\right]$ given there is deforestation. So we want to estimate how treatment changes the probability of deforestation as well as how treatment changes the "rate of deforestation among deforesters" *(paper, pp.424-425)*. We will come to details on how the marginal effects are calculated in this case as well as on how they are implemented later.

In the following code chunk, it is your turn to run a first Tobit regression using the *tobit* command from the package *'AER'* with the same formula as in the OLS regression and call it *tobit_pct_treat*. For some information on the *tobit* command and one alternative, have a look at the *'info' box*.
#< info "Tobit Regressions in R"
The command *censReg(formula, data, left, right)* from the package *'censReg'* needs the arguments *formula* and *data* like the *lm* command. The additional arguments *left* and *right* indicate the limits of the value range of the dependent variable. As it uses maximum likelihood estimation, it is quite time-consuming.  
The command *tobit(formula, data, left, right)* from the package *'AER'* needs the same arguments as input but works more quickly.  
For both commands the default setting defines the left limit as $0$ and the right limit as infinity which is what we need.

Throughout this problem set we will only use the command *tobit* (package *'AER'*) for the Tobit regressions.
#>
```{r "2.1 c)1"}
#< task
#Load package 'AER'
library(AER)
#Tobit regression of pct_deforested on treat
#>
tobit_pct_treat = tobit(pct_deforested ~ treat, data=data_deforest)
#< task
#Show the regression results
showreg(list(tobit_pct_treat), custom.model.names=c("Simple tobit"), digits=3,
        include.aic=FALSE, include.bic=FALSE, include.loglik=FALSE, 
        include.deviance=FALSE, include.wald = FALSE,
        stars=c(0.01,0.05,0.1))
#>
```
As you can see from the statistics at the bottom of the regression results, $53,042$ observations are "left-censored", none is "right-censored" and the rest of the $58,587$ observations are uncensored. What does this mean? An observation is called "left-censored" here, if we observe the limit value (in our case $0$). As no upper limit was specified in the regression, no observation can be "right-censored".  
*logSigma* is an estimate of the natural logarithm of the variance of $\epsilon$.  
As already explained, the coefficients don't provide us with information on the size of the treatment effect, but what is still of interest, are the sign and the significance level of the treatment coefficient estimate: It is negative and significant on the $5 \%$ level.

To get some information on the magnitude of the effect, the function *margEff_tobit_dM_AER(model, data)* calculates the marginal effects of each explanatory variable on $P(y>0|\vec x)$ and on $E[y|y>0, \vec x]$. Have a look at the code and run it by clicking on the *'check' button*. You can find some information on the calculation of those marginal effects and on the according R function in the *'info' boxes* below. Note that the package *'car'* is used in the function *margEff_tobit_dM_AER(model, data)*. If you are interested in the two commands used here to prepare the results for showing them (*convert*) and to show them (*stargazer*), have a look at the *'info' box* right above the code chunk.

#< info "Displaying estimation results: stargazer and convert"
The data frame returned by *margEff_tobit_dM_AER* is prepared here by my function *convert*. The resulting data frame is then presented by the command *stargazer* from the package *'stargazer'*. The latter allows to show a data frame in a nice format in HTML if the argument *summary* is set to *FALSE*. Otherwise it would output summary statistics of the data frame.  
In general, the command *stargazer* can also be used to show regression results. I won't go into detail about this command but if you are interested, have a look at the following link: <http://jakeruss.com/cheatsheets/stargazer.html>

The commented code of the function *convert* is given below:
```{r "info code convert"}
#This function converts a list of data frames containing at least estimates,
#standard errors and significance levels into one single data frame
#which can be used for a clear presentation of all results.

#input: -listOfModels = list of data frames containing regression results
#        for different models; data frames must have the characteristics:
#           * 1 column name contains the term "Estimate"
#           * 1 column name contains the term "SE"
#           * 1 column name is "significance"
#           * each row contains (at least) estimates, std. errors and
#             significance levels for one variable
convert = function(listOfModels){
  #Extract the variable names contained in any of the models
  coefnames = c()
  for (i in 1:length(listOfModels)){
    coefnames = c(coefnames,row.names(listOfModels[[i]]))
  }
  coefnames = unique(coefnames)
  
  #Dummy indicating if currently the first model (first element of
  #the list) is treated
  firstModel = TRUE
  
  #Go through all data frames in the listOfModels...
  for (m in 1:length(listOfModels)){
    #Separate the column containing the significance level
    #from the rest of the data frame
    model = select(listOfModels[[m]], -significance)
    model = round(model, digits=3)
    sig = select(listOfModels[[m]], significance)
    
    #dummy indicating if the first variable in coefnames is currently treated
    first=TRUE
    
    #Go through all variables contained in any of the models' data frames.
    for (i in 1:length(coefnames)) {
      #If it is the first variable: Create a new data frame
      #containing only information on estimate, standard error and significance 
      #level for each variable which is contained in any of the models.
      #If the current model does not use the respective variable,
      #an empty line is added to the data frame. In all other cases,
      #the information is extracted from the row of the current data frame m
      #in which this information for the currently treated variable is contained
      #and written into the new data frame.
      #If it is not the first variable, the respective information/an empty line
      #is added to the data frame just described.
      if (first == TRUE){
        first = FALSE
        if (any(which(row.names(model)==coefnames[i])) == FALSE){
          results = data.frame(estimate = "", se = "", 
                               row.names = coefnames[i])
        } else {
          results = data.frame(estimate = model[which(row.names(model)==coefnames[i]),
                                                grep("Estimate+", colnames(model))],
                               se = paste0("(",as.character(model[which(row.names(model)==coefnames[i]),
                                                                  grep("SE+", colnames(model))]),")", 
                                           as.character(sig[which(row.names(model)==coefnames[i]),1])),
                               row.names = coefnames[i])
        }
      } else {
        if (any(which(row.names(model)==coefnames[i])) == FALSE){
          help = data.frame(estimate = "", se = "",
                            row.names = coefnames[i])
          results = rbind(results, help)
        } else {
          help = data.frame(estimate = model[which(row.names(model)==coefnames[i]),
                                             grep("Estimate+", colnames(model))],
                            se = paste0("(",as.character(model[which(row.names(model)==coefnames[i]),
                                                               grep("SE+", colnames(model))]),")", 
                                        as.character(sig[which(row.names(model)==coefnames[i]),1])),
                            row.names = coefnames[i])
          results = rbind(results, help)
        }
      }
    }
    #The resulting data frame for model m contains information on estimate,
    #standard error and significance level; Each row refers to one variable
    #in coefnames such that all variables appearing in any model are considered
    #in this resulting data frame.
    
    #Name the columns of the resulting data frame
    colnames(results)[(0:((ncol(results)-1)/2))*2+1] = paste0(names(listOfModels)[m], ": ",
                                                              colnames(results)[(0:((ncol(results)-1)/2))*2+1])
    colnames(results)[-((0:((ncol(results)-1)/2))*2+1)] = paste0("(",colnames(results)[-((0:((ncol(results)-1)/2))*2+1)],
                                                                 ".",as.character(m),")")
    #Create the final data frame results_df containing all resulting
    #data frames of the single models.
    if (firstModel == TRUE){
      results_df = results
      firstModel = FALSE
    } else {
      results_df = cbind(results_df, results)
    }
  }
  #Return the final data frame
  return(results_df)
}
```

#>
```{r "2.1 c) 2", results="asis"}
#< task
#Load package 'car'
library(car)
#Marginal effects
marge_pct_treat = margEff_tobit_dM_AER(tobit_pct_treat, data_deforest)
#Load package 'stargazer'
library(stargazer)
#Show the marginal effects
stargazer(convert(marge_pct_treat), type="html", summary=FALSE)
#>
```
#< info "Marginal Effects on E[y|x,y>0] and P(y>0|x)"
As marginal effects can be obtained by taking the partial derivative (in case of continuous variables) or the difference (with the explanatory variable of interest equal to $1$ and to $0$) of the quantity $\mu$ for which the effect shall be computed, the base of the computation of the marginal effects of interest here are formulas for $E\left[y|\vec{x},y>0\right]$ and $P(y>0)$. Both of them can be calculated analytically for the Tobit model introduced above:  
$$\text{(1)   }P\left(y>0|\vec{x}\right) = \Phi\left(\frac{\vec{x}^{\prime}\vec{\beta}}{\sigma}\right)$$
$$\text{(2)   }E\left[y|\vec{x},y>0\right] = \vec{x}^{\prime}\vec{\beta} + \sigma \lambda\left(\frac{\vec{x}^{\prime}\vec{\beta}}{\sigma}\right) \text{    with    } \lambda(z)=\frac{\varphi(z)}{\Phi(z)}$$
Here, $\varphi()$ stands for the normal density function and $\Phi()$ for the normal distribution function.  
The according marginal effects for a binary variable $x_i$ are given by differences of the respective quantity with $x_i=1$ and $x_i=0$. For a continuous variable $x_j$, the marginal effects are as follows:
$$\text{(3)   }\frac{\partial P\left(y>0|\vec{x}\right)}{\partial x_j} = \varphi\left(\frac{\vec{x}^{\prime}\vec{\beta}}{\sigma}\right) \frac{\beta_j}{\sigma}$$
$$\text{(4)   }\frac{\partial E\left[y|\vec{x},y>0\right]}{\partial x_j} = \beta_j\left(1-\lambda((\vec{x}^{\prime}\vec{\beta})/\sigma) \left(\frac{\vec{x}^{\prime}\vec{\beta}}{\sigma} + \lambda((\vec{x}^{\prime}\vec{\beta})/\sigma)\right)     \right)$$
*If you are interested in the derivations, I recommend Wooldridge 2010, Chapter 17.2 to you.*

It is interesting to note two characteristics of the marginal effects depicted here concerning their relation to the Tobit coefficients $\vec{\beta}$: Both marginal effects are  
* **smaller in absolute value** than the corresponding Tobit coefficient $\beta_j$  
* **show the same sign** as $\beta_j$

This is because in both cases the factor with which $\beta_j$ is multiplied in order to obtain the respective marginal effect lies between $0$ and $1$.  
*(Cf. Wooldridge (2010, Chapter 17.2))*
#>
#< info "margEff_tobit_dM_AER(model, data) - Introduction and Code"
The formulas for the marginal effects depicted above can easily be computed in R based on the Tobit regression output and the according data set. However, to be able to get not only estimated marginal effects but also the according standard errors, I apply the *delta method*:
- - -
If for a vector of random variables $\vec{z_n}$ it holds that $\sqrt{n} \left(\vec{z_n}-\mu\right)$ converges in distribution to a normal distribution with mean $\vec{0}$ and variance-covariance matrix $\Sigma$ then for $\vec{g}\left(\vec{z_n}\right)$ with $\vec{g}()$ a vector of functions continuous in $\vec{z_n}$ it holds that $\sqrt{n} \left(\vec{g}\left(\vec{z_n}\right)-\vec{g}\left(\mu\right)\right)$ converges in distribution to a normal distribution with mean $\vec{0}$ and variance-covariance matrix
$$\frac{\partial\vec{g}\left(\mu\right)}{\partial \mu^{\prime}}\Sigma\left(\frac{\partial\vec{g}\left(\mu\right)}{\partial \mu^{\prime}}\right)^{\prime}$$
*(Greene (2008, Appendix D.2.7))*
- - -
In our case, the vector $\vec{z_n}$ equals $\left(\widehat{\vec{\beta}},\widehat{ln(\sigma)}\right)^{\prime}$ with $\vec{\beta}$ the Tobit coefficients and $ln(\sigma)$ the logarithm of the standard deviation of the error term $\epsilon$, the vector $\vec{\mu}$ equals $\left(\vec{\beta},ln(\sigma)\right)^{\prime}$, $\Sigma$ its variance-covariance matrix and the function $\vec{g}()$ is a function calculating the formula of the respective marginal effect for all explanatory variables. By applying the formula for one kind of marginal effect to the vector $\left(\widehat{\beta},\widehat{ln(\sigma)}\right)^{\prime}$ an estimate of the marginal effects of each explanatory variable can be obtained and by applying the delta method on the whole setting just described one obtains the according standard errors.  
*Note that this is done in analogy to an example in Greene (1999).*

This is exactly what the function *deltaMethod(object, vcov., g, constants)* from the package *'car'* does. *(Cf. [CRAN Reference manual 'car'](https://cran.r-project.org/web/packages/car/car.pdf "https://cran.r-project.org/web/packages/car/car.pdf"), pp.45-48)* Here, the argument *object* is the vector of the Tobit coefficient estimates and the logarithmized standard error of $\epsilon$, the argument *vcov.* is the variance-covariance matrix of this vector. The argument *constants* are the further constants used in the formula *g* which is the marginal effect formula (written as a string) in our case .  
This function is the core element of my implementation in the function *margEff_tobit_dM_AER*. If you are interested in the details of the implementation, just have a look at the commented code below.
```{r "info code tobitME"}
#This function estimates the marginal effects of each explanatory variable
#on P(y>0|x) and E[y|x,y>0] for the Tobit model.

#input: - Tobit model estimated by command "tobit" from package 'AER'
#       - corresponding data frame
margEff_tobit_dM_AER = function(tobit,data){
  #load package 'car' for later use of delta method
  library(car)
  
  #extract estimates from estimated Tobit model for coefficients and
  #for the logarithmized variance of the error term epsilon
  tobit_coef = summary(tobit)$coef[,1]
  
  #define number of estimates
  n = length(tobit_coef)
  
  #extract estimates of the coefficients {beta_j} from Tobit
  tobit_beta = tobit_coef[-n]
  
  #concerning all estimates (tobit_coef):
  #rename "intercept" "beta0" and "log(Sigma)" "lnsigma"
  #save renamed coefficients in renamedcoefs
  renamedcoefs = tobit_coef
  names(renamedcoefs)[c(1,n)] = c("beta0", "lnsigma")
  
  #extract explanatory variables from data
  cols = names(tobit_beta)[-1]
  X = data[,cols]
  #calculate the mean of all explanatory variables and name them appropriately
  if ( length(cols)==1){
    X_mean=mean(X)
  } else{
    X_mean=colMeans(X, na.rm=TRUE)
  }
  names(X_mean) = paste(cols, "mean", sep="")
  
  #determine binary variables and continuous variables
  firstfound = FALSE
  for (i in 1:(n-2)) {
    if (length(cols)==1){
      num_contained = unique(X)
    } else{
      num_contained = unique(X[,i])
    }
    if (length(num_contained)==2){
      if ((num_contained[1]==0 & num_contained[2]==1) | (num_contained[1]==1 & num_contained[2]==0)){
        if (firstfound==FALSE){
          firstfound = TRUE
          bin = c(i)
        } else{
          bin = c(bin, i)
        }
      }
    }
    if (length(num_contained)==1){
      if (num_contained==0 | num_contained==1){
        if (firstfound==FALSE){
          firstfound = TRUE
          bin = c(i)
        } else{
          bin = c(bin, i)
        }
      }
    }
  }
  
  conti = seq(1:(n-2))[-bin]
  
  #for each type of marginal effect:
  #- define the formula as a string
  #- calculate estimate of marginal effect and standard error with delta method
  #- add t value, corresponding p value and significance level
  #  to resulting data frame
  #- name the resulting data frame
  
  #marginal effects for continuous variables
  firstconti = FALSE
  for (i in conti){
    #marginal effect on P(y>0|x)
    formula_P_conti_i = paste("(dnorm((beta0 + ", paste(cols, names(X_mean), sep="*", collapse=" + "),
                              ")/exp(lnsigma))/exp(lnsigma)) * ", cols[i], sep="")
    margEff_P_conti_i = deltaMethod(object = renamedcoefs, vcov. = vcov(tobit),
                                    g = formula_P_conti_i, constants = X_mean)
    margEff_P_conti_i = mutate(margEff_P_conti_i, tvalue = Estimate/SE, 
                               pvalue = 2*pt(-abs(tvalue), nrow(data) - (n-1)),
                               significance = as.factor(ifelse(pvalue<0.1, 
                                                               ifelse(pvalue<0.05,
                                                                      ifelse(pvalue<0.01, "***" , "**") , "*"), "")))
    rownames(margEff_P_conti_i)[1] = cols[i]
    colnames(margEff_P_conti_i)[1] = "Estimate P(y>0)"
    
    #marginal effect on E[y|x,y>0]
    formula_E_conti_i = paste("(1-(dnorm((beta0 + ", 
                              paste(cols, names(X_mean), sep="*", collapse= " + ") ,
                              ")/exp(lnsigma))/pnorm((beta0 + ", paste(cols, names(X_mean), sep="*", collapse= " + ") ,
                              ")/exp(lnsigma))) * (  ((beta0 + ", paste(cols, names(X_mean), sep="*", collapse = " + "),
                              ")/exp(lnsigma)) + (dnorm((beta0 + ", paste(cols, names(X_mean), sep="*", collapse= " + ") ,
                              ")/exp(lnsigma))/pnorm((beta0 + ", paste(cols, names(X_mean), sep="*", collapse= " + ") ,
                              ")/exp(lnsigma)))  )) * ", cols[i], sep="")
    margEff_E_conti_i = deltaMethod(object = renamedcoefs, vcov. = vcov(tobit),
                                    g = formula_E_conti_i, constants = X_mean)
    margEff_E_conti_i = mutate(margEff_E_conti_i, tvalue = Estimate/SE, 
                               pvalue = 2*pt(-abs(tvalue), nrow(data) - (n-1)),
                               significance = as.factor(ifelse(pvalue<0.1, 
                                                               ifelse(pvalue<0.05,
                                                                      ifelse(pvalue<0.01, "***" , "**") , "*"), "")))
    rownames(margEff_E_conti_i)[1] = cols[i]
    colnames(margEff_E_conti_i)[1] = "Estimate E[y|y>0]"
    
    #Save results in data frames for each kind of marginal effect
    #of all continous variables
    if (firstconti==FALSE){
      firstconti = TRUE
      margEff_P_conti = margEff_P_conti_i
      margEff_E_conti = margEff_E_conti_i
    } else{
      margEff_P_conti = rbind(margEff_P_conti, margEff_P_conti_i)
      margEff_E_conti = rbind(margEff_E_conti, margEff_E_conti_i)
    }
  }
  
  #marginal effects for binary variables
  firstbin = FALSE
  for (i in bin){
    #adapt the mean of the explanatory variables for the calculations below
    #and save them in X_mean_bin
    X_mean_bin = X_mean
    X_mean_bin[i] = 1
    
    #marginal effect on P(y>0|x)
    formula_P_bin_i = paste("pnorm((beta0 + ", paste(cols, names(X_mean_bin), sep="*", collapse= " + ") ,
                            ")/exp(lnsigma))", " - ", 
                            "pnorm((beta0 + ", paste(cols, names(X_mean_bin), sep="*", collapse= " + "), 
                            " - ", cols[i] , ")/exp(lnsigma))", sep="")
    margEff_P_bin_i = deltaMethod(object = renamedcoefs, vcov. = vcov(tobit),
                                  g = formula_P_bin_i, constants = X_mean_bin)
    margEff_P_bin_i = mutate(margEff_P_bin_i, tvalue = Estimate/SE, 
                             pvalue = 2*pt(-abs(tvalue), nrow(data) - (n-1)),
                             significance = as.factor(ifelse(pvalue<0.1, 
                                                             ifelse(pvalue<0.05,
                                                                    ifelse(pvalue<0.01, "***" , "**") , "*"), "")))
    rownames(margEff_P_bin_i)[1] = cols[i]
    colnames(margEff_P_bin_i)[1] = "Estimate P(y>0)"
    
    #marginal effect on E[y|x,y>0]
    formula_E_bin_i = paste(cols[i], " + exp(lnsigma) * (dnorm((beta0 + ", 
                            paste(cols, names(X_mean_bin), sep="*", collapse= " + ") ,
                            ")/exp(lnsigma))/pnorm((beta0 + ",
                            paste(cols, names(X_mean_bin), sep="*", collapse= " + ") ,
                            ")/exp(lnsigma)))", " - ",
                            "exp(lnsigma) * (dnorm((beta0 + ",
                            paste(cols, names(X_mean_bin), sep="*", collapse= " + ") ,
                            " - ", cols[i], ")/exp(lnsigma))/pnorm((beta0 + ",
                            paste(cols, names(X_mean_bin), sep="*", collapse= " + ") ,
                            " - ", cols[i], ")/exp(lnsigma)))", sep="")
    margEff_E_bin_i = deltaMethod(object = renamedcoefs, vcov. = vcov(tobit),
                                  g = formula_E_bin_i, constants = X_mean_bin)
    margEff_E_bin_i = mutate(margEff_E_bin_i, tvalue = Estimate/SE, 
                             pvalue = 2*pt(-abs(tvalue), nrow(data) - (n-1)),
                             significance = as.factor(ifelse(pvalue<0.1, 
                                                             ifelse(pvalue<0.05,
                                                                    ifelse(pvalue<0.01, "***" , "**") , "*"), "")))
    rownames(margEff_E_bin_i)[1] = cols[i]
    colnames(margEff_E_bin_i)[1] = "Estimate E[y|y>0]"
    
    #Save results in data frames for each kind of marginal effect
    #of all continous variables
    if (firstbin==FALSE){
      firstbin = TRUE
      margEff_P_bin = margEff_P_bin_i
      margEff_E_bin = margEff_E_bin_i
    } else{
      margEff_P_bin = rbind(margEff_P_bin, margEff_P_bin_i)
      margEff_E_bin = rbind(margEff_E_bin, margEff_E_bin_i)
    }
    
  }

  #Create the data frame or warning to return
  if (firstbin ==TRUE) {
    if (firstconti == TRUE){
      return (list(margEff_P = rbind(margEff_P_bin, margEff_P_conti),
                   margEff_E = rbind(margEff_E_bin, margEff_E_conti)))
    } else {
      return (list(margEff_P = margEff_P_bin,
                   margEff_E = margEff_E_bin))
    }
  } else{
    if (firstconti == TRUE){
      return (list(margEff_P = margEff_P_conti,
                   margEff_E = margEff_E_conti))
    } else {
      return ("Error: No marginal effects available.")
    }
  }
}
```
#>
The marginal effects estimates of treatment on the probability of positive deforestation and on the expected percentage of area deforested given there is deforestation are both significant and negative. Treatment is associated with a decrease of the probability of deforestation of $0.5$ percentage points which is almost the same as estimated by the LPM even though the latter estimate was not significant. Furthermore, treatment is associated with $0.022$ percentage points less area deforested. This estimate is not directly comparable to the OLS estimation we conducted in sub exercise **a)**. Rather one would have to conduct the same OLS on a data set restricted to observations with positive deforestation to make the results more comparable.  
This is what we will do in the following code chunk. It is very easy to run a regression on a subset of a data frame with the command *lm()*. In the argument *subset* one can use for example a logical expression to determine the subset used. Just click on the *'check' button* to run the regression.
```{r "2.1 c) 3"}
#< task
#OLS regression of percentage of area deforested on treatment
#given there is positive deforestation (d=1)
ols_pos = lm(pct_deforested ~ treat, data=data_deforest, subset = (d==1))
#Show regression results
showreg(list(ols_pos), custom.model.names=c("% area deforested given d=1"), 
        digits=3, stars=c(0.01,0.05,0.1))
#>
```
As you can see, the estimated treatment coefficient differs from the marginal effect in the Tobit regression, but it is significant and has the same sign. Throughout their paper, Alix-Garcia et al. use this kind of OLS on the subset of the data set together with the OLS regression of the LPM as robustness tests for almost all Tobit regressions (and IV Tobit regressions) they run.

- - -
All in all, the exercise shows that the Classical Linear Regression Model estimated by OLS is not the best method to estimate the treatment effect - the **Tobit model is more appropriate** here. The former as well as the Linear Probability Model might be sensible as robustness tests however.  
It is important to note that up to now we have **only estimated correlations** between treatment and deforestation. For these associations to have a causal interpretation one has to make sure above all that treatment is exogenous meaning that it is not correlated with unobservable variables captured by the disturbance $\epsilon$. At this step, economic thinking is needed to determine variables which are likely to be correlated both with deforestation and with treatment. Go on to the next exercise to take a closer look at this topic.


## Exercise 2.2 -- Adding control variables
Apart from treatment, Alix-Garcia et al. consider some further variables as controls *(cf. paper, tables 2-5)*:  
marginality index, population number, area, baseline forested area, road density, slope and ecoregional controls.  
As those variables are "locality-level characteristics" *(paper, p.423)*, they might be related to the decision if a locality was treated or not. Above all, they are likely to influence deforestation. Unfortunately, Alix-Garcia et al. don't give further reasons for their choice of further explanatory variables in the regression of deforestation (with the exception of comments on slope *(paper, p.424)* and on the marginality index *(paper, p.423)*) but in the light of papers they cite, their choice becomes clearer. If you are interested in the background on probable factors affecting deforestation, have a look at the following *'info' box*.
#< info "Key factors affecting deforestation"
**Angelsen and Kaimowitz (1999)** analyze studies conducted on this question and summarize the results. They distinguish between "immediate causes", "underlying causes" and "sources of deforestation". According to them, the major "source of deforestation" is the expansion of area used for agricultural purposes. *(p.76)* "Immediate causes" are factors influencing the sources of deforestation: input and output prices in the agriculture, prices for forest products, technological progress on farms, wages and employment outside the agricultural work, infrastructure and variables concerning credits and the security of property. *(table 2, p.82)* Changes of the population density as well as of income, "economic growth", technological progress in a more general way and the economic situation in the country may affect those immediate causes and are thus stated as "underlying causes of deforestation". *(table 3, p.87)*   
**Barbier and Burgess (2001)** consider deforestation in tropical countries including Latin America. In their paper they first give a summary on cross-country analyzes on this topic. The key factors of deforestation they detect in this context are largely consistent with those given by Angelsen and Kaimowitz (1999), but they add "scale factors" like area and forested area and variables describing the yields of agricultural production and logging to the list of probable influences. *(pp.417-418)* Like Angelsen and Kaimowitz (1999), they also consider the "growth in agricultural land area" as the major cause of deforestation and find out that features of the agricultural development like for example the proportion of cropland relative to the land area or the "share of agricultural exports" and political instability significantly rise agricultural expansion and thus also deforestation to a similar extent. *(p.429)*  
In their studies on deforestation in Mexico **Deininger and Minten (2002)** consider variables affecting the agricultural productivity of land area and variables relevant for input and output prices of agricultural production. The assumption behind this is that if land area is not forested, it is devoted to agricultural purposes. *(p.944)* Among the first group of variables there are "physio-geographic characteristics" like for example slope or soil characteristics, variables describing the "security of property rights" and "policy variables". For the second group, infrastructure, population density, poverty and the "implicit price of capital" are chosen. *(p.946)*

So all in all, the considered types of variables are consistent among the depicted papers, even though the actually chosen concrete variables differ a bit.
#>

Almost all variables used in the paper can be found in the lists of probable causes described above. But how would we expect the variables to affect the pressure on forests (amount and/or probability of deforestation)? In the following **QUIZ**, please choose for each control variable if you expect it to increase and/or decrease deforestation. There is no real true or false but I will give you my explanation as an answer.
- - -
- - -
#< quiz "Expected sign - poverty index"
question: POVERTY INDEX 
mc: 
  - increase*  
  - decrease* 
success: Your answer is in line with my argumentation. There are arguments for both directions of influence here. Studies show that the relationship between the income/poverty level and deforestation is not linear. This is what we have also seen in *Exercise 1.2*. Deininger and Minten *(2002, pp.946, 950-951, 955)* for example expect poverty to have a positive impact on the probability of deforestation but empirically they get both positive and negative coefficients for poverty. 
failure: You might have good reasons for your answer. However, my hypothesis is a bit different. There are arguments for both directions of influence here. Studies show that the relationship between the income/poverty level and deforestation is not linear. This is what we have also seen in *Exercise 1.2*. Deininger and Minten *(2002, pp.946, 950-951, 955)* for example expect poverty to have a positive impact on the probability of deforestation but empirically they get both positive and negative coefficients for poverty. 
#>
- - -
#< quiz "Expected sign - population"
question: POPULATION 
sc: 
  - increase* 
  - decrease 
multiple: FALSE 
success: Your answer is in line with my argumentation. An increase in the population number might lead to an increased need for land for food as well as for forest products. At the same time it leads to a decrease of wage rates and to technological progress which both reduce the pressure on forests. All in all the positive influences slightly outweigh the negative influences. *(Cf. Angelsen and Kaimowitz (1999, pp.87-88))* 
failure: You might have good reasons for your answer. However, my hypothesis is a bit different. An increase in the population number might lead to an increased need for land for food as well as for forest products. At the same time it leads to a decrease of wage rates and to technological progress which both reduce the pressure on forests. All in all the positive influences slightly outweigh the negative influences.*(Cf. Angelsen and Kaimowitz (1999, pp.87-88))*
#>
- - -
#< quiz "Expected sign - area"
question: AREA 
mc:
  - increase* 
  - decrease 
success: Your answer is in line with my argumentation. Even though this variable is mentioned by Barbier and Burgess *(2001, p.417)*, they don't state the expected sign of influence. One could argue that the percentage of area deforested should not change as the area size changes. In contrast, the probability of deforestation might increase with the area as the probability rises that there is some part of the area well suited for deforestation. 
failure: You might have good reasons for your answer. However, my hypothesis is a bit different. Even though this variable is mentioned by Barbier and Burgess *(2001, p.417)*, they don't state the expected sign of influence. One could argue that the percentage of area deforested should not change as the area size changes. In contrast, the probability of deforestation might increase with the area as the probability rises that there is some part of the area well suited for deforestation. 
#>
- - -
#< quiz "Expected sign - forest baseline"
question: FOREST BASELINE 
mc: 
  - increase* 
  - decrease 
success: Your answer is in line with my argumentation. The greater the area forested in a country, the higher the deforestation rates. *(Cf. Barbier and Burgess (2001, p.418))* This might account as well for smaller regions like the polygons regarded in this problem set. A possible explanation could be that the less area forested, the more area is left for agricultural use. So the pressure on the existing forest is smaller. Another reason might be that the costs for clearing might outweigh the benefits if there is little forested area. 
failure: You might have good reasons for your answer. However, my hypothesis is a bit different. The greater the area forested in a country, the higher the deforestation rates. *(Cf. Barbier and Burgess (2001, p.418))* This might account as well for smaller regions like the polygons regarded in this problem set. A possible explanation could be that the less area forested, the more area is left for agricultural use. So the pressure on the existing forest is smaller. Another reason might be that the costs for clearing might outweigh the benefits if there is little forested area which might make it more economic to buy timber products from neighboring polygons.
#>
- - -
#< quiz "Expected sign - road density"
question: ROAD DENSITY
mc:
  - increase* 
  - decrease
success: Your answer is in line with my argumentation. Angelsen and Kaimowitz *(1999, pp.82,85)*, Barbier and Burgess *(2001, p.418)* and Deininger and Minten *(2002, p.946)* all state a positive relationship between road density and deforestation. Roads improve the access to forests and markets which in turn "accelerates deforestation" *(Angelsen and Kaimowitz (1999), p.85)*.
failure: You might have good reasons for your answer. However, my hypothesis is a bit different. Angelsen and Kaimowitz *(1999, pp.82,85)*, Barbier and Burgess *(2001, p.418)* and Deininger and Minten *(2002, p.946)* all state a positive relationship between road density and deforestation. Roads improve the access to forests and markets which in turn "accelerates deforestation" *(Angelsen and Kaimowitz (1999), p.85)*.
#>
- - -
#< quiz "Expected sign - ecoregional controls"
question: ECOREGIONAL CONTROLS
sc:
  - See some background information* 
success: The three ecoregional controls used in the analyzes by Alix-Garcia et al. are dummy variables which indicate if there are "tropical and subtropical coniferous forests", "tropical and subtropical dry broadleaf forests" and "tropical and subtropical moist broadleaf forests". This way one can test if deforestation varies among different types of forest. Depending on the type of forest it might be more/less difficult to remove it if the deforesters are interested in the area itself and it might be more/less lucrative to deforest for the purpose of producing timber products. *(Cf. original data set "ecological_footprint_restat_deforestation_data.dta")*
failure: Try again.
#>
- - -
- - -
In the following, we will compare the Tobit regression accounting for all these variables to the simple Tobit regression from the last *Exercise 2.1*. To be able to do so, please load the data set again by clicking on the *'check' button*.
```{r "2.2 1"}
#< task
#Load the data set "data_deforest.rds" and save it in data_deforest
data_deforest = readRDS("data_deforestation.rds")
#>
```
In the following code chunk, the simple Tobit regression with sole explanatory variable *treat* from *Exercise 2.2* is run again. Run a second regression adding the following explanatory variables: *pov_ind_95*, *forest_prior*, *ln_pop*, *ln_area_polyg*, *ln_dens_road*, *deg_slope*, *ecoreg_control_1*, *ecoreg_control_2*, *ecoreg_control_3* and call it *tobit_pct_extended*.

```{r "2.2 2"}
#< task
#Load package 'AER'
library(AER)
#Tobit regression of pct_deforested on treat
tobit_pct_treat = tobit(pct_deforested ~ treat, data=data_deforest)

#Tobit regression of pct_deforested on larger set of explanatory variables
#>

tobit_pct_extended = tobit(pct_deforested ~ treat + pov_ind_95 + 
                             forest_prior + ln_pop + ln_area_polyg +
                             ln_dens_road + deg_slope + ecoreg_control_1 +
                             ecoreg_control_2 + ecoreg_control_3,
                           data=data_deforest)

#< task
#Load package 'regtools'
library(regtools)
#Show the regression results
showreg(list(tobit_pct_treat, tobit_pct_extended), 
        custom.model.names=c("Tobit simple", "Tobit extended"), digits=3,
        include.wald = FALSE, stars=c(0.01,0.05,0.1))
#>
```
In order to obtain the marginal effects for the first regression above, the function *margEff_tobit_dM_AER(model, data)* is applied to the simple model *tobit_pct_treat* and the data set *data_deforest*. Do the same for the extended model *tobit_pct_extended* and assign the results to the variable *marge_pct_extended*. The code presenting the results is already given.
```{r "2.2 3", results="asis"}
#< task
#Load package 'car'
library(car)
#Calculate the marginal effects for the simple Tobit model
marge_pct_treat = margEff_tobit_dM_AER(tobit_pct_treat, data_deforest)
#Calculate the marginal effects for the extended Tobit model
#>
marge_pct_extended = margEff_tobit_dM_AER(tobit_pct_extended, data_deforest)
#< task
#Load package 'stargazer'
library(stargazer)
#Show marge_pct_treat and marge_pct_extended
stargazer(convert(list(simple = marge_pct_treat$margEff_P, 
                       extended = marge_pct_extended$margEff_P)), 
          type="html", summary=FALSE)
stargazer(convert(list(simple = marge_pct_treat$margEff_E, 
                       extended = marge_pct_extended$margEff_E)),
          type="html", summary=FALSE)
#>
```
For both the conditional probability and the conditional expected amount of deforestation the marginal effect of treatment in the extended model is even more negative and more significant than in the simple model. Furthermore, the control variables are all (highly) significant except for the variable describing the baseline area forested. With the exception of the coefficient estimate on road density *ln_dens_road* the signs are as expected.

Is this already the answer to the question how treatment with the program affects deforestation? More precisely: Is it already enough to use Tobit instead of OLS regressions and to add the variables depicted above to make sure that the estimated treatment coefficient reflects the *causal* influence of treatment on deforestation?  
Unfortunately, the answer is **no.**  The basic problem is that **treatment "is not randomly assigned"** *(paper, p.423)* for the following two reasons:  
* **1.** A locality could only be treated if it was declined *eligible* which applied only for localities which were *poor enough*. It is likely that those poor localities might have shared further characteristics influencing their deforestation behavior.  
* **2.** Second, treatment was *voluntary* meaning that the localities could decide themselves whether they wanted to participate in the program or not. So it might be the case that those who decided pro participating were systematically different from those deciding against it which might be reflected as well in the deforestation behavior. This means that there might be a **selection bias**.  
*(Cf. paper, p.423)*

In the recent OLS and Tobit regressions the estimated marginal effects of treatment on the probability of deforestation as well as on the amount of deforestation given there was deforestation, are negative. So one could imagine two different kinds of scenarios:  
* **Scenario 1:** The localities treated all showed some common characteristics leading them to show *higher* deforestation rates than the average locality. In this case, the estimator would be *upwards biased* which would *strengthen the result* that treatment reduces deforestation.  
* **Scenario 2:** The localities treated all showed some common characteristics leading them to show *lower* deforestation rates than the average locality. In this case, the estimator would be *downwards biased* which would make it very *unclear* if treatment really reduces deforestation or not.

One possibility to meet this probable selection bias would be to include those certain common characteristics affecting deforestation. This is very difficult and we can never be sure if we really consider enough variables.  
A very elegant solution to this problem is the so called **Regression Discontinuity Design** - a different estimation method which makes use of the way the Oportunidades program was realized.

- - -
So to **sum up** this exercise: The **covariates** used by Alix-Garcia et al. seem to be a **good choice** given the theoretical background and the significance of the estimates. Nevertheless, even taking them into account (and using the Tobit model) does not prevent the estimates from being probably **biased**.

## Exercise [Q2] -- QUIZ on Exercises 2.1 and 2.2
Until now, you have already got to know some possible methods to measure the correlation between treatment and deforestation and some background on variables possibly influencing deforestation. Do you still remember the main aspects of the contents and useful related R functions? If you like, you can find it out here.

- - -
#< quiz "Quiz 2"
parts:
  - question: 1. Estimating a classical linear regression model by OLS would be a very intuitive approach to measure the correlation between treatment and deforestation. Why is it not suitable here?
    choices:
        - The variable pct_deforested describes deforestation which can only take on positive values.*
        - The variable treat is a dummy variable taking on only the values 0 or 1.
    multiple: FALSE
    success: Great! This is correct! A possible way to account for this problem is to use a Tobit model.
    failure: Try again. It might also help you to have a look at *Exercise 2.1 a)* again.
  - question: 2. When running regressions it is usually quite interesting to estimate the marginal effects of the explanatory variables of interest on the dependent variable. What is the correct ending of the following sentence? In an OLS estimation of a classical linear regression model...
    choices:
        - one has to apply special formulas to the estimated coefficients and the standard error of the error term to get the marginal effects on E[y|x] or E[y|y>0,x].
        - the estimated coefficients are equal to the marginal effects on E[y|x].*
    multiple: FALSE
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 2.1 a)* again.
  - question: 3. What about the marginal effects in the Tobit model? What is the correct ending of the following sentence? In a Tobit model...
    choices:
        - one has to apply special formulas to the estimated coefficients and the standard error of the error term to get the marginal effects on E[y|x] or E[y|y>0,x].*
        - the estimated coefficients are equal to the marginal effects on E[y|x].
    multiple: FALSE
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 2.1 c)* again.
  - question: 4. How can Tobit regressions be estimated in R? Choose a correct command and package.
    choices:
        - Tobit ('AER')
        - tobit ('ARE')
        - tobit ('AER')*
    multiple: FALSE
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 2.1 c)* again.
  - question: 5. In *Exercise 2.2* some literature on causes of deforestation was introduced. What is the major source/ the greatest competitor of forests? If you have not read the according *'info' box*, just try to solve it and then have a look at the answer.
    choices:
        - industry
        - agriculture*
        - building construction
    multiple: FALSE
    success: Great! This is correct! As you will see, the findings in this problem set are consistent with the hypothesis of agriculture being a competitor of forests.
    failure: You might not have read the according *'info' box* in *Exercise 2.2* which is no problem at all. What is meant here is agriculture. Try it.
  - question: 6. The results from the Tobit regression estimation which accounts for all available covariates are very likely to have a causal interpretation. 
    choices:
        - correct
        - wrong*
    multiple: FALSE
    success: Great! This is correct! Unfortunately, the estimates are likely to be biased.
    failure: Try again. It might also help you to have a look at *Exercise 2.2* again.
  - question: 7. Given you think the last statement was wrong, which direction of a bias would be worse for the interpretation of the current estimates?
    choices:
        - I think the statement above was correct.
        - upwards bias
        - downwards bias*
    multiple: FALSE
    success: Great! This is correct! If the current treatment estimator was upwards biased, this would mean that the negative sign of the estimated treatment effect was correct. If it was instead downwards biased, we could not say anything about the sign of the true treatment effect at all - which is the worse scenario.
    failure: Try again. You might want to think about the last question again as well. It might also help you to have a look at *Exercise 2.2* again.
#>

#< award "Regression specialist!"
Classical linear regression model, linear probability model or Tobit model -  
you know what to do in theory and in R! Keep it up!
#>


## Exercise 2.3 -- Regression Discontinuity Design
In order to find out whether and how treatment affects deforestation a simple Tobit regression on the variable *treat* and control variables is very likely to be biased. The main reason for this problem is the fact that treatment is not randomly assigned. Fortunately, we can use another method to estimate the (causal) treatment effect almost as well as if treatment was randomly assigned: the **Regression Discontinuity Design (RD Design):**
- - -
Consider the outcome variable $y$, the treatment variable $D$ (equals $1$ if the subject is treated and $0$ else) and the running variable $a$. The causal question of interest is: How does treatment affect the outcome?  
In the **sharp RD design** treatment $D$ is defined as  
$$ D=1 \text{, if } a \geq c \text{   and }D=0 \text{, if } a < c $$
This means that treatment changes discontinuously at the cutoff $c$ and is determined by the running variable $a$ alone. If the probability of treatment changes discontinuously at the cutoff $c$ but the jump is smaller than $100 \%$, one speaks of a **fuzzy RD design**.  
No matter if fuzzy or sharp, the idea of the RD design is that if treatment is the only explanatory variable changing discontinuously at the cutoff $c$, a discontinuity of the conditional expectation of the dependent variable $y$ as a function of the running variable can be attributed to the jump in the treatment variable - it is the **treatment effect**. Put differently, the observations very close to the cutoff are very similar in all of their characteristics and only differ in treatment which is defined by the - kind of "random" - choice if they are located a tiny bit to the left or to the right of the cutoff. So comparing those observations reveals the treatment effect.  
*(Cf. Imbens and Lemieux (2008, Chapter 2); Angrist and Pischke (2015, Chapter 4); paper, p.424)*  
*Please note that the range of validity of the according estimates is regarded in the final interpretation of the results in Exercise 2.7.*
- - -
In our case, the question of interest is: How does treatment with Oportunidades affect the percentage of area deforested? So the treatment variable is in our case $T$, the outcome variable is $\Delta f$, the running variable is the marginality index $I$, the cutoff $c$ equals $-1.22$ and the variable determining to which side of the cutoff an observation belongs is the dummy variable indicating eligibility $E$.

But are we **allowed to apply the RD design** here to get reliable results - and which one (sharp/fuzzy) is suitable?  
In order to answer the first of those two questions *Imbens and Lemieux (2008, section 3)* recommend **checking** the following aspects **graphically** which are also examined, at least partially, in the paper *(cf. sections III.C, III.D)*:  
* Does the outcome variable $\Delta f_i$ change discontinuously at the cutoff and only at the cutoff?  
* Does the treatment variable $T_i$ change discontinuously at the cutoff and only at the cutoff? How large is the jump?  
* Are there any other control variables changing discontinuously at the cutoff?  
* Is there any evidence that the values of the marginality index $I_i$ could have been manipulated by the localities?

For the first three of those points as a method to check they state (among other methods, as we will see later) bar plots showing the mean of the respective variable in a bin (of certain width) of the marginality index within a window around the cutoff. They recommend not averaging over values across the cutoff. This is why we only use plots with bins starting/ending at the cutoff. *Unfortunately, Alix-Garcia et al. only speak of the rounded cutoff $-1.2$ instead of the true cutoff $-1.22$ in their graphical analysis in section III.C of the paper.*

#< info "Remark on the answer to the third question"
In this exercise we will develop the answers to the first two questions as well as to the last question. As the development of the answer to the third question is quite demanding to implement in R, its answer will be just given in the current exercise. If you are interested in some details of R programming, you can solve the optional *Exercise 2.4* which will be dedicated to the development of this answer.
#>

As we need graphics from the *Exercises 1.1* and *1.2* for the current exercise, please first run the following code chunk to load those plots again. The code also loads the data set again. *Note that if you have not gone through those exercises, this is no problem at all because at this point it is not relevant how the plots were produced but only what is the result.*
```{r "2.3 1"}
#< task
#Load the data set "data_deforest.rds" and save it in data_deforest
data_deforest = readRDS("data_deforestation.rds")
#Load packages 'ggplot2' and 'dplyr'
library(ggplot2)
library(dplyr)
#Load the plots p_proptreated, hist_pov_ind and p_pctdefor
plots_1_1 = solve_1_1()
p_proptreated = plots_1_1$p_proptreated
hist_pov_ind = plots_1_1$hist_pov_ind
p_pctdefor = solve_1_2()
#>
```

### a) Discontinuity in deforestation
Let's start with the first question: Can we detect any **discontinuity in deforestation at the cutoff** in a simple bar plot?  
In *Exercise 1.2* we have already visualized the percentage of area deforested over bins of the marginality index of width $0.1$. 
Load the respective graphics again to be able to study it:  
*This corresponds to figure 3 on p.422 in the paper.*
```{r "2.3 a) 1"}
#< task
#Show the plot on the percentage of area deforested over the marginality index
#with a red vertical line indicating the eligibility cutoff
p_pctdefor
#>
```
Taking a closer look at the plot of the percentage of area deforested over the marginality index one might say that the dependent variable increases discontinuously at the eligibility cutoff but it is not convincing yet. Additionally, one cannot say that this "jump" sticks out of the whole curve and that there is no other discontinuous increase or decrease of that size. Unfortunately, Alix-Garcia et al. don't go into this at all.  
Zooming in a small range around the cutoff might help at least to show the discontinuity better. More precisely, we will focus on the range of the marginality index from $-2.0$ to $-0.2$. The variable $smallsample$ in the data set indicates if an observation is part of this range or not. In the following this subsample is called **"restricted sample"** like in the paper *(p.423)*.  
We are going to plot a finer histogram of average values of the outcome for the small sample first and then use a nonparametric regression approach to study the behavior of the outcome at the cutoff.

In the following, we use the restricted sample and a bin width of $0.01$ instead of $0.1$. This time, we will use the command *geom_bar(stat="identity",...)* which plots bars with the value of the $y$ variable. Apart from these aspects, the implementation is analogous to the one of *p_pctdefor*. As the way how to plot such a histogram shall not be central to this exercise (for this purpose see *Exercises 1.1* and *1.2*) the code is already given. Have a look at it and then click on the *'check' button* to run it. If you are interested in what the data frames look like, you can find them in the *'Data Explorer'*.
```{r "2.3 a) 2"}
#< task
#Extract the restricted sample from data_deforest
data_deforest_small = filter(data_deforest, smallsample==1)
#Add the column aux_bins_povind to data_deforest_small
data_deforest_small = mutate(data_deforest_small,
                             aux_bins_povind = round((pov_ind_95 - 0.005), 
                                                     digits = 2))
#Calculate the mean of pct_deforested for each value of aux_bins_povind
dat_pct_small = summarize(group_by(data_deforest_small, aux_bins_povind), 
                          mean_pctdeforested = mean(pct_deforested))
#Create the plot "mean_pctdeforested over bins_povind"
#and save it in p_pct_small
p_pctdefor_small = ggplot(data=dat_pct_small, 
                               aes(x=aux_bins_povind + 0.005, 
                                   y=mean_pctdeforested))+
  geom_bar(col="darkgreen", stat="identity") + 
  geom_vline(xintercept = -1.22, col="red") +
  xlab("bins_povind")
#Show the plot
p_pctdefor_small
#>
```
This graphic gives even less evidence for any jump at the cutoff. In contrary, the behavior of the dependent variable seems to be rather random.

A more advanced approach to study the behavior of the dependent variable in an area close to the cutoff is to use nonparametric regressions and apply them to the left and the right side of the cutoff like it is explained for example by Imbens and Lemieux *(2008, p.622)*. This is what we will do now - more precisely, we will run **kernel regressions for each side of the cutoff** like Alix-Garcia et al.. *(Cf. paper, p.423, figure 4)* For an introduction to kernel regressions, have a look at the *'info' box* below.  
*Please note that the graphics we will develop in the following corresponds to figure 4 on p.423 in the paper even though there is no code available to exactly reproduce this figure.*
#< info "Kernel regressions"
Assume the following general regression equation for some dependent variable $Y_i$ and explanatory variable $X_i$
$$Y_i = m(X_i) + \epsilon_i \text{   with } i=1,...,n$$
where $\epsilon_i$ is a random disturbance with conditional expectation $0$.
In contrast to parametric regressions nonparametric regressions - like for example kernel regressions - do not use parametric assumptions with respect to the functional form of $m(.)$. This function $m(x)$ which is the conditional expectation $E[Y|X_i=x]$ can be written as follows:
$$E[Y_i|X_i=x] = \frac{\int y f(x,y)\text{d}y}{\int f(x,y)\text{d}y}$$
The key idea of a **kernel regression estimator** is to "replace the numerator and denominator [of this equation] with estimators based on locally weighted averages". *(Blundell and Duncan (1998), p.66)*
Estimating the density $f(x,y)$ through a kernel density estimator ($\widehat{f(x, y)} = f_h(x,y) = \frac{1}{n} \sum_{i=1}^n K_h(x - X_i)K_h(y - Y_i)$), plugging it into the fraction and then simplifying the integrals in the formula leads to the formula of the **Nadaraya-Watson kernel regression estimator**:
$$\widehat{m_h^{NW}}(x)=\frac{\sum_{i=1}^n Y_i K_h(x - X_i)}{\sum_{i=1}^n K_h(X-x_i)}$$
Here, $K_h(u)$ is a kernel function like for example the uniform, Gauss or Epanechnikov kernel and $h$ is the bandwidth of the kernel which determines how smooth the estimated conditional mean function finally looks like.  
*(Cf. Hrdle and Linton (1994, Section 3); Blundell and Duncan (1998, Sections I-III))*

In R there are several functions which allow to estimate nonparametric regressions. There are even packages dedicated to special forms of nonparametric regression estimation like for example 'np' and 'locfit'. The package 'ggplot2' provides the command *geom_smooth* where the user can choose also nonparametric smoothing methods. In this problem set we use the command *ksmooth* (from the package *'stats'*) which calculates the Nadaraya-Watson kernel regression estimate.
#>
To do so, **subsamples of the data set** are needed. In the following code chunk please use the command *filter* from the package *'dplyr'* to create the data sets *data_deforest_small_left* and *data_deforest_small_right* which shall be subsamples of the restricted sample with *pov_ind_95 <= (-1.22)* or *pov_ind_95 > (-1.22)*, respectively. You can do this in analogy to the creation of the restricted sample data set *data_deforest_small* in the last code chunk. Information on *filter* is given in the *'info' box*. Further show the head of each data frame. To make those data extracts comparable, the head of the full restricted data set *data_deforest_small* is also shown.
#< info "Data manipulation with 'dplyr' - filter"
You have already got to know the commands *mutate*, *summarize* and *group_by* from the package *'dplyr'*. The command *filter(data, predicates)* with data being a data frame and predicates logical conditions combined with *&* filters the rows of the data frame for which the conditions are fulfilled.
#>

```{r "2.3 a) 3"}
#< task
#Create the subset for the small sample and on the left side of the cutoff
#and show the first six rows of it
#>
data_deforest_small_left =filter(data_deforest,
                                 smallsample==1, pov_ind_95<= (-1.22))
head(data_deforest_small_left)
#< task
#Create the subset for the small sample on the right side of the cutoff
#and show the first six rows of it
#>
data_deforest_small_right = filter(data_deforest, 
                                   smallsample==1, pov_ind_95>(-1.22))
head(data_deforest_small_right)
#< task
#Show the first six rows of data_deforest_small
head(data_deforest_small)
#>
```
One possibility to **estimate kernel regressions** in R is to use the function *ksmooth(x, y, kernel, bandwidth)* (from the package *'stats'*). It returns fitted values $x$ and $y$ from a *Nadaraya-Watson kernel regression* in a list. In the following code chunk, this kernel regression is estimated for the left and the right side of the cutoff separately. The column indicating the marginality index of the respective data set from above is used as input argument *x* and the column indicating the percentage of area deforested of the same data set is used as input argument *y*. Besides, a *"normal"* kernel and a bandwidth equal to $0.5$ are used. The resulting fitted values are called *kernelsmooth_left* and *kernelsmooth_right*, respectively. Finally, it is your turn to address an element of the list by showing the first six rows of the *x* values of *kernelsmooth_left*. You can do so by using the symbol *\$* - as if you addressed a column of a data frame and the command *head* which is also applicable to vectors.  
*Please note that the calculations will take a few seconds.*

```{r "2.3 a) 4"}
#< task_notest
#Run the kernel regression for data_deforest_small_left,
#pct_deforested over pov_ind_95
kernelsmooth_left = ksmooth(x=data_deforest_small_left$pov_ind_95,
                            y=data_deforest_small_left$pct_deforested,
                            kernel=c("normal"), bandwidth = 0.5)
#Run the same kernel regression for data_deforest_small_right
kernelsmooth_right = ksmooth(x=data_deforest_small_right$pov_ind_95,
                             y=data_deforest_small_right$pct_deforested,
                             kernel=c("normal"), bandwidth = 0.5)
#>
#< task
#Show the first six rows of the element x of kernelsmooth_left
#>
head(kernelsmooth_left$x)
```
Finally, we are going to **plot** the two returned sets of **fitted values**. For this purpose we use the package *'ggplot2'* again. After initializing the plot via *ggplot()* please add two blue lines - for the fitted values on the left and on the right side. Note that you cannot just hand over *kernelsmooth_left* and *_right* as input argument *data* and use *x* and *y* as aesthetics because *kernelsmooth_left* and *_right* are lists. So instead, you have to hand over the list elements *x* and *y* from the respective variable to the aesthetics.  
The code for setting some axes features as well as for adding a red vertical line is already given. For information on the according commands see the next *'info' box*.
#< info "'ggplot2': Some axes features"
* *coord_cartesian(xlim, ylim)* sets the limits on the axes.  
* *scale_y_continuous(breaks)* defines at which values of the scale the y axis shall be labeled; analogous for *scale_x_continuous(breaks)* for the x axis
* *ylab(label)* sets the label for the y axis; analogous for *xlab(label)* for the x axis
#>
```{r "2.3 a) 5"}
#< task
#Initialize the plot object and 
#add kernel regression fits for the left side and the right side
#>
p_deforest_kernel = ggplot() + 
  geom_line(aes(kernelsmooth_left$x, kernelsmooth_left$y), color="blue") + 
  geom_line(aes(kernelsmooth_right$x, kernelsmooth_right$y), color="blue")
#< task
#Add a red vertical line at -1.22 and set axes features
p_deforest_kernel = p_deforest_kernel +
  geom_vline(xintercept=-1.22, color="red") +
  coord_cartesian(xlim=c(-2,-0.2), ylim=c(0,0.1)) + 
  scale_y_continuous(breaks = seq(0,0.1,0.02))
#Show the plot
p_deforest_kernel
#>
```
In this kernel regression plot the **discontinuity becomes very clear**. The deforested area increases from about $0.03\%$ of the polygon area deforested on the left side of the cutoff to some value between $0.07\%$ and $0.08\%$ area deforested which is about two and a half times the value on the left of the cutoff.  
Unfortunately, the function *ksmooth()* does not allow to compute standard errors or confidence intervals. This is why I have calculated bootstrapped $95\%$ confidence intervals. As this takes some hours, I just present the final result here:

![Kernel regression with bootstrapped confidence intervals](Kernel_boot_small.png)

As you can clearly see from the picture, the confidence intervals from the regressions on the left and right side do not overlap - so the jump is significant at least on the $5\%$ level. This is consistent with the according plot by Alix-Garcia et al. for which the code for replication is not available. *(Cf. paper, p.423, Figure 4)*  
*Imbens and Lemieux (2008, p.622)* recommend not only testing whether the dependent variable shows a jump at the cutoff but also whether there are further jumps (at other values of the running variable) which might not be explainable. Unfortunately, Alix-Garcia et al. don't consider this aspect in their graphical analysis. However, later, when applying the RD Design, they also run regressions which use "wrong" values for the cutoff. These regressions don't show any significant treatment effects on deforestation. *(Cf. paper, p.425)*
#< info "Remark on graphical check"
I have applied different nonparametric methods on the restricted sample to check both the existence of a significant jump at the cutoff $-1.22$ and the existence of significant jumps at other (wrong) values of the cutoff. They show that the results on those tests are not as clear as presented in the paper but all in all don't pose a problem to the way the authors apply the RD Design in their paper.

**1.** Some results indicate that there is **no significant jump at the cutoff:**  
Applying the command *stat_smooth* from the package *'ggplot2'* with the arguments *method="loess", aes(x=pov_ind_95, y=pct_deforested), span=1, method.args=list(degree=1) or method.args=list(degree=2)* for the left and right side of the true cutoff $-1.22$ (for the restricted sample) leads to a plot which shows a jump at the cutoff. However, the jump is not significant on the $5\%$ level as the according $95\%$ confidence intervals overlap at the cutoff.  
The same can be observed with the command *locfit* from the package *'locfit'* and a Gaussian kernel. Here, the $95\%$ confidence intervals overlap as well.  
However, I doubt that this is representative because the confidence intervals seem to diverge at the axis limits of the regressions.  
**2.** There are indications that there are also **significant jumps at "wrong" cutoffs:**  
Applying the command *geom_smooth* from the package *'ggplot2'* with the method *gam* to "cutoff" values from $-1.8$ to $-0.4$ in steps of $0.2$ (instead of $-1.2$ I use $-1.22$) and additionally $-1.1$ and $-0.9$ leads to the following results: All plots show a jump at the used "cutoff" value. However, the corresponding $95\%$ confidence intervals do not overlap at the "cutoff" in all of the plots, but only for the values $-1.22$ and $-1.1$. This might result from the roughly continuous increase in a small interval from $-1.22$ to $-0.9$. If the authors only applied the standard RD Design, this could be critical but as they additionally use the more advanced approach of a fuzzy RD Design with a special set of instruments *(cf. paper, section III)* (see *Exercises 2.6* and *2.7*) this should not pose a problem for the validity of the results.
#>

### b) Discontinuity in treatment
To check whether **treatment changes discontinuously at the eligibility cutoff** a histogram showing the proportion of localities treated in a bin is helpful. We have already created such a plot in *Exercise 1.1* for the whole sample. Run the following code chunk to show it.  
*This corresponds to figure 3 on p.422.*
```{r "2.3 b) 1"}
#< task
#Show the proportion of localities treated over the marginality index
p_proptreated
#>
```
It is very obvious that **enrollment increases very quickly** at the cutoff from $0.2$ to about $0.8$. However, the increase takes place **over a small interval** of the marginality index on the right side of the cutoff. This is why we cannot allocate the setting clearly to the sharp or the fuzzy RD design. In *Exercises 2.6* and *2.7* we will deal with the problem of a continuous increase.  
A second aspect calling attention is **a second sharp increase** in a range of the marginality index between about $-2.5$ and $-2.0$. The authors attribute this mainly to localities enrolled in the program after 2000. *(Cf. paper, p.422)* For those localities, the eligibility rule was different. They mention that not excluding these localities could bias the results such that the effect probably won't be visible in the regressions but that at the same time, it strengthens the results if the effect is still visible. *(Cf. paper, p.421)*  
Finally, it is remarkable that **enrollment does not stay constantly high** or even increases to the right of the strong increase at the cutoff. In contrast, it decreases again for higher marginality indices. The authors explain this by the fact that the according localities might have been too poor thus not having enough infrastructure to be eligible. *(Cf. paper, p.422)*

All in all, the graphical analysis reveals that the setting deviates from the ideal RD design in terms of treatment. Nevertheless, the most important thing - the strong increase at the cutoff - exists, is much greater than other increases or decreases visible in the plot and the latter ones are explainable.

### c) Continuity of control variables at the cutoff
If apart from the variable indicating treatment (and the variable indicating eligibility, of course) there is another explanatory variable showing a discontinuity at the eligibility threshold or at least in the relevant range around it, one cannot be sure any longer if the jump in the outcome variable is due to the jump in the treatment variable. *(Cf. Imbens and Lemieux (2008); paper, p. 424)*
This is why one should study the behavior of the control variables at the cutoff. According to Alix-Garcia et al. there is actually only one variable **changing significantly discontinuously at the eligibility threshold: slope**. It is significantly higher among the eligible localities, so on the right-hand side of the cutoff. The authors meet this by using *slope* as control variable in each of their regressions. They don't see this discontinuity preventing them to use an RD design, but rather strengthening their results. *(Cf. paper, p.424)*   
By running the same kernel regression like in part *b)* of this exercise for each of the covariates (over the marginality index) this result can be reproduced. Without any confidence intervals, the plot of the lines of fitted values from the different kernel regressions looks as follows:

![Kernel regressions for all covariates](Covariates_kernel.png)

According to that plot, only two variables could pose a problem as they show a jump at the cutoff. To check whether those jumps are significant (on the $5\%$ level) I have calculated bootstrapped $95\%$ confidence intervals for the according kernel regressions. The following plot shows the two lines of fitted values with the confidence intervals:

![Kernel Regressions with confidence intervals of critical covariates](Kernel_degslopeAndForestprior.png)

As you can see from those two graphics, for the variable *forest_prior* the confidence levels from the left and right side overlap at the cutoff. This indicates that the jump at the cutoff is not significant on the $5\%$ level. So we cannot neglect on the five percent significance level the null hypothesis saying that there is no jump. For the variable *deg_slope* however, the confidence intervals do not overlap at the cutoff. This means that we can neglect the null hypothesis (no jump) on the $5\%$ significance level. All in all, this is consistent with the authors' statement depicted above.  
If you are interested in how one can run all the kernel regressions needed for the plots above at once and in how it is possible to get the first of the three plots above, you are invited to solve the optional *Exercise 2.4*.

### d) Distribution of the running variable
*Imbens and Lemieux (2008, p.623)* recommend to graphically check whether the **distribution of the running variable changes discontinuously at the cutoff**. Alix-Garcia et al. don't examine this in detail but they do remark that the empirical distribution of the marginality index is not a uniform distribution *(cf. paper, p.422)*: There are **much more observations for medium values of the marginality index** than for lower and higher values. This also becomes clear from the according histogram from *Exercise 1.1*. Run the following code chunk to see the plot again.
```{r "2.3 d) 1"}
#< task
#Show the empirical density of pov_ind_95
hist_pov_ind
#>
```
From this histogram one can also conclude that **at the cutoff**, there is **no discontinuous change in the density** of the running variable. So there is no evidence that the localities close to the cutoff might have manipulated their poverty rating in order to be eligible or not eligible.

- - -
So to **sum up** we can say the following: There are some critical aspects concerning the question whether the setting fulfills the conditions for the application of RD Designs. However, they could be met by the authors as depicted above. There is only one critical aspect left which has not been solved so far: the continuous increase in deforestation in a small area to the right of the cutoff instead of a clear discontinuity at the cutoff. You will see how this can be coped with in the scope of an RD Design (see *Exercises 2.6* and *2.7*). So we can say that the **RD design** seems to be **suitable**.


## Exercise [2.4] -- Optional: Visualizing the behavior of control variables in R 
In *Exercise 2.3* you have been given an answer to the question if there are any other control variables changing discontinuously at the cutoff based on given graphics. In this exercise we will make use of the power of R concerning **data preparations** and **plotting with** ***'ggplot2'*** to visualize the behavior of the control variables and thus produce one of those graphics.  
*Note that if you are mainly interested in the contents of the paper, you can skip this exercise and start with the regressions.*

In the following, we will focus on the restricted sample. Run the following code chunk to load the full data set again and to filter out the observations in a range from $-2.0$ to $-0.2$ of the marginality index. One cannot detect any difference between the two data sets by looking at the first six rows. However, looking at the last six rows does show a difference. Showing the tail of a data set *x* can be done through the command *tail(x)* which is done here for both data sets.
```{r 2.4 1}
#< task
#Read in the data and save it as data_deforest
data_deforest=readRDS("data_deforestation.rds")
#Load package 'dplyr'
library(dplyr)
#Create the small sample data set
data_deforest_small = filter(data_deforest, smallsample==1)
#Show the first six rows of each data frame
tail(data_deforest)
tail(data_deforest_small)
#>
```
In order to detect if any control variable shows a discontinuity at the cutoff, we will run **kernel regressions for all covariates** and **for each side** of the threshold and then plot the according fitted values.

### a) Kernel regressions
As you have learned in the last exercise, one single kernel regression can be run with the command *ksmooth(x,y)*.    
A great advantage of R is that it offers alternatives to simple loops for repeated computations. One possibility is to use the function *sapply(X, FUN, ... , simplify)*. You find further information on *sapply()* in the *'info' box*.
#< info "sapply()"
Given some function *FUN* with one or further arguments, the function *sapply(X, FUN, ..., simplify)* applies *FUN* to each element of *X*. If *FUN* needs more than one argument, further arguments can be handed over in *"..."*. The default return value is a list. This can be changed by using the *simplify* argument, e.g. *simplify = "array"* to return an array. In order to avoid partial matching it is recommended to use *X* and *FUN* as first arguments of *sapply()*.
#>
In our case, the *sapply* command makes it possible to run kernel regressions for all covariates "at once". However, as you have seen in the last exercise, for the purpose of showing possible jumps at the cutoff, separate regressions for the left and right side have to be run. So we will use the *sapply* command twice: Once for the left-hand side of the cutoff (for all covariates) and once for the right-hand side (for all covariates).

What about the function arguments?  
For the left and the right side, the function to apply several times is *ksmooth*. Unfortunately, we cannot just hand it over to *sapply()* as argument *FUN*  because the order of its arguments and its return value (a list) are not suitable for this purpose. In the next code chunk, a wrapping function, which can be used instead, is defined. It changes the order of the arguments and the return value is a matrix. Have a look at the code and run it.

```{r "2.4 a) 1", optional=TRUE}
#< task
#Definition of the wrapping function of ksmooth()
ksmooth_wrap = function(y, x, kernel){
  res = as.matrix(as.data.frame(ksmooth(x,y, kernel)))
  return(res)
}
#>
```
Apart from *FUN* we also need to hand over to *sapply()* the argument *X* consisting of all columns used as *y* in the function *ksmooth()* and the additional arguments *x* and *kernel* from the function *ksmooth()*.  
The last argument (*kernel*) is quite simple: We will use a Gaussian kernel here, so we set the argument *kernel* to *"normal"*.  
Providing the arguments *X* and *x* is somehow more complex:  
*X* shall be a data frame containing the covariates in the regressions later - the locality-specific characteristics probably influencing deforestation: forest_prior, ln_area_polyg, deg_slope, ln_pop, ln_dens_road.  
The argument *x* is the variable all covariates are regressed on. In our case this is the marginality index *pov_ind_95*. Both *X* and *x* differ for the left-hand and right-hand side, of course.  
We will obtain those arguments for the two sides in three steps:  
* First, we define an auxiliary data frame containing only the marginality index *pov_ind_95* and the covariates mentioned above from the whole restricted sample.  
* Second, we create subsets of the auxiliary data frame containing only observations belonging to the left-hand side or right-hand side of the cutoff, respectively.  
* Third, we disentangle the column containing the marginality index from the rest in order to obtain the input *x* and *X*.

For the first step we use the function *select(data, ...)* from the package *'dplyr'* in order to obtain the auxiliary data frame described above from the data set *data_deforest_small*. It is your task to do this in the next code chunk. If you are not used to the command *select*, have a look at the *'info' box* below. Please call the resulting data frame *data_aux* and show the first six rows of it.
#< info "Data manipulation with 'dplyr' - select"
The command *select(data,...)* from the package *'dplyr'* extracts or removes the columns stated as arguments in *...* (no quotation marks). The latter ones can be treated as if they were indices. A negative sign therefore stands for "removing", whereas a positive sign stands for "extracting".
#>
```{r "2.4 a) 2"}
#< task
#Define the data frame "data_aux"
#>
data_aux=select(data_deforest_small, pov_ind_95, forest_prior, 
                            ln_area_polyg, deg_slope, ln_pop, ln_dens_road)
#< task
#Show the first six rows of data_aux
#>
head(data_aux)
```
The implementation of the second step should be common to you: Filtering observations from a data set. So far, we have done this with the command *filter* from the package *'dplyr'*. However, there is another, a basic command which makes it also possible to extract a subset of a data frame: *subset(data, condition)*. It has the same syntax as the command *filter*. So the only arguments to pass are the data frame and the condition the subset has to fulfill. Use the command *subset* to create the data frames for the left-hand side (marginality index smaller than or equal to $-1.22$) and call it *data_aux_left*. The head of the data set is shown to give you an idea what it looks like.  
*Please note that the right-hand side will be treated later.*
```{r "2.4 a) 3"}
#< task
#Define the subset "data_aux_left"
#>
data_aux_left = subset(data_aux, pov_ind_95 <= -1.22)
#< task
#Show the first six rows of data_aux_left
head(data_aux_left)
#>
```
For the third step, use the simple *$* command to extract the column *pov_ind_95* from the data frame *data_aux_left* (*x_left*) and the command *select* to remove the column *pov_ind_95* from *data_aux_left* (*X_left*). The head of each object is shown here as well.
```{r "2.4 a) 4"}
#< task
#Define "x_left" as described above
#>
x_left = data_aux_left$pov_ind_95
#< task
#Define "X_left" as described above
#>
X_left = select(data_aux_left, -pov_ind_95)
#< task
#Show the head of x_left and X_left
head(x_left)
head(X_left)
#>
```
Now we have all the ingredients to use the function *sapply* for the left-hand side of the cutoff. In the following code chunk, please fill in the missing code and then uncomment it to finally obtain all fitted values for all kernel regressions on the left-hand side. Remember that the argument *simplify="array"* defines the return value. Don't worry - this might take some seconds.
```{r "2.4 a) 5"}
#< task
#res_left = sapply(X = ..., FUN = ..., 
#                  x = ..., 
#                  kernel = ..., simplify = "array")
#>
res_left = sapply(X = X_left, FUN = ksmooth_wrap, 
                  x = x_left, 
                  kernel = "normal", simplify = "array")

```
The code for the right-hand side is analogous. As much more observations are involved in the calculations here, it takes a very long time. This is why we won't run it but just load the final result: the fitted values in the array *res_right*. Just click on the *'check' button* to load it.
```{r "2.4 a) 6"}
#< task
#Load the array of fitted values "res_right"
res_right=readRDS("res_right.Rds")
#>
```
Now we have all the ingredients (except the confidence intervals which are not computable within the frame of this problem set) to check whether the covariates show jumps at the cutoff. A very nice way to finally do so is to use a graphical approach.

### b) Visualizing the fitted values from the kernel regressions
In this part of the exercise we are now going to visualize the given fitted values from the regressions in a plot with multiple lines in different colors - each color standing for one covariate used as argument *y* in the kernel regression.

With the package *'ggplot2'* this can be achieved in the **following steps:**  
* First, the data set containing the points to be plotted has to be transformed into a **"long" format**. In our case this means that there are not separate columns for each line which shall be plotted containing the according *x* and *y* values. Instead, the *x* and *y* values of all lines to be plotted are given in one single pair of columns. An extra column indicates to which line - in our case defined by the choice of the covariate *y* - the value belongs.  
* Second, two arguments have to be added within the **aesthetics command** of the command *geom_line*: *group* and *color*. The first one indicates the column of the data frame defining to which line (to be plotted) a row in the data frame belongs. The second one indicates the column of the data frame defining which rows of the data frame shall be plotted in the same color. In our case, both arguments indicate the same column. This way we get a line plot for each covariate *y* in another color.

Let's start with the **first step:**  
The kernel regression results *res_left* and *res_right* calculated above are given in an array with three dimensions: The first dimension indicates the observation, the second indicates the coordinate and the third indicates the covariate used as *y* in the kernel regression. In order to be able to plot the fitted values of the kernel regression for each covariate over the marginality index with the *'ggplot2'* package, they therefore have to be prepared and finally written into a long data format.  
A great part of the way to this goal can be achieved through the command *melt(data, varnames)* from the package *'reshape2'*. Some general information on this command is given in the *'info' box* below. If it is applied to *res_left* (or *res_right*) as argument *data*, it creates a data frame with one column containing the fitted values from all kernel regressions, one column indicating whether a value in the previously mentioned column belongs to the *x* or the *y* coordinates of the fitted values from the kernel regression, one column indicating to which kernel regression (given by the name of the covariate) it belongs and one column just indicating the row number (observation number).  
In the following code chunk, please apply the command both to *res_left* and to *res_right* and use the argument *varnames* to name the columns of the produced data frame as *"Observation", "coordinate", "covariate"* (use a vector of names). Call the resulting data frames *res_melt_left* and *res_melt_right*. Finally, the first six rows of each data frame are shown.
#< info "melt"
The command *melt(data, varnames)* from the package *'reshape2'* can be applied to data frames, arrays, tables, matrices and lists. If the argument *data* to which it is applied is an array as in our case, it converts the original array into a data frame: All values originally distributed over several dimensions are then given in one single column. The according coordinates are given by the values in the rest of the columns representing the dimensions of the array. The resulting data frame is thus given in a *long format*. The vector of names given in *varnames* is used as the column names of the new data frame.
#>
```{r "2.4 b) 1"}
#< task
#Load package 'reshape2'
library(reshape2)
#Apply the function melt to res_left and 
#call the resulting data frame res_melt_left
#>
res_melt_left = melt(res_left, 
                     varnames = c("Observation", "coordinate", "covariate"))
#< task
#Apply the function melt to res_right and 
#call the resulting data frame res_melt_right
#>
res_melt_right = melt(res_right, 
                      varnames = c("Observation", "coordinate", "covariate"))
#< task
#Show the first six rows of res_melt_left and of res_melt_right
head(res_melt_left)
head(res_melt_right)
#>
```
Still, the data frames don't have the desired format. Instead of one column containing all fitted values we need two columns - one containing all fitted *x* values, one containing all fitted *y* values. In the next code chunk this is achieved in a very simple way: First, the rows of the data frame containing fitted *x* values (and separately, the fitted *y* values) are extracted and saved as auxiliary data frames. Then a new data frame is created which contains the value columns of those two help data frames and a column indicating to which kernel regression (which *y* covariate) they belong. As quite a lot of data frames are created here, for reasons of clarity I don't want to show the head of each one in the slope of this exercise. I thus recommend to you switching to the *'Data Explorer'* where you can find and have a look at all of them.
#< info "Remark on the column covariate in the final data frame"
Please note that the command *melt* converts the data frame such that it goes through the values for different coordinates of the first dimension, holding all other dimensions fixed. Then the second dimension coordinate is increased by one step and again, all values for the different coordinates of the first dimension are run through and so on. This way, the column "covariate" from one of the auxiliary data frames is valid for both of them and can be used in the final data frame.
#>

```{r "2.4 b) 2"}
#< task
#Data preparation for the final data frame
#... for the left-hand side
res_melt_left_x = filter(res_melt_left, coordinate=="x")
res_melt_left_y = filter(res_melt_left, coordinate=="y")
res_melt_left_fin = data.frame(covariate=res_melt_left_x[,"covariate"],
                               value.x = res_melt_left_x[,"value"],
                               value.y = res_melt_left_y[,"value"])
#... for the right-hand side
res_melt_right_x = filter(res_melt_right, coordinate=="x")
res_melt_right_y = filter(res_melt_right, coordinate=="y")
res_melt_right_fin = data.frame(covariate=res_melt_right_x[,"covariate"],
                                value.x = res_melt_right_x[,"value"],
                                value.y = res_melt_right_y[,"value"])
#>
```
Now, based on the final data frames *res_melt_left_fin* and *res_melt_right_fin* we can finally come to the **second step:** plotting the fitted values. For this purpose, please follow the steps in the next code chunk and fill in the correct code.
```{r "2.4 b) 3"}
#< task
#Load package 'ggplot2'
library(ggplot2)
#Plot the fitted values of the kernel regressions
#First initialize a new plot and call it p_kern
#>
p_kern = ggplot()
#< task
#Add lines of the fitted values of the kernel regressions
#for all covariates on the left-hand side and the right-hand side
# - one line and one color for each covariate
#p_kern = p_kern+geom_...(data=..., aes(x=..., y=..., group=..., color=...))
#               +geom_...(data=..., aes(x=..., y=..., group=..., color=...))
#>
p_kern = p_kern +
  geom_line(data=res_melt_left_fin,
            aes(x=value.x,y=value.y, group = covariate, color=covariate)) +
  geom_line(data=res_melt_right_fin, 
            aes(x=value.x, y=value.y, group = covariate, color=covariate))
#< task
#Add a red vertical line at -1.22 and define the coordinates and labels
#Finally show the plot
p_kern = p_kern +
  geom_vline(xintercept= -1.22, color="red") +
  coord_cartesian(xlim=c(-2, -0.2)) + xlab("marginality index") + 
  ylab("covariate") + ggtitle("Covariates over marginality index")
p_kern
#>
```
This is finally the plot which I have presented to you in *Exercise 2.3*. Together with the two plots showing confidence levels of the kernel regressions for *deg_slope* and *forest_prior* it is in line with the authors' statement that only the variable *deg_slope* jumps at the cutoff. Just for completeness, I present you the two graphics again which we cannot reproduce throughout this problem set for the reason of computational time.* 

![Kernel Regressions with confidence intervals of critical covariates](Kernel_degslopeAndForestprior.png)

- - -
So to **sum up:** What have you learned in this optional exercise?  
* How to apply a function to several variables at once (*sapply*)  
* How to select columns of a data frame - a further command of data manipulation (*select*)  
* How to convert data into a long format (*melt*)
* How to elegantly plot several lines (of different colors) in one single graphic (*'ggplot2'*: *group*, *color*)

## Exercise [Q3] -- QUIZ on Exercises 2.3 and 2.4
Before starting to apply the Regression Discontinuity Design you are invited to check whether you have understood the main idea of this method and whether you know if it is applicable in the context of this problem set.

- - -
#< quiz "Quiz Number 3.1"
parts:
  - question: 1. Which of the following aspects would mean that the discontinuity in the dependent variable measuring the percentage of area deforested at the eligibility cutoff could not be attributed to treatment (alone) meaning that the RD Design would lead to unreliable results? Several answers might be correct.
    choices:
        - further covariates show a jump at the cutoff*
        - treatment does not show a jump at the cutoff*
        - the jump of treatment is smaller than 100\%
        - there are further jumps of the dependent variable at other values of the marginality index*
        - the marginality index of the localities around the cutoff seems to be manipulated*
    multiple: TRUE
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at the introductory part of *Exercise 2.3* again.
  - question: 2. What is the great advantage of nonparametric methods?
    choices:
        - They don't make use of assumptions on the functional form.*
        - The computational time is always very small.
    multiple: FALSE
    success: Great! This is correct! Nonparametric methods don't use parametric assumptions on the functional form which makes them very flexible.
    failure: Try again. It might also help you to have a look at the *'info' box* on kernel regressions in *Exercise 2.3* again.
#>

#< award "RD specialist!"
Conditions for the applicability of RD designs are no problem for you.  
You know them in theory, applied to this problem set's issue and  
you know methods how to check them. Well done!
#>

- - -

Please note that the second part of the quiz is *optional* because it treats contents from the optional *Exercise [2.4]*. So if you have solved that exercise and are curious whether you still remember something, go on with those questions. If not, just skip them and have a look at the next exercise. It's all up to you.

- - -

#< quiz "Quiz Number 3.2"
parts:
  - question: 3. In order to apply a function to various variables, which command can be used? Please don't state its arguments and don't use brackets.
    answer: sapply
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise [2.4] a)* again.
  - question: 4. Which data format should usually be used in order to apply the argument *group* for plotting several lines in one graph with the package *'ggplot2'*?
    choices:
        - wide format
        - long format*
    multiple: FALSE
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise [2.4] b)* again.
#>

#< award "R specialist!"
You have dug deeper into R programming in the optional exercise  
and you won't regret it! It is really worth it to become acquaint with  
the vast opportunities this flexible programming language offers!
#>

## Exercise 2.5 -- Sharp RD design using eligibility as proxy variable
Assume for a moment that enrollment in the program does not increase over the length of an interval of the marginality index, but jump from $0$ to $1$ at the cutoff like the variable *eligible* indicating eligibility.  
This means that treatment is perfectly predicted by the eligibility rule and thus determined by the marginality index. Thus, the treatment effect can be measured by a **Sharp RD design** using the variable *eligible* as a proxy for the treatment variable.  
Alix-Garcia et al. use this approach, which they call **"simple approach"** *(paper, p.423)*, in a first step even though the discontinuity is not sharp (compare *Exercise 2.3*). *(Cf. sections III.D, III.E, table 2)* Within this approach the measured effect is not the true **treatment effect** but rather the **"intention-to-treat effect"** *(paper, p.423)*.
- - -
In general, in a sharp RD design the effect of some treatment on an outcome $y$ can be estimated by the coefficient estimate $\hat{\delta}$ from the following regression equation:
$$y = \alpha + \delta D + \gamma a + \epsilon$$
with $D$ a binary variable indicating treatment, $\epsilon$ the error term and $a$ the running variable.  
*(Cf. Angrist and Pischke (2015, Chapter 4))*
- - -
In our case, the outcome is the percentage of area deforested $\Delta f_i$, as a proxy for treatment we use the dummy variable $E_i$ indicating eligibility and the running variable is here the marginality index $I_i$. The regression equation we will use in this exercise in analogy to Alix-Garcia et al. finally deviates a bit from the one above as the authors consider further covariates $\vec{x_i}$ probably influencing deforestation *(cf. paper, section III.D)*:
$$\Delta f_i = \alpha + \delta E_i + \gamma I_i + \vec{\beta}' \vec{x_i} + \epsilon_i$$
Please call to mind that the dependent variable $\Delta f_i$ can only take on positive values. So the regression equation is to be estimated by Tobit and not by OLS (for a remark on this see the following *'info' box*).
#< info "Remark on the functional form"
To be precise, one should actually use the help variable $\Delta f_i ^*$ instead of $\Delta f_i$ in the regression equation above and define $\Delta f_i = max(0,\Delta f_i ^*)$. However, to be consistent with the paper, I will stick to the formula above meaning actually the one just described.
#>

Now, let's run the first regression of the simple approach - a Tobit estimation of the above regression equation using the covariates introduced in the last exercises: *forest_prior*, *ln_area_polyg*, *deg_slope*, *ln_pop*, *ln_dens_road*, *ecoreg_control_1*, *ecoreg_control_2* and *ecoreg_control_3*. The first line of the code chunk loads the data set on deforestation. Have a look at the code and then click on the *'check' button* to run it.  
*This corresponds to table 2, column (1) on p.425 in the paper.*
```{r "2.5 1"}
#< task
#Read in the data and save it as data_deforest
data_deforest = readRDS("data_deforestation.rds")
#Load package 'AER'
library(AER)
#Tobit regression "sharp_RD_simple" of the simple approach
sharp_RD_simple = tobit(pct_deforested ~ eligible + pov_ind_95 + 
                          forest_prior + ln_area_polyg + deg_slope + ln_pop +
                          ln_dens_road + ecoreg_control_1 + ecoreg_control_2 +
                          ecoreg_control_3, data=data_deforest)
#Load package 'regtools'
library(regtools)
#Show the regression results
showreg(list(sharp_RD_simple), custom.model.names=c("Sharp RD simple"), 
        digits=3, include.aic=FALSE, include.bic=FALSE, include.loglik=FALSE,
        stars=c(0.01,0.05,0.1))
#>
```
The most surprising result is obviously the sign of the coefficient of *eligible* which measures the **intention-to-treat effect**: It is **positive and significant** which is consistent with the hypothesis that the results obtained without using the RD design are biased.  
As you can see, the choice of **covariates** seems to be very sensible because most of the according coefficient estimates are significant. The significant covariate coefficient estimates all show the signs expected according to *Exercise 2.2* except road density. This factor was expected to increase the pressure on forests, but the significant estimate in this regression has a negative sign.  
As we know from the last exercises, we cannot draw more information from those coefficients than significance level and sign. For the magnitude of the according effects, we need to calculate the marginal effects. Click on the *'check' button* to do so in the next code chunk.  
*This corresponds to table 2, column (1) on p.425 in the paper.*
```{r "2.5 2", results="asis"}
#< task
#Load package 'car'
library(car)
#Marginal effects
sharp_simple_marge = margEff_tobit_dM_AER(sharp_RD_simple, data_deforest)
#Load package 'stargazer'
library(stargazer)
#Show marginal effects
stargazer(convert(sharp_simple_marge), type="html", summary=FALSE)
#>
```
The first row in this table of results reports the marginal effects on the probability of deforestation (left side) and on the expectation of the percentage of area deforested given there is deforestation (right side) for the eligibility dummy. So according to the model above, the intention to treat **increases the probability of deforestation** by about $1.1$ percentage points and the **expected relative amount of deforestation** by $0.056$ percentage points. The latter seems to be very small, but call to mind the deforestation rate at the left side of the cutoff which we visualized in *Exercise 2.3*: It is about $0.03 \%$. In the light of this small value, the estimated effect is quite high.  
I don't want to go into detail concerning the marginal effects of the covariates as they are rather a means to an end. But it is interesting to compare the influence of eligibility (and thus of the intention to treat) on deforestation with the influence of the covariates. The function *effects_tobit_dM_AER* calculates the effect of all explanatory variables of a Tobit regression estimated with the *tobit* command from the package *'AER'*. The following *'info' box* gives you a short introduction to the function and provides the commented code.

#< info "effects_tobit_dM_AER(tobit, data, p_lower, p_upper) - Introduction and Code"
The function *effects_tobit_dM_AER* is similar to the function *effectplot* from the package *'regtools'* (which you will get to know later). It calculates the effect of each explanatory variable on the two quantities for which we calculate marginal effects, $P(y>0|x)$ and $E[y|x,y>0]$, if the explanatory variable increases from some lower quantile to some higher quantile (or - in the case of a binary variable - from $0$ to $1$). This way, the range of values of the variable is taken into account and the effects of different variables become comparable. For the case of a continuous variable the respective quantiles are plugged into the formulas for $E[y|y>0,x]$ and $P(y>0|x)$ (see *Exercise 2.1*) and the difference is calculated. For the case of a binary variable the effect equals the marginal effect of the binary variable. Please note that for logarithmized variables (in our case always starting with "ln") the effect of the underlying (not logarithmized) variable is calculated.  
The implementation is analogous to the implementation of the marginal effects. So like in the case of the function computing the marginal effects in the Tobit model the return value here is a list with two elements: One data frame containing the effects on $P(y>0|x)$ and one containing those on $E[y|y>0,x]$. Again, the effects, their standard errors, t and p values and the significance levels are calculated by the delta method. If you are interested in further details, have a look at the commented code below.

```{r "info code tobitME"}
#This function estimates the effects of each explanatory variable
#[when changing from its p_lower quantile to its p_upper quantile
#(or from 0 to 1)]
#on P(y>0|x) and E[y|x,y>0] for the Tobit model.

#input: - Tobit model estimated by command "tobit" from package 'AER'
#       - corresponding data frame
#       - probability for lower quantile
#       - probability for upper quantile
effects_tobit_dM_AER = function(tobit,data, p_lower, p_upper){
  #load package 'car' for later use of delta method
  library(car)
  
  #extract estimates from estimated Tobit model for coefficients and
  #for the logarithmized variance of the error term epsilon
  tobit_coef = summary(tobit)$coef[,1]
  
  #define number of estimates
  n = length(tobit_coef)
  
  #extract estimates of the coefficients {beta_j} from Tobit
  tobit_beta = tobit_coef[-n]
  
  #concerning all estimates (tobit_coef):
  #rename "intercept" "beta0" and "log(Sigma)" "lnsigma"
  #save renamed coefficients in renamedcoefs
  renamedcoefs = tobit_coef
  names(renamedcoefs)[c(1,n)] = c("beta0", "lnsigma")
  
  #extract explanatory variables from data
  cols = names(tobit_beta)[-1]
  X = data[,cols]
  #calculate the mean of all explanatory variables and name them appropriately
  if ( length(cols)==1){
    X_mean=mean(X)
  } else{
    X_mean=colMeans(X, na.rm=TRUE)
  }
  names(X_mean) = paste(cols, "mean", sep="")
  
  #determine binary variables and continuous variables
  firstfound = FALSE
  for (i in 1:(n-2)) {
    if (length(cols)==1){
      num_contained = unique(X)
    } else{
      num_contained = unique(X[,i])
    }
    if (length(num_contained)==2){
      if ((num_contained[1]==0 & num_contained[2]==1) | (num_contained[1]==1 & num_contained[2]==0)){
        if (firstfound==FALSE){
          firstfound = TRUE
          bin = c(i)
        } else{
          bin = c(bin, i)
        }
      }
    }
    if (length(num_contained)==1){
      if (num_contained==0 | num_contained==1){
        if (firstfound==FALSE){
          firstfound = TRUE
          bin = c(i)
        } else{
          bin = c(bin, i)
        }
      }
    }
  }
  
  conti = seq(1:(n-2))[-bin]
  
  #for each type of effect:
  #- define the formula as a string
  #- calculate estimate of effect and standard error with delta method
  #- add t value, corresponding p value and significance level
  #  to resulting data frame
  #- name the resulting data frame
  
  #effects for continuous variables
  firstconti = FALSE
  for (i in conti){
    #quantiles --> different if variable is logarithmized
    if(any(grep("ln",cols[i]))==FALSE){
      q_l = quantile(data[,cols[i]],p=p_lower)
      q_u = quantile(data[,cols[i]],p=p_upper)
      
    } else{
      q_l_real = quantile(exp(data[,cols[i]]),p=p_lower)
      q_l = log(q_l_real)
      q_u_real = quantile(exp(data[,cols[i]]),p=p_upper)
      q_u = log(q_u_real)
    }
    #according expl. variables values
    X_mean_conti_upper = X_mean
    X_mean_conti_upper[i] = q_u
    names(X_mean_conti_upper) = paste0(names(X_mean_conti_upper), "upper")
    X_mean_conti_lower = X_mean
    X_mean_conti_lower[i] = q_l
    names(X_mean_conti_lower) = paste0(names(X_mean_conti_lower), "lower")
    
    #effect on P(y>0|x)
    formula_P_conti_i = paste("pnorm((beta0 + ", paste(cols, names(X_mean_conti_upper), sep="*", collapse= " + ") ,
                              ")/exp(lnsigma))", " - ", 
                              "pnorm((beta0 + ", paste(cols, names(X_mean_conti_lower), sep="*", collapse= " + "), 
                              ")/exp(lnsigma))", sep="")
    Eff_P_conti_i = deltaMethod(object = renamedcoefs, vcov. = vcov(tobit),
                                g = formula_P_conti_i, constants = c(X_mean_conti_upper, X_mean_conti_lower))    
    Eff_P_conti_i = mutate(Eff_P_conti_i, tvalue = Estimate/SE, 
                           pvalue = 2*pt(-abs(tvalue), nrow(data) - (n-1)),
                           significance = as.factor(ifelse(pvalue<0.1, 
                                                           ifelse(pvalue<0.05,
                                                                  ifelse(pvalue<0.01, "***" , "**") , "*"), "")))
    rownames(Eff_P_conti_i)[1] = cols[i]
    colnames(Eff_P_conti_i)[1] = "Estimate P(y>0)"
    
    #effect on E[y|x,y>0]
    names(q_l)="q_l"
    names(q_u)="q_u"
    formula_E_conti_i = paste("(q_u - q_l)*", cols[i], " + exp(lnsigma) * (dnorm((beta0 + ", 
                              paste(cols, names(X_mean_conti_upper), sep="*", collapse= " + ") ,
                              ")/exp(lnsigma))/pnorm((beta0 + ",
                              paste(cols, names(X_mean_conti_upper), sep="*", collapse= " + ") ,
                              ")/exp(lnsigma)))", " - ",
                              "exp(lnsigma) * (dnorm((beta0 + ",
                              paste(cols, names(X_mean_conti_lower), sep="*", collapse= " + ") ,
                              ")/exp(lnsigma))/pnorm((beta0 + ",
                              paste(cols, names(X_mean_conti_lower), sep="*", collapse= " + ") ,
                              " - ", cols[i], ")/exp(lnsigma)))", sep="")
    Eff_E_conti_i = deltaMethod(object = renamedcoefs, vcov. = vcov(tobit),
                                g = formula_E_conti_i, 
                                constants = c(X_mean_conti_upper, X_mean_conti_lower, q_l, q_u))
    Eff_E_conti_i = mutate(Eff_E_conti_i, tvalue = Estimate/SE, 
                           pvalue = 2*pt(-abs(tvalue), nrow(data) - (n-1)),
                           significance = as.factor(ifelse(pvalue<0.1, 
                                                           ifelse(pvalue<0.05,
                                                                  ifelse(pvalue<0.01, "***" , "**") , "*"), "")))
    rownames(Eff_E_conti_i)[1] = cols[i]
    colnames(Eff_E_conti_i)[1] = "Estimate E[y|y>0]"
    
    #Save results in data frames for each kind of effect
    #of all continous variables
    if (firstconti==FALSE){
      firstconti = TRUE
      Eff_P_conti = Eff_P_conti_i
      Eff_E_conti = Eff_E_conti_i
    } else{
      Eff_P_conti = rbind(Eff_P_conti, Eff_P_conti_i)
      Eff_E_conti = rbind(Eff_E_conti, Eff_E_conti_i)
    }
  }
  
  #effect for binary variables equal the according marginal effects
  #marginal effects for binary variables
  firstbin = FALSE
  for (i in bin){
    #adapt the mean of the explanatory variables for the calculations below
    #and save them in X_mean_bin
    X_mean_bin = X_mean
    X_mean_bin[i] = 1
    
    #marginal effect on P(y>0|x)
    formula_P_bin_i = paste("pnorm((beta0 + ", paste(cols, names(X_mean_bin), sep="*", collapse= " + ") ,
                            ")/exp(lnsigma))", " - ", 
                            "pnorm((beta0 + ", paste(cols, names(X_mean_bin), sep="*", collapse= " + "), 
                            " - ", cols[i] , ")/exp(lnsigma))", sep="")
    margEff_P_bin_i = deltaMethod(object = renamedcoefs, vcov. = vcov(tobit),
                                  g = formula_P_bin_i, constants = X_mean_bin)
    margEff_P_bin_i = mutate(margEff_P_bin_i, tvalue = Estimate/SE, 
                             pvalue = 2*pt(-abs(tvalue), nrow(data) - (n-1)),
                             significance = as.factor(ifelse(pvalue<0.1, 
                                                             ifelse(pvalue<0.05,
                                                                    ifelse(pvalue<0.01, "***" , "**") , "*"), "")))
    rownames(margEff_P_bin_i)[1] = cols[i]
    colnames(margEff_P_bin_i)[1] = "Estimate P(y>0)"
    
    #marginal effect on E[y|x,y>0]
    formula_E_bin_i = paste(cols[i], " + exp(lnsigma) * (dnorm((beta0 + ", 
                            paste(cols, names(X_mean_bin), sep="*", collapse= " + ") ,
                            ")/exp(lnsigma))/pnorm((beta0 + ",
                            paste(cols, names(X_mean_bin), sep="*", collapse= " + ") ,
                            ")/exp(lnsigma)))", " - ",
                            "exp(lnsigma) * (dnorm((beta0 + ",
                            paste(cols, names(X_mean_bin), sep="*", collapse= " + ") ,
                            " - ", cols[i], ")/exp(lnsigma))/pnorm((beta0 + ",
                            paste(cols, names(X_mean_bin), sep="*", collapse= " + ") ,
                            " - ", cols[i], ")/exp(lnsigma)))", sep="")
    margEff_E_bin_i = deltaMethod(object = renamedcoefs, vcov. = vcov(tobit),
                                  g = formula_E_bin_i, constants = X_mean_bin)
    margEff_E_bin_i = mutate(margEff_E_bin_i, tvalue = Estimate/SE, 
                             pvalue = 2*pt(-abs(tvalue), nrow(data) - (n-1)),
                             significance = as.factor(ifelse(pvalue<0.1, 
                                                             ifelse(pvalue<0.05,
                                                                    ifelse(pvalue<0.01, "***" , "**") , "*"), "")))
    rownames(margEff_E_bin_i)[1] = cols[i]
    colnames(margEff_E_bin_i)[1] = "Estimate E[y|y>0]"
    
    #Save results in data frames for each kind of marginal effect
    #of all continous variables
    if (firstbin==FALSE){
      firstbin = TRUE
      Eff_P_bin = margEff_P_bin_i
      Eff_E_bin = margEff_E_bin_i
    } else{
      Eff_P_bin = rbind(Eff_P_bin, margEff_P_bin_i)
      Eff_E_bin = rbind(Eff_E_bin, margEff_E_bin_i)
    }
    
  }
  
  #Create the data frame or warning to return
  if (firstbin ==TRUE) {
    if (firstconti == TRUE){
      return (list(Eff_P = rbind(Eff_P_bin, Eff_P_conti),
                   Eff_E = rbind(Eff_E_bin, Eff_E_conti)))
    } else {
      return (list(Eff_P = Eff_P_bin,
                   Eff_E = Eff_E_bin))
    }
  } else{
    if (firstconti == TRUE){
      return (list(Eff_P = Eff_P_conti,
                   Eff_E = Eff_E_conti))
    } else {
      return ("Error: No effects available.")
    }
  }
}
```
#>
In the next code chunk please use this function to calculate the **effects of the explanatory variables** in the model *sharp_RD_simple* for changes of the continuous variables from the $10\%$ quantile to the $90\%$ quantile. Assign the result to the variable *sharp_simple_effects*. Save the first list element of *sharp_simple_effects* in the data frame *sharp_simple_effects_P* and the second one in *sharp_simple_effects_E*. The syntax of the command is as follows: *effects_tobit_dM_AER(tobit, data, p_lower, p_upper)* with *tobit* the name of the Tobit regression, *data* the data used in that regression and *p_lower* and *p_upper* the quantiles described above.
```{r "2.5 3"}
#< task
#Calculate the effects for sharp_RD_simple
#>
sharp_simple_effects = effects_tobit_dM_AER(sharp_RD_simple, data_deforest,
                                            p_lower = 0.1, p_upper = 0.9)
#< task
#Extract the data frames (list elements) and 
#save them in sharp_simple_effects_P and sharp_simple_effects_E
#>
sharp_simple_effects_P = sharp_simple_effects[[1]]
sharp_simple_effects_E = sharp_simple_effects[[2]]
```
If you would like to see the structure of the output of the function *effects_tobit_dM_AER*, precise numbers of the effects or the according significance levels just click on the *'data' button* where you find the respective data frames you have just created.  
As rather the rough comparison of the effects than precise numbers are of interest at this point, bar plots showing these effects might be very helpful. The following code chunk uses the command *geom_bar* to create a bar plot for each kind of effect. Have a look at the code and run it. Some information on the used commands and arguments are given in the *'info' box* below.
```{r "2.5 4", fig.height=4, fig.width=11}
#< task
#Load package 'ggplot2'
library(ggplot2)
#Create a bar plot of the effects on P(y>0|x) for sharp_RD_simple
p_sharp_simple_effectP = ggplot() + 
  geom_bar(aes(x=row.names(sharp_simple_effects[[1]]), 
               y=sharp_simple_effects[[1]][,1]), 
           stat="identity", position="dodge") + 
  xlab("explanatory variables") + ylab("Effect on P(y>0|x)") +
  coord_flip()
#Create a bar plot of the effects on E[y|y>0,x] for sharp_RD_simple
p_sharp_simple_effectE = ggplot() + 
  geom_bar(aes(x=row.names(sharp_simple_effects[[2]]), 
               y=sharp_simple_effects[[2]][,1]), 
           stat="identity", position="dodge") + 
  xlab("explanatory variables") + ylab("Effect on E[y|y>0,x]") +
  coord_flip()

#Load package 'gridExtra'
library(gridExtra)
#Show both plots
grid.arrange(p_sharp_simple_effectP, p_sharp_simple_effectE, ncol=2)
#>
```
#< info "'ggplot2' - geom_bar and coord_flip"
The command *geom_bar(aes(x,y), stat="identity", position="dodge")* creates a bar plot. As the argument *stat* is set to *"identity"* the $y$ aesthetic can be used. The argument *position="dodge"* avoids plotting several bars above each other and instead plots them next to each other.  
The command *coord_flip* is responsible for the bars to be horizontal as it swaps the two axes.
#>
As you can see, the effect of the intention to treat on $P(y>0|x)$ is not that large compared to the other covariates. It is very similar in absolute size to the effect of road density, population and slope. The effect of the intention to treat on $E[y|y>0,x]$ is even very small compared to the effects of the covariates. Still there are some covariate effects of more or less the same absolute magnitude: slope, road density and the forest baseline.

### a) Tobit specifications for robustness
In general, there is one main **risk** when applying the RD design: It might happen that the estimates indicate a treatment effect different from zero even though the real functional relationship is a polynomial function with a very steep slope at the cutoff.  
To avoid this misinterpretation there are two optional approaches: 
* Including **higher orders of the running variable** as regressors to model a possible nonlinear relationship of the dependent variable and the running variable
* Using a **restricted sample** in which only observations in a smaller area around the cutoff are considered (and don't include higher orders of the running variable)

*(Cf. Angrist and Pischke (2015, Chapter 4); paper, p.424)*

Alix-Garcia et al. use both options *(cf. paper, sections III.D, III.E, table 2)*: In the first option they consider a fourth-order polynomial of the marginality index $I$. In the second option they use the restricted sample you have got to know already in the last chapters and which covers the range of the marginality index from $-2.0$ to $-0.2$.

It is your turn now to run those two specifications for the simple approach above. Run a regression called *sharp_RD_fourth* which differs from *sharp_RD_simple* by considering additionally the marginality index to the power of two, three and four which are given in the data explicitly as *pov_ind_2*, *pov_ind_3* and *pov_ind_4*. Run a second regression called *sharp_RD_restricted* which differs from *sharp_RD_simple* by using the small data set. The latter data set as well as the presentation of the results are already given in the code chunk. The results of the first regression as well as of the two specifications here will be shown. *Note that you can just uncomment the given code (which is the one of the regression sharp_RD_simple) and adapt it by filling in the missing parts in "...".*  
*This corresponds to table 2, columns (2) and (3) on p.425 in the paper.*
```{r "2.5 a) 1"}
#< task
#Load package 'dplyr'
library(dplyr)
#Create the small sample data set
data_deforest_small = filter(data_deforest, smallsample==1)

#Adapt the following code for the Tobit regression 
#including higher-order terms of pov_ind_95

#sharp_RD_fourth = tobit(pct_deforested ~ eligible + pov_ind_95 + 
#                          ... +
#                          forest_prior + ln_area_polyg + deg_slope + ln_pop +
#                          ln_dens_road + ecoreg_control_1 + ecoreg_control_2 +
#                          ecoreg_control_3, data = data_deforest)
#>
sharp_RD_fourth = tobit(pct_deforested ~ eligible + pov_ind_95 + 
                          pov_ind_2 + pov_ind_3 + pov_ind_4 +
                          forest_prior + ln_area_polyg + deg_slope + ln_pop +
                          ln_dens_road + ecoreg_control_1 + ecoreg_control_2 +
                          ecoreg_control_3, data = data_deforest)
#< task
#Adapt the following code for the Tobit regression for the restricted sample

#sharp_RD_restricted = tobit(pct_deforested ~ eligible + pov_ind_95 +
#                          forest_prior + ln_area_polyg + deg_slope + ln_pop +
#                          ln_dens_road + ecoreg_control_1 + ecoreg_control_2 +
#                          ecoreg_control_3, data = ...)
#>
sharp_RD_restricted = tobit(pct_deforested ~ eligible + pov_ind_95 + 
                              forest_prior + ln_area_polyg + deg_slope +ln_pop+
                              ln_dens_road+ecoreg_control_1+ ecoreg_control_2 +
                              ecoreg_control_3, data=data_deforest_small)
#< task
#Show the regression results
showreg(list(sharp_RD_simple, sharp_RD_fourth, sharp_RD_restricted), 
        custom.model.names=c("Sharp RD simple", "Sharp RD polynomial", 
                             "Sharp RD restricted sample"), digits=3,
        include.aic=FALSE, include.bic=FALSE, include.loglik=FALSE, 
        stars=c(0.01,0.05,0.1))
#>
```
One can see that the coefficient estimate of the **intention-to-treat effect** is **positive and significant** at least on the $10 \%$ level in all three specifications. The covariates' coefficients which are significant behave similar to the ones in the simple regression. This strengthens the good feedback for the choice of the covariates.  
In order to obtain the marginal effects for the two model specifications please use the function *margEff_tobit_dM_AER(model, data)* in the next code chunk. As the focus of this part of the problem set lies on the treatment (or intention-to-treat) effect, this time only those marginal effects which belong to the variable *eligible* shall be shown. The according code for this is already given.  
*This corresponds to table 2, columns (2) and (3) on p.425 in the paper.*
```{r "2.5 a) 2", results = "asis"}
#< task
#Compute marginal effects for sharp_RD_fourth
#>
sharp_fourth_marge = margEff_tobit_dM_AER(sharp_RD_fourth, data_deforest)
#< task
#Compute marginal effects for sharp_RD_restricted
#>
sharp_restricted_marge = margEff_tobit_dM_AER(sharp_RD_restricted, 
                                              data_deforest_small)
#< task
#Show the results of all specifications until now
results.P.df = convert(list("simple_full" = sharp_simple_marge[[1]]["eligible",],
                          "simple_fourth" = sharp_fourth_marge[[1]]["eligible",],
                          "simple_small" = sharp_restricted_marge[[1]]["eligible",]))
stargazer(results.P.df, type = "html", summary=FALSE)
results.E.df = convert(list("simple_full" = sharp_simple_marge[[2]]["eligible",],
                          "simple_fourth" = sharp_fourth_marge[[2]]["eligible",],
                          "simple_small" = sharp_restricted_marge[[2]]["eligible",]))
stargazer(results.E.df, type = "html", summary=FALSE)
#>
```
#< info "Remark on the estimated marginal effects"
Please note that some of the results presented here (estimated marginal effects and standard errors) deviate a bit from those presented in the paper. *(Cf. paper, table 2)* Running the available STATA code in STATA however produces results which are consistent with the ones presented here.
#>
In the first data frame the estimated marginal effects on the probability of deforestation of eligibility are shown. As you can see, they are quite similar in all three approaches. This is the desired result because if the estimates do not vary a lot for different specifications, this speaks in favor of the discontinuity and against the error described at the beginning of this sub-exercise. *(Cf. Angrist and Pischke (2015, Chapter 4))* 
The second data frame shows the estimated marginal effects on the relative amount of deforestation for all three specifications. Here, the estimate in the polynomial specification deviates from the according two other estimates but towards an even stronger effect. All three effects are significant (at least on the $10\%$ level).

- - -
The authors remark that the "results are robust to including just second- and third-order polynomials
of the index as well" *(paper, p.424)*. In the following, the according regressions are run. While for the specification with a third-order polynomial the coefficient estimate of *eligible* is still significant on the $10\%$ level and positive, this is not the case for the specification using a second-order polynomial. Just click on the *'check' button* to see the regression results. For comparison, the simplest specification and the fourth-order polynomial specification are shown again.

```{r "2.5a) 3"}
#< task
#Simple approach with second-order polynomial of pov_ind_95
sharp_RD_second = tobit(pct_deforested ~ eligible + pov_ind_95 + 
                          pov_ind_2 + 
                          forest_prior + ln_area_polyg + deg_slope + ln_pop +
                          ln_dens_road + ecoreg_control_1 + ecoreg_control_2 +
                          ecoreg_control_3, data = data_deforest)
#Simple approach with third-order polynomial of pov_ind_95
sharp_RD_third = tobit(pct_deforested ~ eligible + pov_ind_95 + 
                         pov_ind_2 + pov_ind_3 +
                         forest_prior + ln_area_polyg + deg_slope + ln_pop +
                         ln_dens_road + ecoreg_control_1 + ecoreg_control_2 +
                         ecoreg_control_3, data = data_deforest)
#Show the regression results
showreg(list(sharp_RD_simple, sharp_RD_second, sharp_RD_third,
             sharp_RD_fourth), 
        custom.model.names=c("Sharp RD simple", 
                             "Sharp RD 2nd-order pol.",
                             "Sharp RD 3rd-order pol.", 
                             "Sharp RD 4th-order pol."), digits=3,
        include.aic=FALSE, include.bic=FALSE, include.loglik=FALSE, 
        stars=c(0.01,0.05,0.1))
#>
```
So the results might not be as clear as indicated in the paper. However, as the simplest, the third-order polynomial and the fourth-order polynomial approach give positive, significant and similar estimates of the coefficient of *eligible* and besides - as you will see in part *b)* - OLS robustness tests further support this result, the intention-to-treat effect will be treated as significant in the following.

### b) OLS checks for robustness
Apart from the different specifications of the Tobit regressions, Alix-Garcia et al. add two OLS regressions to test for robustness *(cf. paper, section III.E, table 2)*:  
* The first one estimates the **linear probability model** 
$$d = \alpha + \delta E_i + \gamma_1 I_i + \gamma_2 I_i^2 + \gamma_3 I_i^3 + \gamma_4 I_i^4 + \vec{\beta}' \vec{x_i} + \epsilon_i$$
with *d* being the dummy indicating whether there was deforestation and the same covariates $\vec{x_i}$ as in the Tobit regressions above.  
* The second one estimates by OLS the **linear regression model**
$$\Delta f_i = \alpha + \delta E_i + \gamma_1 I_i + \gamma_2 I_i^2 + \gamma_3 I_i^3 + \gamma_4 I_i^4 + \vec{\beta}' \vec{x_i} + \epsilon_i$$
using only data on those localities showing positive deforestation (*d* equals $1$).  
Both regressions are based on data sets over the whole range of marginality indices and include a fourth-order polynomial of the marginality index.

In the following code chunk please use the command *lm(formula, data)* to run the OLS regression *simple_d* estimating the LPM and the OLS regression *simple_pct* estimating the linear regression model. Use the subset of observations with positive deforestation (*d* = $1$) for the latter one. The code for showing the results is already given. As you can see, again *robust* standard errors accounting for heteroskedasticity are calculated.  
*This corresponds to table 2, columns (4) and (5) in the paper.*
```{r "2.5 b) 1"}
#< task
#Run the OLS regression simple_d of the dummy d
#>
simple_d = lm(d ~ eligible + pov_ind_95 + pov_ind_2 + pov_ind_3 + pov_ind_4 +
                forest_prior + ln_area_polyg + deg_slope + ln_pop +
                ln_dens_road + ecoreg_control_1 + ecoreg_control_2 +
                ecoreg_control_3, data=data_deforest)
#< task
#Run the OLS regression simple_pct of pct_deforested
#>
simple_pct = lm(pct_deforested ~ eligible + pov_ind_95 + pov_ind_2 + pov_ind_3+ 
                  pov_ind_4 +forest_prior + ln_area_polyg + deg_slope + 
                  ln_pop + ln_dens_road + ecoreg_control_1 + ecoreg_control_2 +
                  ecoreg_control_3, data=data_deforest, subset = (d==1))
#< task
#Show regression results
showreg(list(simple_d, simple_pct), robust = c(TRUE,TRUE), robust.type="HC1",
        custom.model.names=c("P(d=1)", "pct_defor (d=1)"), 
        digits=3, stars=c(0.01,0.05,0.1), include.adjrs=FALSE,
        include.rsquared=FALSE, include.rmse=FALSE)
#>
```
What about the marginal effects of eligibility - or, put differently, which intention-to-treat effects are estimated by these two robustness tests?  
In a linear regression model as well as in a linear probability model the marginal effects are given by the coefficients. The coefficients from the LPM are directly comparable to the marginal effects on the probability of positive deforestation from the Tobit regressions. The robustness test for the probability of deforestation (LPM) estimates an increase by about $1.3$ percentage points through eligibility which is almost the same as estimated by the sharp RD design specifications.  
The estimated intention-to-treat effect from the second OLS regression is larger than the largest according marginal effect from the three RD design specifications. The authors explain this by the fact that the OLS regression on the sample with positive deforestation does not take into account the (low) probability of deforestation. *(Cf. paper, p.425)*
#< info "Remark on the corresponding results in the paper"
Even though in a classical linear regression model the marginal effects are given by the coefficients, Alix-Garcia et al. don't just indicate the same results for the marginal effect (and its standard error) of eligibility on $E[y|y>0,x]$ as the coefficient estimate (and its standard error), but slightly different numbers. *(Cf. paper, table 2)*
#>
- - -
So all in all, the simple approach suggests that the **intention to treat increased deforestation** - both in probability and in magnitude - significantly which is consistent with the results of the further Tobit specifications as well as with the robustness tests using OLS.

## Exercise 2.6 -- Fuzzy RD design - First Stage Regression

In the last exercise we simplified the behavior of treatment assuming that it increased from $0$ to $1$ all at once at the eligibility cutoff. Actually, as we saw in *Exercise 2.3*, the reality looks different:
* Enrollment does **not increase from $0$ to $1$**, but from about $0.2$ at the left side of the cutoff to about $0.8$ at the right side. One can account for this by using the **fuzzy RD design**.
* Enrollment does **not change discontinuously** at the cutoff, but continuously over a small range of the marginality index around the cutoff. One can adapt the classical fuzzy RD design to this feature by using a **specific set of instrumental variables**.
#< info "Skipping another step on the way to the most accurate estimation"
One could now at first estimate regressions based on a less stronger simplification: Ignoring the second aspect (continuous increase close to the cutoff) and just using the normal fuzzy RD design. Alix-Garcia et al. do this in section III.E of the paper and report the results in table 4. Actually, those results  are quite similar to the results for the approach which takes into account both aspects from above. As the more precise solution is theoretically the more sensible one and at the same time - as you will see - can be shown to be well-suited here, the "simple" fuzzy RD approach could be just regarded as a robustness test which strengthens the results. We will therefore skip this step and come to the most precise solution immediately.
#>
- - -
The setting of a **fuzzy RD design** only differs from the one of a sharp RD design by the fact that the discontinuous increase is *smaller than* $100\%$. This feature indicates that treatment is not determined by the running variable alone but also by other factors. Unfortunately, this makes it probable that there are also *unobservable factors* affecting treatment which means that the treatment variable might be endogenous. There are several possible ways to actually implement the fuzzy RD design with one of them being an instrumental variable (IV) estimation. Compared to other implementation options the IV approach has the advantage to allow controlling for further quantities as well. *(Cf. Jacob and Lefgren (2004, Chapter V.))*  
*If you need some introduction to this technique, have a look at the 'info' box below.*

#< info "Instrumental Variable Estimation"
An instrumental variable (IV) estimation is one possibility to **get consistent estimates** even though not all explanatory variables are exogenous meaning that there is **at least one independent variable** which is **correlated with the error term**.  
The idea of an IV estimation is to filter the variation in the endogenous explanatory variable which is not correlated with the error term by filtering the variation of this variable which can be attributed to the variation in a so-called instrumental variable. Only this variation is then used to estimate the respective coefficient. To make this possible, an instrumental variable - also called instrument - has to meet the following conditions:
* It is not correlated with the error term.  
* It is not correlated with the endogenous explanatory variable.

Consider the linear model $y=X \beta + \epsilon$ with the matrix $X = \left(X_{exo}, X_{endo}\right)$ of observations belonging to the explanatory variables, the index "endo" denoting endogenous and "exo" exogenous variables. Define the matrix of observations on the instrumental variables as $Z=\left(X_{exo}, Z_{excluded}\right)$. Here, "excluded" denotes that those instrumental variable are not part of the original model. For each endogenous variable at least one of those has to be included in $Z$. Then the IV estimate of $\beta$ can be obtained in two steps:  
* 1. Regress the set of explanatory variables $X$ on the set of instrumental variables $Z$ and calculate the predicted values $\hat{X}$ from this regression. The corresponding regression equation is thus
$$X = Z \gamma + u$$
with $\gamma$ the coefficients of the variables in $Z$ and $u$ the error term. The predicted values $\hat{X}$ are thus given by
$$\hat{X} = Z\left(Z'Z\right)^{-1}Z'X$$
* 2. Regress the dependent variable $y$ from the original model on the set of predicted values $\hat{X}$ from the first step. The corresponding regression equation thus only differs from the original one by using $\hat{X}$ instead of $X$. The IV estimator of $\beta$ is thus given by
$$\widehat{ \beta_{IV}} = \left(\hat{X}'\hat{X}\right)^{-1} \hat{X}'y$$

Due to this estimator being calculable in two steps it is called two-stage least squares estimator. Note that estimates can also be obtained by just inserting the first into the second formula and then calculating it.  
*(Cf. Kennedy (2008, Chapter 9))*  
For further details on this estimation technique, its advantages and disadvantages, how to find good instruments and suitable tests, see also *Hackl (2013, Chapter 15)* and *Greene (2008, Chapter 12)*.
#>
Allowing for a continuous increase within a small zone of the running variable can be achieved by using an **appropriate set of excluded instruments** $\vec{v_i}$ in the instrumental variable estimation of the fuzzy RD design. *(Cf. paper, section III.D; Jacob & Lefgren (2004, Chapter V.))* Instead of just using $E_i$ for this purpose (classical fuzzy RD design), Alix-Garcia et al. choose the following set of variables as excluded instruments *(cf. paper, section III.D)*:
$$E_i \text{,   }E_i \times I_i \text{,   } M_i \text{,   } M_i \times I_i$$
Here, $E_i$ and $I_i$ are defined as before; $M_i$ is an indicator of the *marginal zone*. This is the range of the marginality index from $-1.22$ to $-0.9$ within which the probability of treatment increases strongly.  
In the following we are going to **check the suitability and the meaning of this set of excluded instruments**.
- - -
The estimation approach is thus based on the following **system of regression equations** *(cf. paper, p.423)*:  
$$T_i = \omega + \vec{\tau}' \vec{v_i} + \mu I_i + \vec{\Gamma}'\vec{x_i} + \nu_i$$
$$\Delta f_i = \alpha + \delta T_i + \gamma I_i + \vec{\beta}' \vec{x_i} + \epsilon_i$$
Here, $\Delta f_i$ is the percentage of area deforested, $I_i$ the marginality index, $\vec{x_i}$ the vector of covariates (same as in the last exercises) and $\epsilon_i$ and $\nu_i$ are the error terms (all for observation $i$). The variable $T_i$ stands for an indicator of treatment of locality $i$ and $\vec{v_i}$ for the excluded instrumental variables introduced above. Please note that again, the second stage equation is used here as a short form of a Tobit model with latent variable $\Delta f_i$ (see *Exercise 2.5*). As this model is an instrumental variable estimation model with a dependent variable only taking on positive values in the second stage, this is a so-called **Tobit model with an endogenous regressor**. To simplify the notation, I will refer to it as the **IV Tobit Model**.

### a) Suitability of excluded instruments
In the following code chunk we will estimate the **first stage** regression equation from above for the full sample and two further specifications: One for the full sample which uses a fourth-order polynomial of the marginality index and one for the restricted sample which uses a linear function of the marginality index like the equation above. Just get familiar with the code and then click on the *'check' button*.
```{r "2.6 a) 1"}
#< task
#Read in the data and save it as data_deforest
data_deforest=readRDS("data_deforestation.rds")
#OLS estimation of first stage, full data set, linear relation
elaborate_first_full = lm(treat ~ eligible + eligible_povind95 +
                            marginal + marginal_povind95 + 
                            pov_ind_95 + 
                            forest_prior + ln_area_polyg + deg_slope + 
                            ln_pop + ln_dens_road + ecoreg_control_1 + 
                            ecoreg_control_2 + ecoreg_control_3, 
                          data=data_deforest)
#OLS estimation of first stage, full data set, fourth-order polynomial
elaborate_first_fourth = lm(treat ~ eligible + eligible_povind95 +
                              marginal + marginal_povind95 +
                              pov_ind_95 + pov_ind_2 + pov_ind_3 + pov_ind_4 +
                              forest_prior + ln_area_polyg + deg_slope + 
                              ln_pop + ln_dens_road + ecoreg_control_1 + 
                              ecoreg_control_2 + ecoreg_control_3, 
                            data=data_deforest)
#OLS estimation of first stage, small sample, linear relation
elaborate_first_small = lm(treat ~ eligible + eligible_povind95 + 
                             marginal + marginal_povind95 + pov_ind_95 + 
                             forest_prior + ln_area_polyg + deg_slope + 
                             ln_pop + ln_dens_road + ecoreg_control_1 + 
                             ecoreg_control_2 + ecoreg_control_3, 
                            data=data_deforest, subset = smallsample==1)
#>
```
For all of the OLS regressions which Alix-Garcia et al. run they use robust standard errors. *(Cf. available STATA code files)* Up to now, we have always calculated them within the *showreg* command. One possibility to calculate robust standard errors separately is the following: First use the command *vcovHC(reg, type="HC1")* of the package *'sandwich'* to calculate the kind of heteroskedasticity robust variance covariance matrix ("HC1") (for some regression *reg*) which is analogue to the one calculated by the command *vce(robust)* in STATA. Then, to obtain the according standard errors, take the square root of the diagonal elements of this matrix. Please uncomment the code and fill in the missing parts for the regression *elaborate_first_full*. I recommend to you having a look at the data frame *vcov_robust_full* in the *'Data Explorer'* to get an impression of what it looks like.
```{r "2.6 a) 2"}
#< task
#Load package 'sandwich'
library(sandwich)
#Estimate robust standard errors for elaborate_first_full
#vcov_robust_full = vcovHC(... , ...)
#se_robust_full = ...
#>
vcov_robust_full = vcovHC(elaborate_first_full, type="HC1")
se_robust_full = sqrt(diag(vcov_robust_full))
```
The same has to be done for the other two regressions. Just click on the *'check' button*.
```{r "2.6 a) 3"}
#< task
#Estimate robust standard errors for the two additional specifications
vcov_robust_fourth = vcovHC(elaborate_first_fourth, type="HC1")
se_robust_fourth = sqrt(diag(vcov_robust_fourth))
vcov_robust_small = vcovHC(elaborate_first_small, type="HC1")
se_robust_small = sqrt(diag(vcov_robust_small))
#>
```
Finally, the following code chunk shows the regression results from all three first stage specifications above. Instead of the original standard errors, the robust standard errors are used. This time, the confidence intervals of the coefficient estimates are displayed instead of the standard errors. Click on the *'check' button* to run the code.  
*This corresponds to table 3, columns (2), (3), (4) on p.426 in the paper.*
```{r "2.6 a) 4", results="asis"}
#< task
#Load package 'stargazer'
library(stargazer)
#Show regression results
stargazer(elaborate_first_full, elaborate_first_fourth, 
          elaborate_first_small,
          se = list(se_robust_full, se_robust_fourth, se_robust_small), 
          ci = TRUE, column.labels=c("Linear in marg. index", 
                                     "Fourth-order polynomial of marg. index",
                                     "Small sample"), 
          dep.var.caption="", dep.var.labels="", no.space=TRUE, 
          single.row=TRUE, digits=3, star.cutoffs=c(0.1,0.05,0.01), 
          type="html", omit.stat="all")
#>
```
The size and the meaning of the estimates of the excluded instruments' coefficients will be subject to a more elaborate analysis later in sub-exercise **b)** of this exercise. As those coefficient estimates do not vary a lot between the three specifications, we will only focus on the first regression there.  
To check the significance of the instruments we make use of the confidence intervals which you find underneath each coefficient estimate in the output above: If the whole interval of the $95 \%$ confidence interval is positive (or negative), the coefficient is significant on the $5 \%$ level. As you can see, the first four coefficients, which are the **excluded instruments**, are  **significant** at least on the $5 \%$ level in each specification. From the significance stars you can see further that most of them are even significant on the $1 \%$ level.  
Note that all covariates as well as the marginality index, which serve as (included) instruments here, are also significant on a very high level.  

Another test for the suitability of the set of instruments is the *F-test* on the joint significance of all excluded instruments. The following code chunk runs this test for the regression model *elaborate_first_full*. The computational steps are explained underneath.  
*This corresponds to table 3, column (2) on p.426 in the paper.*
```{r "2.6 a) 5", results="asis", fig.height=10}
#< task
#Compute the F-statistic for excluded instruments
#Estimate the reduced regression equation without excluded instruments
elaborate_full_reduced = lm(treat ~ pov_ind_95 + 
                               forest_prior + ln_area_polyg + deg_slope + 
                               ln_pop + ln_dens_road + ecoreg_control_1 + 
                               ecoreg_control_2 + ecoreg_control_3, 
                             data=data_deforest)
#Load package 'lmtest'
library(lmtest)
#Run a Wald test on the reduced and the full model
test = waldtest.default(elaborate_full_reduced, elaborate_first_full, 
                        vcov=vcovHC, test="F")
#Show the results
stargazer(test, summary=FALSE, type="html", rownames=FALSE)
#>
```
Here, the *F-test* is realized in the following steps: First of all, in addition to the original model *elaborate_first_full* a second model is estimated which does not contain the coefficients which shall be tested. In a second step, a Wald test is computed by the command *waldtest(object1, object2, vcov)* from the package *'lmtest'* where *object1* and *object2* are for example regression objects. The argument *vcov=vcovHC* determines that heteroskedasticity robust standard errors are computed. The output shows the degrees of freedom of both models and the difference in the degrees of freedom ($4$), namely the number of coefficients to be tested. The F-statistic has the value $2,016.058$ which is a bit smaller than the one stated in the paper *(p.426, Table 3, column (2))*. The probability to obtain the observations in *data_deforest* even though none of the excluded instruments affects the dependent variable is approximately $0$. So the **excluded instruments are jointly significant**.  
All in all we can state that the **choice of instruments seems to be sensible**.

### b) Visualization of excluded instrument coefficient estimates
We have just checked the significance of the excluded instruments. But up to now you might miss a more concrete idea of what the different coefficient estimates mean.  
This is why in the following, we will graphically visualize  
* the marginal effect of the variable $E$ changing from $0$ to $1$ keeping $M$ fixed  
* the marginal effect of the variable $M$ changing from $0$ to $1$ keeping $E$ fixed  
* the effect of both variables together

Let's first consider the first two cases. The according marginal effects are given by the following equations:
$$E[T|\vec{z}, E=1]-E[T| \vec{z}, E=0] = \tau_{E} + \tau_{E \times I} \cdot I$$
$$E[T|\vec{z}, M=1]-E[T| \vec{z}, M=0] = \tau_{M} + \tau_{M \times I} \cdot I$$
To visualize the effects we just multiply the first equation by the respective variable $E$ and the second equation by $M$. This way, we are free to choose the interval on the x axis for the plot and automatically set the other covariates to zero such that the values on the y axis represent directly the marginal effects. Just click on the *'check' button* to visualize the **effect of the variable $E$**.  
*Please note that we will only do these visualizations for the results from the regression elaborate_first_full. For the other regressions this would be analogous.*
```{r "2.6 b) 1", fig.height=4}
#< task
#Calculate the values for the y axis fitted_elig
fitted_elig = coef(elaborate_first_full)["eligible"] * data_deforest$eligible + 
  coef(elaborate_first_full)["eligible_povind95"] * data_deforest$eligible_povind95
#Load package 'ggplot2'
library(ggplot2)
#Create the plot of fitted_elig over pov_ind_95
p1 = ggplot(data=data_deforest) + 
  geom_line(aes(x=pov_ind_95, y=fitted_elig), col="blue") +
  coord_cartesian(xlim=c(-2,0), ylim=c(-0.75,1)) +
  xlab("marginality index")
#Show the plot
p1
#>
```
As expected, being eligible for the program increases the probability of being treated a lot: Right at the cutoff by about $0.621 + (-0.063) \cdot (-1.22) = 0.69786$, so by almost $70$ percentage points. The higher the marginality index, the lower this increase, but the decline is very slow ($-0.063$ per unit marginality index).  
Now, it is your turn to visualize the **effect of the variable $M$**. You can do this in analogy to the code above but please use the color *darkgreen* instead of *blue* for the line.
```{r "2.6 b) 2", fig.height=4}
#< task
#Calculate the values for the y axis fitted_marg analogously to fitted_elig
#>
fitted_marg = coef(elaborate_first_full)["marginal"] * data_deforest$marginal +
  coef(elaborate_first_full)["marginal_povind95"] * data_deforest$marginal_povind95
#< task
#Create the plot p2 analogously to the plot p1 but with fitted_marg as y values
#Show the plot
#>
p2 = ggplot(data=data_deforest) +  
  geom_line(aes(x=pov_ind_95, y=fitted_marg), col="darkgreen") +
  coord_cartesian(xlim=c(-2,0), ylim=c(-0.75,1)) +
  xlab("marginality index")
p2
```
Being in the marginal zone between $-1.22$ and $-0.9$ decreases the increase resulting from being solely eligible. At the cutoff this reduction has a minimum of $1.117 + 1.331 \cdot (-1.22) = -0.50682$ which means that at that point the effect of eligibility is reduced by more than $50$ percentage points. The reduction becomes smaller (according to amount) the higher the marginality index is. This way, at the right end of the marginal zone the reduction of the effect of eligibility almost vanishes.  
Last but not least the **effect of both variables** is visualized in the next code chunk. The code is already given.
```{r "2.6 b) 3", fig.height=4}
#< task
#Calculate the values for the y axis fitted_inst
fitted_inst = coef(elaborate_first_full)["eligible"] * data_deforest$eligible + 
  coef(elaborate_first_full)["eligible_povind95"] * data_deforest$eligible_povind95 +
  coef(elaborate_first_full)["marginal"] * data_deforest$marginal +
  coef(elaborate_first_full)["marginal_povind95"] * data_deforest$marginal_povind95
#Create the plot of fitted_elig over pov_ind_95
p3 = ggplot(data=data_deforest) + geom_line(aes(x=pov_ind_95, y=fitted_inst), 
                                            col="black") +
  coord_cartesian(xlim=c(-2,0), ylim=c(-0.75,1)) +
  xlab("marginality index")
#Show the plot
p3
#>
```
#< info "Remark on the other specifications"
As already mentioned above I don't run the same visualization for the other two specifications we have considered in this exercise. The graphics should be very similar to the one we have just obtained because the coefficient estimates do not vary a lot. There is one exception: In the regressions *elaborate_first_fourth* and *elaborate_first_small* the estimated coefficient of *eligible_povind95* is not negative but positive (and significant at least on the $5\%$ level). The most apparent (but still small) change in the graphics would therefore be that in the zone to the right of $-0.9$ the slope would be (slightly) positive.
#>

- - -
First of all, it becomes clear that the **excluded instruments serve the purpose** for which they were constructed: They allow for a **continuous increase in treatment over the marginal zone**.  
Concerning the results, we can say: As expected, holding all other variables constant, **being in the marginal zone** as well as **being only eligible increases the probability of treatment** and **within the marginal zone the probability of treatment is rising continuously**.

Putting this together with the results from part *a)* indicating that the coefficients on the **instruments are all significant on their own and jointly** this set of instrumental variables is a good base for the most advanced regression approach in the next exercise.

## Exercise 2.7 -- Fuzzy RD design - Full Regression
After checking the quality of the instruments in the last exercise, we are now able to run the full IV Tobit regression. We want to estimate the **treatment effect** on the probability and the amount of deforestation and see whether the more approximate results of the simple approach from *Exercise 2.5* will be confirmed.

As already announced in *Exercise 2.6* we will use again three model specifications for the estimation of the treatment effect in the fuzzy RD design like Alix-Garcia et al.: the "standard" model as specified in the last exercise along with two specifications which were already used in the simple approach (sharp RD) and for which we evaluated the first stage regression in the last exercise - a specification containing a fourth-order polynomial of $I_i$ (estimated for the full sample) and a specification like in the original model but estimated for the restricted sample. *(Cf. paper, sections III.D, III.E, table 5)*  
Recall the regression equations of the "standard" model:
$$T_i = \omega + \vec{\tau}' \vec{v_i} + \mu I_i + \vec{\Gamma}'\vec{x_i} + \nu_i$$
$$\Delta f_i = \alpha + \delta T_i + \gamma I_i + \vec{\beta}' \vec{x_i} + \epsilon_i$$
The choice of using these three models results from the same idea as in the simple approach: trying to avoid a misinterpretation of a very steep continuous increase at the cutoff as discontinuity.

First, we have to load again the data set *"data_deforestation.rds"* and create again the small sample data set. Click on the *'check' button* to run the according code.
```{r "2.7 1"}
#< task
#Read in the data and save it as data_deforest
data_deforest=readRDS("data_deforestation.rds")
#Load package 'dplyr'
library(dplyr)
#Create reduced sample data set
data_deforest_small = filter(data_deforest, smallsample==1)
#>
```
There are several ways to implement the estimation of an IV Tobit Model. In this problem set we will use the function *Newey_SB_ivtobit(data, ystr, Ystr, X1str, X2str)* which calculates the **Newey two-step estimator** of the coefficients in the main regression. As indicated by the suffix *str* the respective arguments in brackets have to be passed to the function as strings. The meaning of the arguments is as follows:  
*y* stands for the dependent variable (here: *pct_deforested*), *Y* for the endogenous variable (here: *treat*), *X1* stands for a vector of the exogenous variables in the main regression (here: running variable *pov_ind_95*, possibly additional higher order terms of it and further covariates), *X2* stands for the excluded instruments (here: *eligible*, *eligible_povind95*, *marginal*, *marginal_povind95*). The argument *data* indicates the data frame containing the observations of the variables just mentioned.  
If you are interested in the background of the estimator or the function and its code, have a look at the *'info' box* below.

#< info "Newey_SB_ivtobit - Code and introduction into Newey two-step estimator and Smith-Blundell procedure"
**STATA** offers two estimators for a Tobit model with one or more endogenous regressor(s): maximum likelihood and the Newey two-step estimator. Both estimators are provided by different options of the STATA command *ivtobit*. Unfortunately, in **R** there is no such function. This is why I had to implement it on my own - and I decided to implement the Newey two-step estimator. This is because compared to a maximum likelihood estimation the computational time is much smaller and it does not require guessed starting values which makes it more adequate for this problem set.  
If you are interested in the theory on maximum likelihood estimation of an IV Tobit model, I can recommend *Wooldridge (2010, Chapter 17.5.2)* to you.

The implementation of the Newey two-step estimator in the function ***Newey_SB_ivtobit*** is based on the following sources:  
* the original paper *Newey (1987)*  
* the help page of the STATA command [*ivtobit*](http://www.stata.com/manuals13/rivtobit.pdf "http://www.stata.com/manuals13/rivtobit.pdf")  
* the help page of the STATA command [*ivprobit*](http://www.stata.com/manuals13/rivprobit.pdf "http://www.stata.com/manuals13/rivprobit.pdf").

If you are just interested in the estimation procedure, I recommend the last source to you. The algorithm on page 12 can be adapted to the algorithm for the IV Tobit two-step estimator by just replacing the probit estimation by a Tobit estimation. *(Cf. [Stata ivtobit help site](http://www.stata.com/manuals13/rivtobit.pdf "http://www.stata.com/manuals13/rivtobit.pdf"), p.10)*

The IV Tobit model underlying the function *Newey_SB_ivtobit* is the following *(Newey (1987))*:
$$\text{(I)} \quad Y_t=\vec{\pi_1}^{\prime}\vec{X_{1t}} + \vec{\pi_2}^{\prime}\vec{X_{2t}} + V_t \quad \text{with} \quad t=1,...,n$$
$$\text{(II)} \quad y_t=max(0,y_t^*)=max \left(0, \beta_0 Y_t + \vec{\gamma_0}^{\prime}\vec{X_{1t}} + u_t \right) \quad \text{with} \quad t=1,...,n$$
Here, $y$ refers to the function argument *ystr*, $Y$ to *Ystr*, $\vec{X_1}$ to *X1str* and $\vec{X_2}$ to *X2str*.  
The error terms $u_t$ and $V_t$ are assumed to be multivariate normally distributed with expected value zero and variance-covariance matrix $\Sigma$ conditional on $\vec{X_{1t}}$ and $\vec{X_{2t}}$.

The different steps for the calculation of the Newey two-step estimator are explained in the code through comments.

The function *Newey_SB_ivtobit* also computes **marginal effects**.  
Here, the estimates needed for the calculations are taken from the estimation procedure by **Smith and Blundell**. *(For more information see for example Wooldridge (2002, Chapter 16.6.2.))* This procedure uses the same model as above and is given by two of the calculation steps of the Newey two-step estimation procedure (in the code labeled as (I) and (III)). Further, the following relation which follows from the multivariate distribution is used *(Wooldridge (2002, chapter 16.6.2))*:
$$u = V \rho + \epsilon \quad \text{with} \epsilon \sim N(0,\tau_1^2)$$
The variance of the error term $V$ is denoted by $\tau_2^2$.
The marginal effects estimation in my implementation is based on the following equations:
$$(1)\text{   }P\left(y>0|\vec{X_{1}},Y\right) = \Phi\left(\frac{\vec{\gamma_0}^{\prime}\vec{X_{1}}+\beta_0 Y}{\sqrt{\tau_1^2 + \rho^2\tau_2^2}}\right)$$
$$(2)\text{   }E[y|y>0,\vec{X_{1}},Y] = E_{V}\left[\vec{\gamma_0}^{\prime}\vec{X_{1}} + \beta_0 Y + \rho V + \tau_1 \lambda\left(\frac{\vec{\gamma_0}^{\prime}\vec{X_{1}}+\beta_0 Y + \rho V}{\tau_1}\right)\right] \text{   with   } \lambda(z)=\frac{\phi(z)}{\Phi(z)}$$

Those formulas are based on theoretical thoughts, approximations and simplifications. The implementation is restricted to the calculation of marginal effects of one endogenous binary variable as in this problem set the (probably) endogenous variable in the IV Tobit model is the dummy variable indicating treatment. For this case the marginal effect can be obtained by calculating the difference between the respective equation given the binary variable takes on the value $1$ and the same equation given the variable takes on the value $0$. Equation $(2)$ is estimated by the empirical mean over the estimated error term $V$ given by the residuals $\hat{V}$. The estimation of the marginal effects is again (like in the Tobit model) implemented using the delta method because this allows to get standard errors for the estimates as well. All in all, the values of the estimates come very close to those in the paper. As sources for the model and the calculations I used Wooldridge (2002), sections 2.2.5, 15.7.2, 16.2, 16.6.2; Wooldridge (2010), section 17.5.2 and the sources mentioned above.

*You can find more details about the model, the Newey two-step estimator, the Smith-Blundell estimator, the implementation and above all about the derivation of the marginal effects formulas in an IV Tobit model in the appendix A of this problem set.*

Please note that the function *Newey_SB_ivtobit* is only applicable to IV Tobit regressions with one binary endogenous variable. Below you can see the commented code of the function.
```{r "info code Newey_SB_ivtobit"}
library(AER)
library(dplyr)
library(dplyrExtras)
#function to calculate Newey two-step estimates for IV Tobit 
#and marginal effects on P(y>0) and on E[y|y>0]

#implementation of Newey two-step estimator based on:
#- Newey, EFFICIENT ESTIMATION OF LIMITED DEPENDENT VARIABLE MODELS WITH ENDOGENOUS EXPLANATORY VARIABLES,
#  Journal of Econometrics 36 (1987), p.231-250
#- http://www.stata.com/manuals13/rivtobit.pdf
#- http://www.stata.com/manuals13/rivprobit.pdf p.11-12

#input (all as strings): ystr - dependent variable, Ystr - endogenous explanatory variable (only one!),
#X1str - exogenous explanatory variables, X2str - excluded instruments
#General remark: Please note that I indicate the maximization condition for the estimates calculated here,
#                but I don't use the function censReg ('censReg' package) calculating maximum likelihood
#                estimates for reasons of computational time but the command tobit ('AER' package) which
#                should deliver the same numerical results
Newey_SB_ivtobit = function(data, ystr, Ystr, X1str, X2str){
  #Extract the observations of the variables of interest from the data
  X1 = subset(data,select=c(X1str))
  X2 = subset(data,select=c(X2str))
  y = subset(data, select=c(ystr))
  Y = subset(data, select=c(Ystr))
  
  #(I) OLS regression: Y_t = X1_t pi1 + X2_t pi2 + V_t = X_t pi + V_t
  x1_formula = paste(X1str, collapse = " + ")
  x2_formula = paste(X2str, collapse = " + ")
  formula_1 = paste(Ystr," ~ ", x1_formula, " + ",x2_formula, sep="")
  first = lm(formula_1, data=data)
  #save estimate of pi in pi_l_hat which equals theta_1_l_hat
  pi_l_hat=coef(first)
  #Use pi_l_hat to calculate D_hat
  D_hat = cbind(pi_l_hat, rbind(diag(x=1, nrow=ncol(X1)+1, ncol=ncol(X1)+1), 
                                matrix(0,nrow=ncol(X2) , ncol(X1)+1)))
  
  #(II) two-step estimator: 
  #max{alpha, lambda, sigma^2, psi} sum{t}l(y_t, X_t alpha + V_t_hat lambda, sigma^2, psi)/n
  #--> take residuals from OLS (I) as V_t_hat
  #    Tobit regression: y_t = max(0, X_t alpha + V_t_hat lambda + e_t) with e_t|Y ~ N(0,sigma^2)
  formula_2 = as.formula(paste0(ystr," ~ ",x1_formula," + ",x2_formula," + residuals(first)"))
  second = tobit(formula_2, data=data)
  n_2 = length(coef(second))
  #save estimate of alpha in alpha_l_hat, estimate of lambda in lambda_l_hat
  alpha_l_hat = coef(second)[1:(n_2-1)]
  lambda_l_hat = coef(second)[n_2]
  #calculate estimate of (J^(-1))alpha_alpha
  J_alpha_alpha = vcov(second)[(1:(n_2-1)), (1:(n_2-1))]
  
  #(III) 2SIV estimator:
  #max{delta, rho, sigma^2, psi} sum{t}l(y_t, Z_t delta + V_t_hat rho, sigma^2, psi)
  #with Z_t = [Y_t, X_1_t]
  #--> take residuals from OLS (I) as V_t_hat
  #    Tobit regression: y_t = max(0, Z_t delta + V_t_hat rho + e_t)
  #                          = max(0, Y_t beta + X_1_t gamma0 + V_t_hat rho + e_t)
  formula_3 = as.formula(paste0(ystr," ~ ", Ystr, " + ", x1_formula, " + residuals(first)"))
  third = tobit(formula_3, data=data)
  #save estimate of beta in beta_l_hat and name the elements
  beta_l_hat = coef(third)[2]
  names(beta_l_hat)=paste0(Ystr, "_hat")
  
  #(IV) OLS regression of Y_t (lambda_hat - beta_hat) on X_t
  #--> take lambda_l_hat from (II) as lambda_hat and beta_l_hat from (III) as beta_hat
  Y_tilde = as.numeric(as.matrix(Y*(lambda_l_hat - beta_l_hat)))
  formula_4 = as.formula(paste0("Y_tilde", " ~ ", x1_formula, " + ", x2_formula))
  fourth = lm(formula_4, data=data)
  #save estimate of covariance matrix in vcov_tilde
  #this equals Nu_hat^2 (X'X/n)^(-1)
  vcov_tilde = vcov(fourth)
  
  #Calculate Omega_l_hat
  omega_l_hat = J_alpha_alpha + vcov_tilde
  
  #Calculate Newey estimator delta_Al_hat and the estimate of its asymptotic covariance matrix
  delta_Al_hat = solve(t(D_hat)%*%solve(omega_l_hat)%*%D_hat)%*%t(D_hat)%*%solve(omega_l_hat)%*%alpha_l_hat
  vcov_hat_delta = solve(t(D_hat)%*%solve(omega_l_hat)%*%D_hat)
  #add according variable names and column names to vcov_hat_delta
  row.names(vcov_hat_delta)=c(Ystr, "Intercept", colnames(X1))
  colnames(vcov_hat_delta)=c(Ystr, "Intercept", colnames(X1))
  #calculate standard errors of delta_Al_hat
  se_delta_hat = sqrt(diag(vcov_hat_delta))
  
  #Output preparation
  df = data.frame(Estimate = as.vector(delta_Al_hat), SE = as.vector(se_delta_hat))
  
  #Adding t values, p values and significance stars
  #and setting row names of resulting data frame
  df = mutate(df, tvalue = delta_Al_hat/se_delta_hat, 
              pvalue = 2*pt(-abs(tvalue), nrow(data - length(X1str)-length(Ystr) - 1)),
              significance = as.factor(ifelse(pvalue<0.1, ifelse(pvalue<0.05,ifelse(pvalue<0.01, "***" , "**") , "*"), "")))
  row.names(df)=c(Ystr, "Intercept", colnames(X1))
  ###########################################################
  #marginal effects
  #Used source (apart from those mentioned above):
  # - Wooldridge, J. M. (2002): Econometric Analysis of Cross Section and Panel Data. Cambridge, MA: MIT Press.,
  #   sections 2.2.5, 15.7.2, 16.2, 16.6.2
  row.names(delta_Al_hat)=c(Ystr, "Intercept", colnames(X1))
  names(se_delta_hat)=c(Ystr, "Intercept", colnames(X1))
  
  #estimates for tau1, tau2
  #lntau1 from third regression, tau2sqr from first OLS regression
  n_tobitSM_coef = nrow(summary(third)$coef)
  lntau1 = summary(third)$coef[n_tobitSM_coef,1]
  names(lntau1)="lntau1"
  tau2sqr = var(residuals(first))
  names(tau2sqr)="tau2sqr"
  
  #rho_hat from third regression
  rho_hat_SM = coef(third)[length(coef(third))]
  names(rho_hat_SM)="rho_hat_SM"
  
  #used estimates: all (except tau2sqr) from third regression
  #in order to get a suitable covariance matrix for the estimates
  gamma0_SM_hat = coef(third)[-c(2,length(coef(third)))]
  names(gamma0_SM_hat)=c("Intercept", colnames(X1))
  beta_SM_hat = coef(third)[2]
  names(beta_SM_hat)=c(Ystr)
  X1_mean = colMeans(cbind(1,X1), na.rm=TRUE)
  names(X1_mean)[1]="Intercept"
  names(X1_mean)=paste(names(X1_mean),"mean", sep="")
  
  #marginal effect on P(y>0|X)
  formula_p = paste("pnorm((",
                    paste(names(gamma0_SM_hat), names(X1_mean), sep="*", collapse= " + "), 
                    "+ ", names(beta_SM_hat), 
                    ")/sqrt(exp(2*lntau1) + rho_hat_SM^2 * tau2sqr)",
                    ") - pnorm((",
                    paste(names(gamma0_SM_hat), names(X1_mean), sep="*", collapse= " + "),
                    ")/sqrt(exp(2*lntau1) + rho_hat_SM^2 * tau2sqr)", ")")
  
  #variance/covariance input for deltaMethod
  vcov_help = vcov(third)
  vcov_1 = vcov_help[,-2]
  vcov_1 = cbind(vcov_help[,2],vcov_1)
  vcov_2 = vcov_1[-2,]
  vcov_2 = rbind(vcov_1[2,],vcov_2)
  vcov_p = vcov_2
  vcov_p=rbind(cbind(vcov_p,0),0)
  n_p = nrow(vcov_p)
  row.names(vcov_p)[(n_p-2):n_p]=c("rho_hat_SM", "lntau1", "tau2sqr")
  colnames(vcov_p)[(n_p-2):n_p]=c("rho_hat_SM", "lntau1", "tau2sqr")
  
  margeP = deltaMethod(object=c(beta_SM_hat, gamma0_SM_hat, rho_hat_SM, lntau1, tau2sqr),
                       vcov.=vcov_p, g=formula_p, constants = X1_mean)
  row.names(margeP)="marginal effect P(y>0)"
  
  #marginal effect on E[y|y>0]
  #used estimates: all from third regression in order to get a suitable covariance matrix for the estimates
  
  #variance/covariance matrix of input parameters of marginal effects calculation
  vcov_E = vcov_p[-nrow(vcov_p),]
  vcov_E = vcov_E[,-ncol(vcov_E)]
  
  #calculate marginal effect
  n_res=length(residuals(first))
  xbeta_u = X1_mean%*%gamma0_SM_hat + beta_SM_hat + rho_hat_SM *residuals(first)
  xbeta_d = X1_mean%*%gamma0_SM_hat + rho_hat_SM*residuals(first)
  tau1 = exp(lntau1)
  lambda_u = dnorm(xbeta_u/tau1)/pnorm(xbeta_u/tau1)
  lambda_d = dnorm(xbeta_d/tau1)/pnorm(xbeta_d/tau1)
  
  margeE = (1/n_res)*sum(xbeta_u + tau1*lambda_u -
                           (xbeta_d + tau1*lambda_d))
  
  #calculate gradient of margeE
  C_beta = 1-(1/n_res)*sum(lambda_u*((xbeta_u/tau1) + lambda_u))
  C_gamma = (1/n_res)*X1_mean*(sum(lambda_d*((xbeta_d/tau1)+lambda_d)) - 
                                 sum(lambda_u*((xbeta_u/tau1)+lambda_u)))
  C_rho = (1/n_res)*sum(residuals(first)*lambda_d*((xbeta_d/tau1)+lambda_d)) - 
    (1/n_res)*sum(residuals(first)*lambda_u*((xbeta_u/tau1)+lambda_u))
  C_lntau1 = (1/n_res)*tau1*sum(lambda_u-lambda_d) + 
    (1/n_res)*(1/(tau1)^2)*sum((xbeta_u*xbeta_u*lambda_u) - (xbeta_d*xbeta_d*lambda_d)) + 
    (1/n_res)*(1/tau1)*sum((xbeta_u*lambda_u*lambda_u) - (xbeta_d*lambda_d*lambda_d))
  C=as.vector(c(C_beta, C_gamma, C_rho, C_lntau1))
  
  #calculate variance/covariance matrix of margeE according to the delta method
  vcov_margE = t(C)%*%vcov_E%*%C
  se_margE = sqrt(vcov_margE)
  
  
  #prepare RESULTS for output
  df_marge = data.frame(Estimate=c(margeP[1,1],margeE), SE=c(margeP[1,2],se_margE))
  
  #Adding t values, p values, significance stars and add row names
  df_marge = mutate(df_marge, tvalue = Estimate/SE,
                    pvalue = 2*pt(-abs(tvalue), nrow(data - length(X1str)-length(Ystr) - 1)),
                    significance = as.factor(ifelse(pvalue<0.1, ifelse(pvalue<0.05, ifelse(pvalue<0.01, "***" , "**") , "*"), "")))
  rownames(df_marge)=c("marginal effect on P(y>0)", "marginal effect on E[y|y>0]")
  #row.names(df_marge)=c("marginal effect on P(y>0)", "marginal effect on E[y|y>0]")
  
  results = list("coefficients" = df, "marginalEffects" = df_marge)
  return(results)
}
```

#>

In the following code chunk, please run the first regression as specified in the regression equations above and save it in the variable *elab_std*.  
*This corresponds to table 5, column (1) on p.428 in the paper.*
```{r "2.7 2"}
#< task
#Load packages 'AER', 'car'
library(AER)
library(car)
#Estimate first regression specification (the "standard" model)
#>
elab_std = 
  Newey_SB_ivtobit(data=data_deforest, ystr="pct_deforested",
                      Ystr="treat", X1str=c("pov_ind_95", "forest_prior",
                                            "ln_area_polyg", "deg_slope",
                                            "ln_pop", "ln_dens_road", 
                                            "ecoreg_control_1", 
                                            "ecoreg_control_2", 
                                            "ecoreg_control_3"),
                      X2str=c("eligible", "eligible_povind95", "marginal", 
                              "marginal_povind95"))
```
Further, the two robustness specifications are estimated analogously and the results of all three regressions are given. Just click on the *'check' button* to run the code. This might take some seconds.  
*This corresponds to table 5, columns (2) and (4) on p.428 in the paper.*
```{r "2.7 3", results="asis"}
#< task
#Estimate fourth-order polynomial specification
elab_fourth = 
  Newey_SB_ivtobit(data=data_deforest, ystr="pct_deforested",
                      Ystr="treat", X1str=c("pov_ind_95", "pov_ind_2", 
                                            "pov_ind_3", "pov_ind_4", 
                                            "forest_prior", "ln_area_polyg", 
                                            "deg_slope", "ln_pop", 
                                            "ln_dens_road", "ecoreg_control_1", 
                                            "ecoreg_control_2", 
                                            "ecoreg_control_3"),
                      X2str=c("eligible", "eligible_povind95", "marginal", 
                              "marginal_povind95"))
#Estimate restricted sample specification
elab_small = 
  Newey_SB_ivtobit(data=data_deforest_small, ystr="pct_deforested",
                      Ystr="treat", X1str=c("pov_ind_95", "forest_prior",
                                            "ln_area_polyg", "deg_slope",
                                            "ln_pop", "ln_dens_road", 
                                            "ecoreg_control_1", 
                                            "ecoreg_control_2", 
                                            "ecoreg_control_3"),
                      X2str=c("eligible", "eligible_povind95", "marginal", 
                              "marginal_povind95"))
#Load package 'stargazer'
library(stargazer)
#Show the regression results
results.df = convert(list("elab_std" = elab_std[[1]],
                          "elab_small" = elab_small[[1]],
                          "elab_fourth" = elab_fourth[[1]]))
stargazer(results.df, type="html", summary=FALSE)
#>
```
The estimated **treatment effect is positive and significant** on the $5 \%$ level in all three specifications. *Apart from the significance stars you can check this with a simple rule of thumb: If the treatment estimates are about twice as large as the according standard errors, they are significant on the* $5 \%$ *level.* In general, the results are very similar to those from the simplest approach using eligibility as a proxy for treatment in the sharp RD design.
#< info "Remark on the results"
Please note that for all regressions run by *Newey_SB_ivtobit* the results (can) deviate a bit from those in the paper. This is because the function uses a Newey two-step estimator whereas in the paper (most of the time) a maximum likelihood estimator is used.  
Concerning those marginal effects which are also calculated by *Newey_SB_ivtobit* there are sometimes also (very small) deviations from the respective numbers in the paper. This is because the marginal effects are calculated differently here.

Concerning the coefficients, I test the plausibility of the results obtained by *Newey_SB_ivtobit* by running in STATA the *ivtobit* command with the method *twostep* as this method estimates as well Newey two-step estimators. For the regressions above (belonging to table 5 in the paper) the results are consistent with those obtained with that STATA command.  
For the marginal effects I cannot proceed analogously because my marginal effects calculation is based on estimates from the procedure by Smith and Blundell. However, there is no analogous STATA marginal effects command. So here I will just check whether the estimates are close to those in the paper.
#>
As the regressions are basically Tobit regressions, the size of the estimated coefficients again has no sensible interpretation. We thus again need to calculate the marginal effects separately. Actually we have already done so because the *Newey_SB_ivtobit()* command does not only return a data frame containing coefficient estimates and standard errors but also a data frame containing the marginal effects of treatment on the probability of deforestation and on the expected percentage of area deforested given there is deforestation. *Note that we focus on treatment here because this is the effect we are after.* The following code chunk shows those results. Just click on the *'check' button* to see them.  
*This corresponds to table 5, columns (1), (2) and (4) on p.428 of the paper.*
```{r "2.7 4", results = "asis"}
#< task
#Show the marginal effects
margEff.df = convert(list("elab_std" = elab_std[[2]],
                          "elab_small" = elab_small[[2]],
                          "elab_fourth" = elab_fourth[[2]]))
stargazer(margEff.df, type="html", summary=FALSE)
#>
```
The results indicate that **treatment increases** both the **probability of deforestation and the percentage of area deforested significantly**. The first effect varies between $1.8$ and $3.7$ percentage points and the second between about $0.09$ and $0.18$ percentage points. These estimates are a bit higher than the ones of the intention-to-treat effect in the simple approach of *Exercise 2.5* (between $1.1$ and $1.6$ percentage points and between $0.05$ and $0.08$ percentage points) but the dimension roughly stays the same and - above all - both approaches state a positive and significant effect.   
#< info "Remark on the results"
As expected, the estimates deviate a bit from those in the paper (see *'info' box* above), but only marginally. *(Cf. paper, table 5)*
#>
Still the marginal effects estimated here seem to be quite small at first sight. To be able to judge it one should compare the numbers to the **baseline probability respectively amount of deforestation** like Alix-Garcia et al. do as well. *(Cf. paper, p.426)* A sensible number for this purpose is the baseline value of the respective quantity among the localities which were not eligible for the program. In the following two code chunks, it is on you to determine those values. Just follow the steps in the code chunk and fill in the correct code.  
*This corresponds to p.426 in the paper.*
```{r "2.7 5"}
#< task
#Use the command filter() to create the subsample 
#data_deforest_noneligible of data_deforest with eligible==0
#>
data_deforest_noneligible = filter(data_deforest, eligible==0)
#< task
#Show the first six rows of data_deforest_noneligible
#>
head(data_deforest_noneligible)
#< task
#Calculate the mean of the variable d in this data set
#>
mean(data_deforest_noneligible$d)
```
As you can see, the baseline probability of deforestation is quite small (about $4.8 \%$). So a marginal effect of about $3.0$ percentage points increases this probability by more than a half.
#< info "Remark on the baseline probability of deforestation"
Please note that in the authors state a number which deviates a bit from the one we have obtained here for the baseline probability of deforestation. *(Cf. paper, p.426)* However, as is the STATA code made available by the authors the according calculation is not included, I cannot tell where this (small) deviation comes from.
#>

The marginal effect on the expected percentage of area deforested is given here under the condition that there is positive deforestation. The baseline number for this should therefore use the same condition. In addition to the data set above one should therefore filter the localities fulfilling this condition. As the needed computations are analogous to the example above, the according code is already given. Just run it.  
*This corresponds to p.426 in the paper.*
```{r "2.7 6"}
#< task
#Use the command filter() to create the subsample 
#data_deforest_noneligible of data_deforest with eligible==0
data_deforest_noneligible_posdefor = filter(data_deforest, eligible==0 & d==1)
#Show the first six rows of data_deforest_noneligible_posdefor
head(data_deforest_noneligible_posdefor)
#Calculate the mean of the variable pct_deforested in this data set
mean(data_deforest_noneligible_posdefor$pct_deforested)
#>
```
The baseline deforestation is almost $0.6 \%$ of the polygon area. Thus the results indicate that treatment increases the relative amount of deforestation by up to one third which is also quite a lot.

Alix-Garcia et al. run some further regressions as robustness tests which support the results. *(Cf. paper, table 5)* We won't reproduce them here in order not to overwhelm you with regression estimates and thus lose sight of the essential. However, if you are interested in those tests, have a look at the *'info' box* below.
#< info "Robustness tests"
The authors run three robustness regressions here:  
* ***prop_treat*** **instead of** ***treat:***The first one refers to the following (potential) problem with the command *ivtobit* in STATA and thus also with my function *Newey_SB_ivtobit*: It is dedicated to Tobit models with *continuous* endogenous variables. However, the authors apply it to the binary variable *treat*. To check whether the results are nevertheless reliable they run an additional regression using the continuous variable *prop_treat* instead of *treat*. *(Cf. paper, table 5, column (3), section III.D)* It describes the proportion of treated households among all households in a locality. *(Cf. paper, section III.E)* At first sight, the results seem to reveal a problem with the IV Tobit regression using *treat* because the marginal effects, which are fortunately significant as well, are larger. Actually, this is not a problem as the authors explain in a similar case in the paper because to make the marginal effects from the two regressions comparable, one should multiply the marginal effects estimates from the regression with dependent variable *treat_prop* with the average proportion of households treated in a locality. *(Cf. paper, p.426)*  Actually, in this case this approach indicates that the estimated marginal effects from the two regressions are consistent with each other.   
Please note that the authors run also the first stage regression of this IV Tobit regression. *(Cf. paper, table 3, column (5))* The estimates indicate that the choice of the fuzzy set of instruments (with *prop_treat* instead of *treat*) is well suited in this case as well because the instruments are significant and jointly significant and have the same (sensible) signs as the specifications with *treat*.  
* **IV OLS regressions:** Apart from that, Alix-Garcia et al. run two regressions analogously to the OLS regressions in the simple approach in *Exercise 2.5*: the first one uses the dependent variable *d* and the full sample and the second one uses the dependent variable *pct_deforested* and the sample with localities showing positive deforestation. This time they use **IV OLS** to account for the endogeneity of treatment. *(Cf. paper, table 5, columns (5) and (6))* Like in the according regressions in *Exercise 2.5* the first one gives an estimate of the marginal effect on the probability of deforestation which is similar to the one obtained by the IV Tobit specifications. The second one gives again a marginal effect on the expectation of percentage area deforested (given there is deforestation) which is larger than the one by IV Tobit but also again explainable by the missing consideration of the probability of deforestation. *(argumentation analogous to paper, p.425)*

So all in all, those further robustness tests don't make us reject any of the results obtained above.
#>
- - -
**All in all**, the results from the estimation using the fuzzy RD design are consistent with those from the simplest approach using the sharp RD design with eligibility as a proxy for treatment. They indicate that **treatment** with the program **increases both the probability of deforestation and the percentage of area deforested**.
- - -

One aspect to remark here is that in general, treatment effects measured by a classical RD design are only valid for subjects with a value of the running variable very close to the cutoff. This results from the basic idea of the RD design: If treatment is the only variable changing discontinuously at the cutoff, the localities very close to the cutoff are very similar despite treatment. So in spite of the fact that we cannot observe the same localities once treated and once not treated we can determine the (causal) treatment effect by just comparing the localities very close to the cutoff - treated versus untreated. This is why the measured effect is also only valid for a very small area around the cutoff. *(Cf. Angrist and Pischke (2015, Chapter 4))*  
*Jacob and Lefgren (2004, Chapter V)* treat the interpretation for the case using a RD design with excluded instruments that allow a fast but *continuous* increase in some marginal zone - like in this exercise. They come to the conclusion that the area for which the measured effect is valid is a bit larger, but still limited to a value of the running variable close to the cutoff.

So in our case, the estimated intention-to-treat effect from the simple approach in *Exercise 2.5* is only valid for localities with a marginality index close to the eligibility cutoff; the estimated treatment effect from the more elaborate approach of this exercise is valid "in the population that is just below the
marginality level required to be able to receive payments" *(paper, p.432)*. The effects measured are thus so-called **Local Average Treatment Effects (LATE)**. *(p.433)*

- - -
As a result from *Exercise 2* as a whole we can thus state that it suggests the following: **An increase in income through the participation in the program increases the pressure on forests given the population is quite poor**.
- - -

## Exercise [Q4] -- QUIZ on Exercises 2.5, 2.6 and 2.7
Finally, we have found reliable results on the question how the Oportunidades program affected deforestation - but have you really understood the line of argumentation? And do you still remember the new R commands? Test it in the following quiz!

- - -
#< quiz "Quiz Number 4"
parts:
  - question: 1. Using eligibility as a proxy for treatment and applying a sharp RD Design already gives estimates of the treatment effect.
    choices:
        - correct
        - wrong*
    multiple: FALSE
    success: Great! This is correct! Using eligibility instead of treatment means that we measure the intention-to-treat effect.
    failure: Try again. It might also help you to have a look at the introductory part of *Exercise 2.5* again.
  - question: 2. We use three specifications of the RD Design - both in the simple approach (eligibility as proxy for treatment) and in the elaborate approach (fuzzy RD design with special set of instruments) in order to avoid a misinterpretation of a steep polynomial function as discontinuity. Which result is the most desirable for the reliability of our results? Please complete. The estimated effects of the intention to treat/treatment of the three approaches...
    choices:
        - ...differ clearly from each other.
        - ...do not vary a lot.*
    multiple: FALSE
    success: Great! This is correct! This speaks in favor of the existence of a discontinuity at the cutoff.
    failure: Try again. It might also help you to have a look at *Exercise 2.5 a)* again.
  - question: 3. Why does the setting in this problem set deviate from the one for which we could use the classical fuzzy RD design?
    choices:
        - Enrollment does not increase discontinuously at the cutoff but increases quickly and continuously over a small interval.*
        - Enrollment does not increase from 0 to 1 at the cutoff.
    multiple: FALSE
    success: Great! This is correct! The fuzzy RD design can be perfectly used if the increase is smaller than 1, but still there should be some discontinuity. This is why we adapt the fuzzy RD design by using a special set of excluded instruments.
    failure: Try again. It might also help you to have a look at the introductory part of *Exercise 2.6*.
  - question: 4. There are several ways in R to obtain robust standard errors. The package 'sandwich' contains many commands on standard error-related questions. What is the command to compute an estimated heteroskedasticity-consistent covariance matrix? Please just give the command without its arguments or brackets.
    answer: vcovHC
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 2.6 a)* again.
  - question: 5. One hypothesis test in *Exercise 2.6* on the results of the OLS estimation of the first stage regression equation shows that the used instruments are *jointly significant*. How is the test called? (Please use a hyphen.)
    answer: F-test
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 2.6 a)* again.
  - question: 6. The results from *Exercise 2* indicate that treatment with the program increases both the probability and the amount of deforestation. This result cannot be simply generalized for people of arbitrary degree of poverty (marginality index).
    choices:
        - correct*
        - wrong
    multiple: FALSE
    success: Great! This is correct! The estimated effect is a "local average treatment effect". So it is valid only for those people who are very close to the eligibility cutoff and thus for very poor people.
    failure: Try again. It might also help you to have a look at the end of *Exercise 2.7* again.
#>

#< award "Good job!"
You have not just quickly skimmed through the last exercises  
but you know well why we did which steps and at which points to be careful,  
not forgetting your progress in R!  
Congratulations - this way you have mastered  
the first half of the problem set in a very profound way!
#>


## Exercise 3.1 -- Household-level investigations
In *Exercise 2* we found out that treatment with Oportunidades increased deforestation but so far it is not clear why. Which mechanism links the treatment with the program and deforestation?  
To answer this question Alix-Garcia et al. focus on the **household level** and look for a **microbehavioral explanation** of the observed increase in deforestation. This seems reasonable as the treatment actually addresses households through cash-transfers.

As the data set *"data_deforestation.rds"* contains only data on the locality level we need a **new data set** for the investigations on a micro-level explanation. This exercise is going to introduce the data set which will be used for this purpose: *"data_household.rds"*. Let's first have a look at where the data comes from:

### a) Background of the Progresa Data - a randomized experiment
In the initial phase of the program, which was called Progresa, some studies were conducted on the success of the program on the household level in parallel to the initial rollout of the program.  
The **experimental design** made use of the fact that the program was introduced in multiple stages over several years such that the treatment of those localities chosen to be treated actually started at different points in time. For the study, $24,000$ households from $506$ localities were surveyed several times (before and - if they were treated within this period - after the treatment). $320$ of those localities were chosen randomly from the group of localities with a starting point of treatment within the study period and $186$ were randomly chosen from those planned to be treated later. This ensured that **treatment was randomized at the locality level**. The first group of localities formed the *treatment group* whereas the second group of localities formed the *control group*.  
Within a locality, **treatment on the household level** was determined by surveys detecting if the household was *poor enough* to participate. Those poor households are in the following called "eligible". *Note that this differentiation on the household level was done both in the treatment group and the control group.* So regarding the total set of eligible households (from both groups), those from the treatment group were treated within the investigation period, whereas those from the control group were not treated within this time. The random choice about which of those households were treated makes it possible to determine the **treatment** (or intention-to-treat) **effect** and the **spillover effect**.  
*(Cf. Skoufias (2005, pp. XI, 2, Chapter 3); paper, section IV.A)*

Alix-Garcia et al. use the "baseline surveys" from 1997 and 1998 (beginning of the investigations) as well as the "follow-up survey" from 2000 (end of the study). *(paper, p.427)* The information is captured by the data set *"data_household.rds"* and is referred to as the *"Progresa data"* by the authors.
#< info "Remark on the name PROGRESA"
Note that originally the name Progresa refers to the program from 1997 to 2000 (its initial phase). In the paper Alix-Garcia et al. always use the name Progresa when referring to the evaluation study described above. For the usual rollout of the program they use the name Oportunidades - even though this name was just used under a new government. *(Cf. Skoufias (2005, p.13))* Nevertheless, it makes it easier to distinguish between the random evaluation study and the normal rollout as described in the last exercises if we just call the first one Progresa and the second one Oportunidades. So for practical purposes I stick to this naming in the problem set.
#>
In the following code chunk, please load the data set *"data_household.rds"* with the command *readRDS("filename")* and save it in the variable *data_household*.
```{r "3.1 a) 1"}
#< task
#Read in the data set and save it in the variable data_household
#>
data_household=readRDS("data_household.rds")
```
To get an idea about what the data set looks like, you can either run the following (optional) chunk or just click on the *'data' button* in the code chunk above.
```{r "3.1 a) 2", optional=TRUE}
#< task
#Show the first six rows of data_household
head(data_household)
#>
```
What you might notice first is that for some observations there are missing values. This is indicated by *NA* or by an empty field. In the regressions of this problem set, we will deal with this in the following way: If an observation does not contain the value of the variable which is of interest for the computation, it will be ignored.
#< info "Comment on this data set"
Please note that this data set is a **subset** of the original data set "ecological_footprint_household_analysis.dta" containing only the variables which will be used in the following regressions. Also note that again, the **variable names** were changed into **more intuitive** ones using speaking abbreviations based on the English language.
#>
The variables can be divided into three main groups:
* *treat*, *poor*, *post_year* state if a household was in the treatment group, eligible and if the observation was made in the posttreatment period
* *dens_road_inv* and variables ending in *"_densroadinv"* give information related to the infrastructure quantity of the locality a household belongs to
* *dpw_beef*, *dpw_milk*, *n_cows* etc. - some household-specific consumption and production variables

We will have a look at the first two groups of variables in this exercise, leaving the consumption and production variables to *Exercise 3.2*.

### b) Treatment and Eligibility in the Progresa Data
The variables defining to which categories (treated/control, eligible/noneligible) an observation belongs are the variables *treat* and *poor*. Being part of the treatment group is indicated by *treat* being equal to $1$ (for the control group $0$, respectively). Being poor enough to be eligible is indicated by *poor* being equal to *"pobre"* (Spanish for "poor"), being "too wealthy" by *poor* being equal to *"no pobre"* ("not poor"), respectively. *(Cf. available STATA code files)*  
The following code chunk plots the number of eligible and the number of noneligible households for both the treatment and the control group. To do so, first the new column *treatment* is added to the data frame which indicates treatment and control group as strings and not as numbers. This column is used for plotting because the original column *treat* would be treated like a continuous variable which does not look nice. The resulting data frame is called *d*. An explanation of the code is given in the two *'info' boxes* below.
```{r "3.1 b) 1"}
#< task
#Add a column indicating treatment and control group as strings
d = mutate(data_household, treatment = ifelse(treat==0, "control", "treat"))
#Load package 'ggplot2'
library(ggplot2)
#Plot histograms of poor households for treatment and control group
ggplot(d, aes(x=poor, group=treatment, fill = treatment)) +
  geom_bar( width = 0.5, position = "dodge") +
  ylab("Frequency")
#>
```
#< info "ifelse"
The command *ifelse(test, yes, no)* analyzes the condition given as first argument. If this condition is fulfilled, it gives back the term in *yes*, if it is not fulfilled, it gives back the term in *no*. Here, *ifelse* is used to assign the string *"control"* to the rows of the newly created column *treatment* in which *treat* equals $0$ and *"treat"* to all other rows.
#>
#< info "ggplot2: Graphs for different groups of observations - in the same figure"
*Please note that if you have gone through the optional Exercise 2.4, this is just a revision for you.*  
The package *'ggplot2'* offers the possibility to plot the same graphics (lines, points, histograms) for different groups of observations - if desired even in different colors. This is only possible if there is a variable which defines to which group the observation belongs. This variable has to be included in the aesthetics as argument *group*. Different colors for different values of the grouping variable can be obtained by handing the variable over to the argument *fill* (for filling color of bars) or *color* (for the contour of bars, lines etc.). If the grouping variable is not continuous one should use the argument *color = factor(grouping variable)* because this way, a discrete range of colors is chosen.
#>
The treatment group is given in blue color and the control group is given in red color. These are the original groups. In the next exercises we will form two new groups to run regressions: one of "poor" and thus eligible households and another of "not poor enough" households. This way treatment and control households can be compared whereby exogeneity of treatment is ensured by the randomization on the locality level.  
What doesn't become clear from this graphic but will play an important role later, is the fact that all observations are also categorized according to their date: If an observation was made after the treatment or before the treatment is given by the variable *post_year*. Please note that like in the chunk above, we won't use the column *post_year* for this purpose but the column *period* which contains strings indicating the pre- and posttreatment period. The code for this is already given. It is now your turn to create a graphic analogous to the one above which shows a histogram of the frequency of observations. This time again different colors shall denote the distinction between treatment and control group but there are two differences:  
* The $x$ axis shall denote the time aspect (*period*).  
* There shall be a separate histogram for the different levels of the factor variable *poor* (*pobre*, *no pobre* and *NA*). Information on how to realize this is given in the next *'info' box*.

Please use the data set *d* again for plotting.
#< info "ggplot2: Graphs for different groups of observations - in different figures"
There is a second possibility to create the same graphic for different groups of variables: Using a separate figure for each group. This can be done by adding the command *+ facet_wrap(~grouping variable)*. Please note that the respective variable does not have to be defined in the aesthetics.
#>
```{r "3.1 b) 2", fig.width = 12}
#< task
#Add a column indicating pre- and posttreatment period as strings
d = mutate(d, period = ifelse(post_year==0, 
                                         "pre_treatment", "post_treatment"))
#Plot histograms of post- and pretreatment observations
#for treatment and control group
#>
ggplot(d, aes(x=period, group=treatment, fill = treatment)) +
  geom_bar( width = 0.5, position = "dodge") +
  ylab("Frequency") +
  facet_wrap(~poor)
```
We can see that the numbers of observations are not equal across all four categories within one figure. In general there are more observations from treated households which makes sense because the treatment group consists of $320$ localities while the control group consists only of $186$. How these four categories in each figure will be used later on to detect the effect of treatment you will learn in the next exercise.

### c) Road density in the Progresa Data
The second group of variables will be important in the third main part of this problem set and concerns the *road density* of the locality a household belongs to. *Road density* is defined as the total length (in km) of roads "within a $10$ km buffer of each locality" *(paper, p.428)*. In the data set the variable *dens_road_inv* describes its inverse. All variables ending in "_densroadinv" are interaction terms containing this variable.  
In the following code chunk, please calculate the empirical quantiles of the column *dens_road_inv* in the data set *data_household* for a sequence of probabilities from $0$ to $1$ by steps of $0.5$. You can do so by using the command *quantile(x, probs=seq(from, to, by))* with *x* the element to be evaluated, and *probs* the sequence of probabilities. Save the result in the variable *q_densroadinv*. Additionally, calculate the mean of the variable and save it in *mean_densroadinv*. Show both variables.
```{r "3.1 c) 1"}
#< task
#Compute the empirical quantiles of dens_road_inv and show it
#>
q_densroadinv = quantile(data_household$dens_road_inv, probs=seq(0,1,0.5))
q_densroadinv
#< task
#Compute the mean of dens_road_inv and show it
#>
mean_densroadinv = mean(data_household$dens_road_inv)
mean_densroadinv
```
It seems tricky to interpret these values. This is why in the next code chunk, the inverse of each value is calculated. To get an idea about the meaning of the numbers, two further numbers are given: the length (in km) of a road running on a circle of radius $10$ km around the locality and the length (in km) of a road just passing through the circle of radius $10$ km around the locality. Just click on the *'check' button* to run the code. A remark on the $100\%$ quantile of *dens_road_inv* is given in the next *'info' box*.
#< info "Remark on dens_road_inv"
You might wonder why the $100\%$ quantile of *dens_road_inv* is not infinitely large but $1.0$. Studying the original data set ("ecological_footprint_household_analysis.dta") which includes a further column containing the real road density suggests the following: The inverse road density is not exactly calculated as the inverse of the road density, but as the inverse of *(road density + 1)*. This way the variable *dens_road_inv* reaches its maximum ($1.0$) when the road density is equal to $0$ meaning that there are no roads in the locality to which the household belongs.
#>
```{r "3.1 c) 2"}
#< task
#Calculate the inverse of q_densroadinv
1/(as.numeric(q_densroadinv))
#Calculate the inverse of mean_densroadinv
1/(mean_densroadinv)
#Perimeter of a circle with radius 10
2*pi*10
#Diameter of a circle with radius 10
2*10
#>
```

As you can see, on average the total length of roads within a buffer of $10$ km around a locality is about $20$ km. This is just the length of a road passing just once through the locality and its surrounding area - so a quite basic infrastructure. Still, there are localities with a smaller total length of roads. The largest length of roads is a bit less than twice the perimeter of a circle with radius $10$.

## Exercise 3.2 -- Household investigations - Looking for the mechanism
Households can respond to an increase in income in two ways: They can change their *consumption* behavior and/or their *production* behavior which in turn can influence their *deforestation* behavior. There are several approaches in the literature treating possible **mechanisms linking income increases and behavioral changes on the household level and deforestation**. For mechanisms concerning the **production** behavior of households have a look at the *'info' box* below.
#< info "Background on possible production mechanisms"
Alix-Garcia et al. give a short overview on possible ways how income changes might be related to production and deforestation *(cf. paper, section II.B)*, among others:  
* If people have more money on hand, they can invest in more effective agricultural methods. As the competition for land between agriculture and forests is an important factor for deforestation, this can slow down further deforestation. The results from some first studies (available in the early 1990s) on the effect of a more intense and sustainable agriculture on deforestation confirmed this expectation. *(Cf. Shortle and Abler (1999); World Bank (1992), especially pp.10,135)*  
* Given the competition for land between agriculture and forests it also seems intuitive that deforestation drops if off-farm labor becomes more attractive than on-farm labor. So if income changes are related to this change, they might lead to a release in pressure on forests. Deininger and Minten (1999,2002) consider the case of increased returns to off-farm labor. In contrast to that, Alix-Garcia et al. argue that in the case of the Mexican poverty alleviation program this mechanism is rather unlikely, but income increases might make on-farm labor less attractive by increasing the "opportunity cost of leisure" *(paper, p.419)*.
#>
The studies in the paper focus on a very simple mechanism - a **consumption-driven mechanism:**
- - -
As you know from *Exercise 2.2*, one main aspect in determining factors of deforestation is the competition for land area between forests and agriculture. Consistent with this idea, Alix-Garcia et al. focus on two competitive types of products in the context of a plausible mechanism: forest products and agricultural products. Foster and Rosenzweig *(2003)* found out that higher incomes increased demand of both types. As in their case, the increase of the demand of forest products is higher than the increase in the demand of agricultural products for a given income increase, they conclude that higher incomes finally reduce deforestation. Studies on the program *Progresa* (the "birth name" of Oportunidades) indicate however that the program increased the consumption of land-intensive agricultural goods like for example meat. This is why in the paper, both types of products are investigated in terms of consumption.  
Consumption in turn is related to deforestation through production. Put differently, the environmental consequences of the increased demand for specific products are visible where the according production increase of those products is located.  
*(Cf. paper, sections II.B and IV.A)*
- - -
### a) Consumption changes
Three variables are used to measure changes in demand: 
* *nrooms* indicates the number of rooms in a household and is used as a "proxy for timber demand" *(paper, p.427)*.
* *dpw_beef* indicates the days per week with consumption of beef (land-intensive product)
* *dpw_milk* indicates the days per week with consumption of milk (land-intensive product)

*(Cf. paper, table 6)*

As becomes clear from this choice of variables, Alix-Garcia et al. "concentrate on the demand for animal protein" here *(paper, p.429)*.

Given the household data set we can measure the **treatment effect on the consumption behavior/demand** using the subset of **poor households** and applying the **difference-in-difference estimator**.  
The following code chunk estimates the treatment effect on *n_rooms*. As treatment was determined on the locality level, the authors use standard errors clustered on the variable *code_loc*. *(Cf. paper, section IV.A)* This is why we use the command *felm(formula | 0 | 0 | clusterVariable, data, subset, ...)* from the package *'lfe'* instead of the *lm* command. Actually, such simple OLS regressions are not the real purpose of this function, but also work and the command offers an elegant way to compute clustered standard errors. For more information, see the *'info' box*. Run the code chunk by clicking on the *'check' button*. The econometric background and thus the meaning of the estimated coefficients will be explained underneath.
#< info "Clustered standard errors with felm"
The command *felm* is dedicated to linear models with multiple group fixed effects and offers a wide range of features. The structure of the command equals the structure of the *lm* command but there is a difference in the syntax of the formula:  
*y ~ x1 + x2 | [factors] | [IV specification] | [cluster specification]*  
As you can see, factors, instrumental variables and variables to be used for clustering have to be added in separate fields (denoted here in square brackets). If those specifications are not needed, they should be set to $0$. In our case the last field is of interest as it defines the cluster variable(s).
#>
*This corresponds to table 6, column (1) on p.429 in the paper.*
```{r "3.2 a) 1"}
#< task
#Read in the data set and save it in the variable data_household
data_household = readRDS("data_household.rds")
#Load package 'lfe'
library(lfe)
#DD estimator of treatment effect on n_rooms
dd_rooms = felm(n_rooms ~ treat + post_year + te | 0 | 0 | code_loc, 
                data = data_household, subset = (poor=="pobre"), 
                na.action=na.omit)
#Load package 'regtools'
library(regtools)
#Show the regression results
showreg(list(dd_rooms), 
        custom.model.names=c("Rooms in Home"), 
        digits=3, stars=c(0.01,0.05,0.1),
        include.adjrs=FALSE, include.rsquared=FALSE)
#>
```
The regression equation used in the OLS regression above can be written formally as follows *(paper, p.427)*:
$$y_{it} = \gamma_0 + \gamma_1 T_i + \gamma_2 P_t + \gamma_3 T_iP_t + \nu_{it}$$
with $i$ indicating the household and $t$ the point in time the observation belongs to, $y$ the dependent variable (in our case *n_rooms*), $T_i$ a dummy variable indicating if the household $i$ belongs to the treatment group (in our case *treat*) and $P_t$ a dummy variable indicating whether the observation was made in the posttreatment period (in our case *post_year*). The interaction term $T_i \times P_t$ is given by the variable *te* in our case.  *(Cf. paper, section IV.A)*  
The coefficient $\gamma_3$ of the interaction term indicates the **treatment effect** and is called the **difference-in-difference** estimator, short **DD estimator**. Let's have a look at the theory behind this estimation:
- - -
The idea of the **DD estimator** is to compare the development over time between a treatment and a control group while only the treatment group receives some treatment from some point in time on. It can be used under the assumption of "common trends" *(Angrist and Pischke (2015, p.184))* meaning that both groups would change in the same way over time if there was no treatment. The regression equation above is the general approach of determining the estimator, but its name becomes clear if you just look at the two further ways in which the coefficient $\gamma_3$ measuring the treatment effect can be written:
$$\hat{\gamma_3} = (E[y|T=1,P=1] - E[y|T=1,P=0]) - (E[y|T=0,P=1]-E[y|T=0,P=0])$$
$$\hat{\gamma_3} = (E[y|T=1,P=1] - E[y|T=0,P=1]) - (E[y|T=1,P=0]-E[y|T=0,P=0])$$
In the first representation the treatment effect is measured by looking at the difference in the expected values of $y$ in the treatment group ($T=1$) after ($P=1$) and before ($P=0$) the treatment taking into account that there might be changes in $y$ over time which don't result from being treated and can therefore be captured by the temporal change of $y$ in the control group ($T=0$).  
In the second representation the treatment effect is measured by comparing the difference in $y$ in the treatment and control group taking into account that there might be systematic, static differences between these two groups.  
*(Cf. Wooldridge (2010, Chapter 6.5.2); Angrist and Pischke (2015, Chapter 5))*
- - -
The regression results above indicate that concerning the variable *n_rooms* there is no significant treatment effect. So within the limits of the data we cannot find a significant effect on the **demand of forest products**.

Let's have a look at the two variables describing the **demand of agricultural products:** *dpw_beef* and *dpw_milk*. It is your turn now to perform the two according regressions for the DD estimator analogous to the one in the last code chunk. Templates of the regressions are already given such that you only have to fill in the missing parts and uncomment the lines. The code showing the results is also given.  
*This corresponds to table 6, columns (3) and (5) on p.429 in the paper.*
```{r "3.2 a) 2"}
#< task
#Regression with dependent variable dpw_beef and clustered standard errors
#dd_beef = felm(... , data = data_household, subset = (poor=="pobre"),
#               na.action=na.omit)
#Regression with dependent variable dpw_milk and clustered standard errors
#dd_milk = felm(... , data = data_household, subset = (poor=="pobre"), 
#               na.action=na.omit)
#>
dd_beef = felm(dpw_beef ~ treat + post_year + te | 0 | 0 | code_loc, 
                data = data_household, subset = (poor=="pobre"), 
               na.action=na.omit)
dd_milk = felm(dpw_milk ~ treat + post_year + te | 0 | 0 | code_loc, 
                data = data_household, subset = (poor=="pobre"), 
               na.action=na.omit)
#< task
#Show the results of all three regressions
showreg(list(dd_rooms, dd_beef, dd_milk), 
        custom.model.names = c("Rooms in Home", "Days Ate Beef", 
                               "Days Drank Milk"), 
        digits=3, stars=c(0.01,0.05,0.1),
        include.adjrs=FALSE, include.rsquared=FALSE)
#>
```
As you can see from the second and third column of the results, **treatment changes the consumption of beef and milk significantly.**

In the following we are going to **visualize the effects of treatment on consumption**, so the effect on the variables *dpw_beef* and *dpw_milk* when *te* changes from $0$ to $1$. For this purpose one can use the command *effectplot(reg, vars, ...)* from the package *'regtools'*. Here, the input argument *reg* stands for regression results and *vars* for the explanatory variables of which the effect shall be plotted. Please note that the argument *vars* has to be given as a character vector. Unfortunately, the *felm* command is not supported as input regression. So we will first estimate *dd_beef_lm* and *dd_milk_lm* with the command *lm* again which gives us the same results with the exception of the standard errors which are not clustered. The following code chunk estimates those two regressions. Just click on the *'check' button*. If you are not familiar with the command *effectplot*, have a look at the *'info' box* to understand the interpretation below.

#< info "Effectplot"
The command *effectplot(reg, vars, numeric.effect, ...)* from the package *'regtools'* visualizes the effect of the variables indicated in *vars* on the dependent variable for the regression given in *reg*. It takes into account that the change an explanatory variable can induce in the dependent variable does not only depend on the size of the estimated partial effect (in the linear model $\hat{\beta}$) but also on the value range of this explanatory variable. What is thus shown in the bar graph given back by *effectplot()* is the estimated change in the dependent variable if the explanatory variable changes from some lower quantile to an upper quantile. The default are the $10 \%$ and $90 \%$ quantiles. The argument *numeric.effect="10-90"* sets these parameters.  
If the explanatory variable only contains the values $0$ and $1$ and shall thus be treated as a dummy variable (argument *dummy01 = TRUE*, the default), the expected change in the dependent variable for a change in the explanatory variable from $0$ to $1$ is shown.
#>
```{r "3.2 a) 3"}
#< task
#OLs regressions for dpw_beef and dpw_milk
dd_beef_lm = lm(dpw_beef ~ treat + post_year + te,
             data = data_household, subset = (poor=="pobre"), 
             na.action=na.omit)
dd_milk_lm = lm(dpw_milk ~ treat + post_year + te,
             data = data_household, subset = (poor=="pobre"), 
             na.action=na.omit)
#>
```
With the results from those regressions, please use the command *effectplot* to plot the treatment effects (effects of variable *te* on *dpw_beef* and *dpw_milk*) from both regressions *dd_beef* and *dd_milk* in the following code chunk. The code for showing the plots next to each other with comparable axis limits is already given. Finally, use the command *grid.arrange* to plot both graphics above each other. 

```{r "3.2 a) 4"}
#< task
#Create the plot p_beef showing the treatment effect from dd_beef_lm
#>
p_beef = effectplot(dd_beef_lm, vars="te")
#< task
#Create the plot p_milk showing the treatment effect from dd_milk_lm
#>
p_milk = effectplot(dd_milk_lm, vars="te")
#< task
#Load package 'gridExtra'
library(gridExtra)
#Plot both graphics above each other with comparable axis limits
grid.arrange(p_beef + scale_y_continuous(limits=c(0,0.5)), 
             p_milk + scale_y_continuous(limits=c(0,0.5)))
#>
```
Treatment increases the variable *dpw_beef* (*dpw_milk*) in expectation by about $0.114$ ($0.337$) which means that within about two months (three weeks) there is in expectation one more day of eating beef. This effect seems to be very small in absolute terms. So again, we should keep in mind the usual level of consumption of those products:

As reference value Alix-Garcia et al. take the **"mean dependent variable in baseline"** *(paper, table 6)* for each of these variables, the average demand of the products in the pre-treatment period. In the following we will compute this value for all three variables and use the pipe operator *%>%* for it. If you are not used to it, have a look at the *'info' box*.
#< info "Pipe operator"
The pipe operator *%>%* can be used to avoid nested commands. It takes the output of the function in front of it and hands it over to the function after it as the first input argument. This way, multiple (two or more) commands can be linked together in a clearer way. If you write the command over several lines (which is recommended for better readability), insert a line break after each pipe operator.
*Example:*  
```{r "Pipe"}
#Load package 'dplyr'
library(dplyr)

readRDS("data_household.rds") %>%
  filter(poor=="pobre" & n_rooms != "NA") %>%
  select(n_rooms) %>%
  unique()
```
This command takes the household data set from the file *"data_household.rds"*, filters the rows for which the variable *poor* equals *"pobre"* and then calculates the mean of the variable *n_rooms* of those filtered observations.  
A nice overview on pipe operators is given on the following site:  
<http://seananderson.ca/2014/09/13/dplyr-intro.html>
#>
In the following code chunk, the mean of the variable *dpw_beef* (*base_beef*) and the mean of the variable *dpw_milk* (*base_milk*) shall be calculated for the poor population in the pre-treatment period. The code for the computation of *base_beef* is already given in a nested form. Try to understand the code and to replicate the calculations to obtain *base_milk*, but this time using pipe operators.  
As commands from the package *'dplyr'* are used for this purpose, the following *'info' box* gives a summary of the most important commands in *'dplyr'* you have already got to know throughout this problem set. Don't hesitate to use the *'hint' button*.
#< info "Data manipulation with 'dplyr' - overview"
* *mutate(data=..., column=...)* returns the original data frame with a new column added or an existing column changed - depending on the column indicated as argument.
* *group_by(data=..., variable=...)* "groups" the data according to the variable given as argument such that the surrounding command is computed for all observations with the same value of this variable.
* *filter(data, predicates)* filters the rows from the data frame for which the conditions given by the predicates are fulfilled.
* *select(data, columns)* keeps (only) the columns given as arguments with positive sign or removes those with negative sign.
* *summarize(data=..., column=...)* does the same as *mutate* with the difference that it only gives back the newly computed values. If combined with *group_by* the resulting data frame contains as many elements as there are unique values of the variable used for grouping.
#>
*This corresponds to table 6, columns (3) and (5) on p.429 of the paper.*
```{r "3.2 a) 5"}
#< task
#Compute the variable base_beef
summarize(
  filter(data_household, post_year==0 & poor=="pobre" & dpw_beef!="NA"),
  base_beef=mean(dpw_beef))
#Compute the variable base_milk with the pipe operator
#>
data_household %>%
  filter(post_year==0 & poor=="pobre" & dpw_milk != "NA") %>%
  summarize(base_milk = mean(dpw_milk))
```
As you can see, the consumption of milk and above all of beef had been very low on average before the treatment: On average, people consumed beef once in about three weeks and milk about twice in three weeks. This means that the **relative increase in demand caused by treatment is quite high**: A higher income through treatment increases the frequency of eating beef by about one third of the baseline frequency and the frequency of eating milk by about one fourth compared to the average frequency before the treatment period.
- - -
So according to our estimates, there is **no significant effect of treatment on the demand of forest products**, but there is a **positive, significant effect** of treatment **on the demand of agricultural products**.
- - -
### b) Plausibility check: Consumption as driving reason for deforestation?
Remember that we are actually looking for a microeconomic explanation of the increase in deforestation in response to the program. The results up to now indicate that treatment with the program leads to an increase in the households' consumption of land-intensive goods. This already leads us closer to our goal. With the concrete quantitative estimates of the treatment effect on consumption we can check now whether the estimated size of the deforestation increase in response to treatment with the program *(Exercise 2)* makes sense given the estimated increase in consumption in response to treatment *(current exercise)*.

Alix-Garcia et al. conduct a small study on this question. They first use various estimates to get an **approximate number of the additional land size per locality** to support the estimated increase in beef and milk production: almost $0.25$ square kilometers. *(Cf. paper, p. 430)*   
Further they simulate the **expected average area deforested per locality** in response to treatment. In the code chunk below we will replicate these calculations. To make it comprehensible, we will use estimates we have calculated ourselves, even though this is not exactly (but comparable to) what the others use: In the fourth-order polynomial regression specification in the most elaborate approach (*Exercise 2.7*) the estimated marginal effect on the expected percentage of polygon area deforested given there is deforestation was $0.180\%$. Multiplying this with the average polygon size among treated localities with positive deforestation and with the proportion of those localities with positive deforestation among the treated localities gives an estimate of the area deforested due to treatment. Just go through the instructions in the next code chunk and run the necessary calculations for the last two factors. *(Cf. paper, p.430)*

*This corresponds to the verbal description on p.430 in the paper.*
```{r "3.2 b) 1"}
#< task
#Read in the data and save it as data_deforest
data_deforest = readRDS("data_deforestation.rds")
#>
#< task
#Use the command filter to take the subset data_treat
#with treat=1 of data_deforest
#>
data_treat = filter(data_deforest, treat==1)
#< task
#Show the first six rows of data_treat
#>
head(data_treat)
#< task
#Take the exponential of the column ln_area_polyg in the subset of data_treat
# with d=1 and save it in area_treat_d
#>
area_treat_d = exp(subset(data_treat, d==1)$ln_area_polyg)
#< task
#Take the mean of area_treat_d
#>
mean(area_treat_d)
#< task
#Take the mean of column d in data_treat which gives you the fraction of
#localities with positive deforestation within the treatment group
#>
mean(data_treat$d)
```

So the estimated increase in deforestation is approximately $0.0018 \cdot 31.924 \cdot 0.094 \approx 0.005$ square kilometers. This is much smaller than the estimated additional land needed for agricultural purpose due to the increase in consumption given at the beginning of this plausibility check ($\approx 0.25 \thinspace km^2$).  
Alix-Garcia et al. argue that great parts of the additional land needed might not have been forested before and that the effect on deforestation might be underestimated. *(Cf. paper, p.430)* *An explanation of why the effect might be underestimated will be given in Exercise 3.6.*  
- - -
So far, a **demand-driven increase in deforestation** in response to treatment seems **plausible to be a major explanation of the observed treatment effect** because  
* there is a positive, significant effect of treatment on the consumption of the land-intensive products beef and milk  
* the estimated increase in deforestation in response to treatment makes sense given the estimated increase in the demand of agricultural products

## Exercise 3.3 -- Production changes - a simple approach
Consumption by itself does not increase deforestation. A further intermediate step is needed to relate the two of them: production. So in the following, let's see whether we can observe **local production changes** as a result of treatment and as possibly responsible for the increase in deforestation.  
In contrast to consumption changes, for which it was sufficient to study the eligible (poor) households, we now have to look at both poor and wealthier households to study the changes in production. Why? The rise in demand we detected in the last exercise doesn't have to be sourced (as a whole) from the treated households but could be sourced from neighboring, untreated households in the same locality as well. *(Cf. paper, sections II, IV)* And, both a change in production among the eligible, poor households and a change in production among the ineligible, wealthier households from the same locality could lead to the local increase in deforestation we measured in the locality-level approach from *Exercise 2*. We call the effect of treatment on the behavior of the treated households **treatment effect** and the effect on the behavior of the "too wealthy" households **spillover effect**. *(Cf. paper, p.427)*  
For the estimation of both kinds of effects we again use the difference-in-difference approach.

There are three variables used for measuring production:
* *n_plots* indicates the number of plots used for production  
* *ln_hectares* indicates the natural logarithm of 1 + number of hectares of land used for production  
* *n_cows* indicates the number of cows owned by a household

*(Cf. paper, tables 7,8 and p.428)*

As you can see, the chosen variables belong to the production of agricultural goods. The production of timber products need not be considered here anymore because we could not find any significant effect of treatment on the demand of timber products.

### a) Treatment effect on production
Let's first focus on the **treatment effect** which means that we use the subset of **poor** households. *(Cf. paper, p.427)* As we want to estimate a simple DD estimator again, we use the same regression equation as in the last exercise *(paper, p.427)*:
$$y_{it} = \gamma_0 + \gamma_1 T_i + \gamma_2 P_t + \gamma_3 T_iP_t + \nu_{it}$$
with $i$ indicating the household and $t$ the point in time the observation belongs to, $T_i$ indicating treatment or control group and $P_t$ indicating the pre- or posttreatment period. *(Cf. paper, p.427)* This time the dependent variables of the three regressions are given by the variables measuring production as introduced above.  
The following code chunk loads the data set again and runs the OLS regressions to determine the DD estimator. As the R code is analogous to the regression code in the last exercise, it is already given here. Please click on the *'check' button* to run the code.  
*This refers to paper, table 7, columns (1), (3), (5) on p.429 in the paper.*
```{r "3.3 a) 1"}
#< task
#Read in the data set and save it in the variable data_household
data_household = readRDS("data_household.rds")
#Load package 'lfe'
library(lfe)
#Estimate OLS regressions with dependent variable n_plots, ln_hectares, n_cows
#for the poor households
dd_plots_poor = felm(n_plots ~ treat + post_year + te | 0 | 0 | code_loc, 
                data = data_household, subset = (poor=="pobre"), 
                na.action=na.omit)
dd_lnhectares_poor = felm(ln_hectares ~ treat + post_year + te |0|0|code_loc, 
                          data = data_household, subset = (poor=="pobre"), 
                          na.action=na.omit)
dd_cows_poor = felm(n_cows ~ treat + post_year + te | 0 | 0 | code_loc, 
                data = data_household, subset = (poor=="pobre"), 
                na.action=na.omit)
#Load package 'regtools'
library(regtools)
#Show the results of all three regressions
showreg(list(dd_plots_poor, dd_lnhectares_poor, dd_cows_poor), 
        custom.model.names=c("Number of Plots", "ln(1+Hectares)", 
                             "Number of Cows"), 
        digits=3, stars=c(0.01,0.05,0.1),
        include.adjrs=FALSE, include.rsquared=FALSE)
#>
```
As you can see from the results, only the coefficients of the dummy variable indicating the posttreatment period *post_year* are significant: For reasons unrelated to treatment the number of plots as well as the number of cows decrease a bit from the pre- to the posttreatment period while the logarithm of the total hectares of land increases. For all three production variables the treatment effect is not significant. So we **cannot observe a significant treatment effect on production**.

### b) Spillover effect on production
What about neighboring, **wealthier households** in treated localities? Do they show a significant change in production in the treatment period compared to the control group of wealthier households in untreated localities? The next code chunk runs the according regressions. Actually, these regressions only differ in one aspect from the regressions above.
- - -
#< quiz "Spillover vs. treatment"
question: Which argument of the *felm* command do we have to change in order to measure the spillover effect instead of the treatment effect?
choices:
    - formula
    - data
    - subset*
multiple: FALSE
success: Great! This is correct! The subset of wealthier households has to be used instead of the subset of poor households, so *subset = (poor=="no pobre")*. The coefficient on *te* then measures the spillover effect. *(Cf. paper, p.427)* 
failure: Try again. Think about the difference between the spillover and the treatment effect.
#>
- - -
The necessary adaptions are already given in the next code chunk. Have a look at the code and then run it by clicking on the *'check' button*.  
*This corresponds to table 8, columns (1), (3), (5) on p.430 in the paper.*
```{r "3.3 b) 1"}
#< task
#Estimate OLS regressions with dependent variable n_plots, ln_hectares, n_cows
#for the wealthier households
dd_plots_w = felm(n_plots ~ treat + post_year + te | 0 | 0 | code_loc, 
                data = data_household, subset = (poor=="no pobre"), 
                na.action=na.omit)
dd_lnhectares_w = felm(ln_hectares ~ treat + post_year + te | 0 | 0 | code_loc, 
                data = data_household, subset = (poor=="no pobre"), 
                na.action=na.omit)
dd_cows_w = felm(n_cows ~ treat + post_year + te | 0 | 0 | code_loc, 
                data = data_household, subset = (poor=="no pobre"), 
                na.action=na.omit)
#Show the results of all three regressions
showreg(list(dd_plots_w, dd_lnhectares_w, dd_cows_w), 
        custom.model.names=c("Number of Plots", "ln(1+Hectares)", 
                             "Number of Cows"), 
        digits=3, stars=c(0.01,0.05,0.1),
        include.adjrs=FALSE, include.rsquared=FALSE)
#>
```
Like in the regressions for the poor households, only the coefficient of the dummy on the posttreatment period is significant: For reasons different from treatment the number of plots and the number of cows are smaller (on average) among wealthier households after the neighboring households have been treated than before; in contrast, the area in hectares is larger. Besides, we **cannot detect a significant spillover effect** for any of the three production variables.
- - -
So, to **sum up the results** from the previous and the current exercise:  
An increase in household income through treatment with the program leads to a **higher demand in agricultural, land-intensive products**. At the same time **no significant change in the local production** can be measured - neither among the treated households themselves nor among their untreated neighbors.

Does this mean that we have not found the underlying mechanism of how treatment leads to an increased deforestation yet? Or is it just that we haven't dug deeply enough? The next exercise will enlighten this.

## Exercise [Q5] -- QUIZ on Exercises 3.1, 3.2 and 3.3
In the last three exercises you have got to know the second data set used in this problem set and you have conducted some first studies to find out a possible mechanism linking the conditional cash transfer program and deforestation.
In the following quiz, you can check whether you have understood the main ideas and still remember the new R functions. Good luck!

- - -
#< quiz "Quiz Number 5"
parts:
  - question: 1. Please complete the following sentence. Several answers might be correct. The Progresa Data refers to...
    choices:
        - the very early phase (first three years) of the Mexican poverty alleviation program*
        - observations on the locality level
        - an experimental study on the success of the program*
    multiple: TRUE
    success: Great! This is correct! The only wrong statement here is the second one because the observations are given on the household level.
    failure: Try again. It might also help you to have a look at the beginning of *Exercise 3.1* again.
  - question: 2. When running regressions based on the Progresa Data to estimate the treatment effect there is again the same risk like in Exercise 2 that the results could be biased.
    choices:
        - correct
        - wrong*
    multiple: FALSE
    success: Great! This is correct! Fortunately, in the Progresa Data treatment on the locality level was assigned randomly.
    failure: Try again. It might also help you to have a look at *Exercise 3.1 a)* again.
  - question: 3. Alix-Garcia et al. study the change in household behavior in response to treatment to find out the mechanism linking treatment and deforestation. On what kind of mechanism do they focus in their studies?
    choices:
        - production-driven mechanism
        - consumption-driven mechanism*
    multiple: FALSE
    success: Great! This is correct! Production only plays a role in their studies because it is the location of the according production change needed to satisfy the increase in consumption where the change in deforestation finally takes place. However, the driving force in the mechanism linking treatment and deforestation which is considered here is consumption.
    failure: Try again. It might also help you to have a look at the introductory part of *Exercise 3.2* again.
  - question: 4. One possibility to run an OLS regression with clustered standard errors is to use the command *felm* from the package *'lfe'*. What is the correct syntax of the command for regressing *y* on *x* and using the variable *cl* for clustering and the data set *data*?
    choices:
        - felm(y ~ x, cluster = "cl", data=data)
        - felm(y ~ x | 0 | 0 | cl, data=data)*
        - felm(y ~ x | cl, data=data)
    multiple: FALSE
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 3.2 a)* again.
  - question: 5. Please complete the following sentence. In the case of how we apply the DD estimator, it considers the difference in deforestation between...
    choices:
        - poor and wealthier households
        - treatment and control group
        - pre- and posttreatment period
        - poor and wealthier households as well as between pre- and posttreatment period
        - treatment and control group as well as between pre- and posttreatment period*
    multiple: FALSE
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 3.2 a)* again.
  - question: 6. OLS coefficient estimates are not always directly comparable. Which function from the package *'regtools'* have you got to know that overcomes this problem by showing the effect of each variable on the dependent variable within the scope of the data set you work on? Please only write down the command without any arguments or brackets.
    answer: effectplot
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 3.2 a)* again.
  - question: 7. We do not only study changes of agricultural production within the group of poor/eligible households, who can be treated with the program, but also production changes among the ineligible households, who are not poor enough. Why is this sensible? Multiple answers might be correct.
    choices:
        - because the increase in consumption due to treatment can also be sourced by neighboring non-treated households*
        - because both an increase in production among poor as well as among wealthier households in a treated locality could be the cause for the local increase in deforestation*
        - because the wealthier households own all plots usable for agricultural production
    multiple: TRUE
    success: Great! This is correct! The question about who actually sources the increase in consumption and thus where the environmental effect of the program takes place will accompany us through the rest of *Exercise 3*.
    failure: Try again. It might also help you to have a look at the introductory part of *Exercise 3.3* again.
  - question: 8. It is possible to estimate the treatment (or intention-to-treat) effect by using the subset of ... households or the spillover effect by using the subset of ... households. Please choose the correct completion.
    choices:
        - poor, wealthier*
        - wealthier, poor
    multiple: FALSE
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 3.3* again.
#>

#< award "Good start into the second half!"
    You know well the data set and the main estimation method used throughout *Exercise 3*  
    and seem to have understood the first steps on our way to a  
    household-level explanation of the program's effect on deforestation.  
    Well done! This is a good start in this second main part of the problem set!
#>

## Exercise 3.4 -- The role of infrastructure - Production/Consumption
In the last two exercises we found out that an increase in income due to treatment is associated with an increase in the demand of the agricultural products beef and milk on the household level. As goods are traded within markets, this demand does not have to be sourced by the treated households alone but can also be sourced by other members of the market. In the last exercise we therefore also considered untreated households of the same localities to see whether their production increased due to the treatment of their poorer neighbors.

But who says that the increased demand has to be sourced from inside the same locality? The more infrastructure there is, the better a locality is connected to other localities which means that the market is larger. This way, the goods needed to meet the risen demand can be bought from and thus produced in a larger area. The production effect is thus spread and it might be more difficult to see any effect in the treated locality itself. *A deeper insight into the theoretical background of this line of argument and its sources will be given in Exercise 3.6.* So a possible explanation why we could not observe any significant treatment or spillover effect on production in the last exercise is that the effect could probably be too small if it is averaged across the localities' levels of road density.  
In the following we will therefore no more assume the effect on production to be the same for all localities (homogeneous in infrastructure quantity) but allow the **local effect on production** to be **heterogeneous** with respect to **road density as a "proxy for market connectedness"** *(paper, p.428)*. Let's see whether this more differentiated approach provides some evidence for the microeconomic explanation of a demand-driven increase in deforestation and at the same time more information about where the risen demand is met and thus where the environmental consequences take place.

### a) A heterogeneous estimation approach
The following regression equation can be used to estimate a DD estimator varying in road density *(paper, p.428)*:
$$y_{it}= \beta_0 + \beta_1 T_i + \beta_2 P_t + \beta_3 (T_i \times P_t) + \beta_4 R_i + \beta_5 (R_i \times T_i) + \beta_6 (R_i \times P_t) + \beta_7 (R_i \times T_i \times P_t) + \epsilon_{it}$$
Here, $R_i$ indicates the inverse of the road density (in the data set *dens_road_inv*) with road density being the total length of roads "within a $10$ km buffer" *(paper, p.428)* around the locality household $i$ belongs to. All other variables are defined as before.  
In our data set, all variables and interaction terms are given as separate variables: $R_i \times T_i$ by *treat_densroadinv*, $R_i \times P_t$ by *postyear_densroadinv* and $R_i \times T_i \times P_t$ by *te_densroadinv*. So theoretically we could proceed like in *Exercise 3.2* writing down all variables and interaction terms as single explanatory variables (additively) in the *felm* formula. This gives correct results but needs a long formula. The more elegant alternative in R is to just write *treat* * *post_year* * *dens_road_inv*. This creates the same regression (for details see *'info' box*).
#< info "Interaction terms and crosses in lm/felm formula"
The syntax for the right-hand side of the formula for *lm* or *felm* (part with ordinary variables) is as follows:  
* $x_1 + x_2$ stands for $\beta_1 x_1 + \beta_2 x_2$ (plus intercept)
* $x_1 : x_2$ stands for $\gamma \left(x_1 \cdot x_2\right)$ (interaction of $x_1$ with $x_2$)
* $x_1 * x_2$ stands for $\delta_1 x_1 + \delta_2 x_2 + delta_3 \left(x_1 \cdot x_2 \right)$ (the cross of $x_1$ and $x_2$)

Here, $\beta_1$, $\beta_2$, $\gamma$ and $\delta_1$, $\delta_2$, $\delta_3$ are coefficients.
#>
Again, we will investigate both how treatment changes the production among poor households and among wealthier neighboring households.

### b) Production - Heterogeneous estimation of treatment effect
First, we are going to use the heterogeneous approach from above to estimate road density-sensitive treatment and spillover effects on **production**. We are going to start considering the subset of **eligible (poor) households** to determine the **road density-sensitive treatment effect**.  
In the following code chunk, it is your task to run the OLS regression of the heterogeneous estimation approach for the dependent variable *n_plots*. Make use of the simplified writing of the cross product introduced above. As Alix-Garcia et al. again use standard errors clustered according to the variable *code_loc* for this regression, please do so as well. *(Cf. paper, section IV.A)* You can use the given template for this purpose, fill in the missing parts and uncomment it. The code loading the data set and the package *'lfe'* is already given.  
*This corresponds to table 7, column (2) on p.429 in the paper.*
```{r "3.4 b) 1"}
#< task
#Read in the data set and save it in the variable data_household
data_household = readRDS("data_household.rds")
#Load package 'lfe'
library(lfe)
#Estimate OLS regression with dependent variable n_plots with standard errors
#clustered according to code_loc for the poor households
#dd_plots_poor_rd = felm(formula = ...,
#                        data = data_household, subset = ...,
#                        na.action=na.omit)
#>
dd_plots_poor_rd = felm(n_plots ~ treat*post_year*dens_road_inv|0|0|code_loc,
                        data = data_household, subset = (poor=="pobre"),
                        na.action=na.omit)
```

In the next code chunk, the same is done for the other two production variables *ln_hectares* and *n_cows*. Just click on the *'check'* button to run the regressions and to see the results of all three regressions.  
*This corresponds to table 7, columns (4) and (6) on p.429 in the paper.*
```{r "3.4 b) 2"}
#< task
#Estimate OLS regressions with dependent variable ln_hectares, n_cows
#with standard errors clustered according to code_loc for the poor households
dd_lnhectares_poor_rd = felm(ln_hectares ~ 
                               treat*post_year*dens_road_inv|0|0|code_loc,
                             data = data_household, subset = (poor=="pobre"), 
                             na.action=na.omit)

dd_cows_poor_rd = felm(n_cows ~ treat*post_year*dens_road_inv|0|0|code_loc,
                       data = filter(data_household, poor=="pobre"),
                       na.action=na.omit)

#Load package 'regtools'
library(regtools)
#Show the results of all three regressions
showreg(list(dd_plots_poor_rd, dd_lnhectares_poor_rd, dd_cows_poor_rd), 
        custom.model.names=c("Number of Plots", "ln(1+Hectares)", 
                             "Number of Cows"),
        digits=3, stars=c(0.01,0.05,0.1),
        include.adjrs=FALSE, include.rsquared=FALSE)
#>
```
As we are interested in the treatment effect measured by the DD estimator $\widehat{\delta_{DD}}$ the coefficient estimates of interest are those of *treat:post_year* ($\widehat{\beta_3}$) and *treat:post_year:dens_road_inv* ($\widehat{\beta_7}$). This is because in this model the **difference-in-difference estimator of the treatment/spillover effect** is given by
$$\widehat{\delta_{DD}} = \widehat{\beta_3} + \widehat{\beta_7} R_i$$
Here, $\widehat{\beta_3}$ is the estimated effect for the case in which the inverse road density is close to $0$ (very much infrastructure) and $\widehat{\beta_7}$ describes how the effect changes with the inverse road density. *(Cf. paper, p.428)* This can be shown by using again the definition of the DD estimator introduced in the last exercise (difference in differences in expectation values). *If you are interested in the according derivation, have a look at the 'info' box.*

#< info "DD estimator varying in R"
Let's call the DD estimator $\widehat{\delta_{DD}}$.  
We take the second equation introduced in *Exercise 3.2* as definition of $\delta_{DD}$. (Taking the first equation works analogously.)
$$\widehat{\delta_{DD}} = \left(E\left[y|T=1,P=1\right] - E\left[y|T=0,P=1\right]\right) - \left(E\left[y|T=1,P=0\right]-E\left[y|T=0,P=0\right]\right)$$
In the following, we calculate the four expectation values for the regression equation with road density:

$$E\left[y|T=1,P=1\right] = \widehat{\beta_0} + \widehat{\beta_1} + \widehat{\beta_2} + \widehat{\beta_3} + \left(\widehat{\beta_4} + \widehat{\beta_5} + \widehat{\beta_6} + \widehat{\beta_7}\right) R_i $$
$$E\left[y|T=0,P=1\right] = \widehat{\beta_0} + \widehat{\beta_2} + \left(\widehat{\beta_4} + \widehat{\beta_6}\right) R_i $$
$$E\left[y|T=1,P=0\right] = \widehat{\beta_0} + \widehat{\beta_1} + \left(\widehat{\beta_4} + \widehat{\beta_5} \right) R_i $$
$$E\left[y|T=0,P=0\right] = \widehat{\beta_0} + \widehat{\beta_4} R_i $$

Inserting those equations into the definition and simplifying it, we obtain
$$\widehat{\delta_{DD}} = \left(\widehat{\beta_1} + \widehat{\beta_3} + \widehat{\beta_5} R_i + \widehat{\beta_7} R_i\right) - \left(\widehat{\beta_1} + \widehat{\beta_5} R_i \right)$$
$$\widehat{\delta_{DD}} = \widehat{\beta_3} + \widehat{\beta_7} R_i$$
This is what was stated above.
#>
Both components of the DD estimator, the coefficients on *treat:post_year* and *treat:post_year:dens_road_inv*, are not significant for the variables describing agricultural land - like in the homogeneous model estimation. For the variable describing the number of cows the coefficient $\widehat{\beta_3}$ describing the treatment effect for very well connected households is not significant either and the coefficient $\widehat{\beta_7}$ is only significant on the $10\%$ level which is quite weak. Further, the authors argue with the help of further computations that it is "unlikely that recipient households are supplying their entire increase in demand". *(paper, p.428)* So one can say that **regardless of the quantity of infrastructure no "substantial increase in agricultural production"** *(paper, p.428)* can be measured **among treated households**. The authors further give a possible explanation for this observation: The payments of the poverty alleviation program are conditional on children attending school (among other conditions). Thus the common practice of using child labor cannot be practiced easily anymore which makes an increase in production more difficult. *(Cf. paper, p.428)*

If treatment increases consumption among the treated households but seems not to increase their production in an adequate way to supply this consumption increase as a whole, this means that the increased consumption at least in parts has to be sourced by other households.

### c) Production - Heterogeneous estimation of spillover effect
Let's see whether and how the **spillover effect** on production changes with infrastructure quantity. The following code chunk runs the same regressions as above for the subset of **wealthier (noneligible) households**.  
*This corresponds to table 8, columns (2), (4) and (6) on p.430 in the paper.*
```{r "3.4 c) 1"}
#< task
#Estimate OLS regressions with dependent variable n_plots, ln_hectares, n_cows
#for the wealthier households
dd_plots_w_rd = felm(n_plots ~ treat * post_year * dens_road_inv|0|0|code_loc,
                     data = data_household, subset = (poor=="no pobre"), 
                     na.action=na.omit)

dd_lnhectares_w_rd = felm(ln_hectares ~ 
                            treat * post_year * dens_road_inv|0|0|code_loc,
                          data = data_household, subset = (poor=="no pobre"), 
                          na.action=na.omit)

dd_cows_w_rd = felm(n_cows ~ treat * post_year * dens_road_inv|0|0|code_loc,
                    data = data_household, subset = (poor=="no pobre"), 
                    na.action=na.omit)

#Show the results of all three regressions
showreg(list(dd_plots_w_rd, dd_lnhectares_w_rd, dd_cows_w_rd), 
        custom.model.names=c("Number of Plots", "ln(1+Hectares)", 
                             "Number of Cows"), 
        digits=3, stars=c(0.01,0.05,0.1),
        include.adjrs=FALSE, include.rsquared=FALSE)
#>
```
While the spillover effect on the number of plots is not significant and does not vary significantly with (inverse) road density either, this is different with the other two variables describing production:  
The coefficient estimate $\widehat{\beta_3}$ is still not significant, but the coefficient estimate $\widehat{\beta_7}$ is significant on the $1 \%$ level in both the regression *dd_lnhectares_w_rd* and *dd_cows_w_rd*. In both regressions this latter estimated coefficient is positive which means that the spillover effect is **increasing in the inverse road density**. Put differently, **the less infrastructure around a locality**, **the greater the production increase** among non-beneficiary households in response to the treatment of their poor neighbors.  
Besides, the results indicate that **for high levels of infrastructure** (inverse road density close to $0$) there is **no significant spillover effect** on production whereas **at very low levels** there is a **highly significant production effect** on the neighboring households.

How big is the **effect of road density on the spillover effect**?  
To answer this question we are going to determine the changes of the spillover effect on the total hectares and the number of cows when the inverse road density changes from its $10 \%$ quantile to its $90 \%$ quantile. This number is more informative than the coefficient $\widehat{\beta_7}$ because it takes into account the limited range of values of the inverse road density.

In the following code chunk please write a function which takes as input argument *est* the estimate $\widehat{\beta_7}$, as argument *expl_var* the values of the inverse road density and as arguments *p_l* and *p_u* the probabilities of a lower and an upper quantile. It shall return the change in the estimated spillover effect for the inverse road density changing from the lower quantile to the upper quantile. *If you are not sure how to calculate this, have a look at the 'info' box below.* The functional frame and a description of the intermediate steps are already given. Just uncomment the lines containing code and fill in the missing parts.
#< info "Effect of inverse road density on spillover effect"
The spillover effect, which I will abbreviate as $spe$, is given by
$$spe = \beta_3 + \beta_7 R_i$$
The difference in the spillover effect when $R_i$ is changing from some lower quantile $R_l$ to some upper quantile $R_u$ is then given by
$$\Delta spe = \left(\beta_3 + \beta_7 R_u \right) - \left(\beta_3 + \beta_7 R_l \right) = \beta_7 \left(R_u - R_l\right)$$
#>

```{r "3.4 c) 2"}
#< task
#effect = function(est, var_expl, p_l, p_u){
  #Use the function quantile (see Ex. 3.1) to calculate the quantiles l and u
  #for var_expl for the probabilities p_l and p_u


  #Calculate the difference delta in the spillover effects
  #for var_expl changing from l to u

  #return delta
#  return(as.numeric(delta))
#}
#>
effect = function(est, var_expl, p_l, p_u){
  #Use the function quantile (see Ex. 3.1) to calculate the quantiles l and u
  #for var_expl for the probabilities p_l and p_u
  l = quantile(var_expl, p_l)
  u = quantile(var_expl, p_u)

  #Calculate the difference delta in the spillover effects
  #for var_expl changing from l to u
  delta = est *(u - l)

  #return delta
  return(as.numeric(delta))
}

```
Now apply the function *effect* to the regression *dd_cows_w_rd* and assign the result to the variable *effect_cows*. To do so, first define the input argument *est* as the estimated coefficient of *treat:post_year:dens_road_inv*. Note that you can extract the coefficient of a variable using the command *coef(regression)["variable"]*. The code defining the argument *var_expl* as the variable describing the inverse road density (with *NA* values removed) is already given. Use the probabilities $0.1$ and $0.9$ as input arguments *p_l* and *p_u*. Finally, show the result.
```{r "3.4 c) 3"}
#< task
#Define the input argument est as described in the text
#>
est = coef(dd_cows_w_rd)["treat:post_year:dens_road_inv"]
#< task
#Define the input argument var_expl as described in the text
var_expl = na.omit(filter(data_household, poor=="no pobre")$dens_road_inv)
#Apply the function effect to est, var_expl and the probabilities 0.1 and 0.9
#>
effect_cows = effect(est, var_expl, 0.1, 0.9)
#< task
#Show the result
#>
effect_cows
```
So if the *inverse* road density increases from its $10 \%$ to its $90 \%$ quantile which is equivalent to the **road density decreasing from its $90 \%$ quantile to its $10 \%$ quantile, the spillover effect on the number of cows rises in expectation by about $0.141$ cows.** This is a bit less than $10 \%$ of the average number of cows within wealthier households before the treatment period (called *"mean dependent variable in baseline"* in the paper) which is about $1.577$ cows.

For the regression on *ln_hectares* we can use the function *effect* as well - but the interpretation of the result is different:  
The return value approximately equals the change of the spillover effect on the (real, not logarithmized) number of hectares relative to the spillover effect at the lower quantile of road density. This is approximately correct because the product of the coefficient estimate $\widehat{\beta_7}$ and difference between the $90 \%$ and the $10 \%$ quantile is smaller than $0.1$.

The following code chunk calculates the change in the spillover effect on the number of hectares. Just click on the *'check' button*.
```{r "3.4 c) 4"}
#< task
#Apply the function effect from above to the regression dd_lnhectares_w_rd
effect_hectares = effect(coef(dd_lnhectares_w_rd)["treat:post_year:dens_road_inv"], 
                     na.omit(filter(data_household, poor=="no pobre")$dens_road_inv), 
                     0.1,0.9)
#Show the result
effect_hectares
#>
```
So if **road density changes from its** $90 \%$ **quantile to its $10 \%$ quantile, the spillover effect on the number of hectares rises by about** $2.1 \%$ of the spillover effect with $R$ equal to the $10\%$ quantile.

### d) A remark on consumption
We have just found out that the spillover effect concerning the production response is heterogeneous over different levels of infrastructure quantity. But this need not result from a heterogeneous increase in consumption over different levels of infrastructure quantity. Under the assumption of a homogeneous treatment effect on consumption we did estimate a significant increase of the consumption of land-intensive goods. If we run regressions for the three variables of consumption using the heterogeneous estimation approach, we will notice that the significant estimates of the treatment effect on the dependent variables don't deviate a lot from those in the homogeneous approach and that the treatment effect on all three variables describing consumption does not change significantly with the inverse road density. Click on the *'check' button* to see the according regression results.  
*This corresponds to table 6, columns (2), (4) and (6) on p.429 in the paper.*
```{r "3.4 d) 1"}
#< task
#Estimate OLS regressions with dependent variable n_plots, ln_hectares, n_cows
#for the wealthier households
dd_rooms_rd = felm(n_rooms ~ treat * post_year * dens_road_inv|0|0|code_loc,
                   data = data_household, subset = (poor=="pobre"), 
                   na.action=na.omit)

dd_beef_rd = felm(dpw_beef ~ treat * post_year * dens_road_inv|0|0|code_loc,
                  data = data_household, subset = (poor=="pobre"), 
                  na.action=na.omit)

dd_milk_rd = felm(dpw_milk ~ treat * post_year * dens_road_inv|0|0|code_loc,
                  data = data_household, subset = (poor=="pobre"), 
                  na.action=na.omit)

#Show the results of all three regressions
showreg(list(dd_rooms_rd, dd_beef_rd, dd_milk_rd), 
        custom.model.names=c("Rooms in Home", "Days Ate Beef", 
                             "Days Drank Milk"), 
        digits=3, stars=c(0.01,0.05,0.1),
        include.adjrs=FALSE, include.rsquared=FALSE)
#>
```
As you can see, in the regressions for *dpw_beef* and *dpw_milk* the coefficient on *treat:post_year* is still significant. The according coefficient estimates in the homogeneous approach of *Exercise 3.2* were $0.114$ and $0.337$, so the coefficients estimates from the heterogeneous approach here are quite close to them. The coefficient on *treat:post_year:dens_road_inv* for the same regressions is not significant which is consistent with the hypothesis that the consumption increase is homogeneous in road density. The first regression concerning the number of rooms did not reveal a significant treatment effect on consumption in the homogeneous approach. The heterogeneous approach here does not so either - regardless of the infrastructure quantity.  
So as already revealed the **consumption** of the agricultural products beef and milk indeed **increases homogeneously among treated households with respect to infrastructure quantity** in response to treatment - or, put differently, in expectation to the same extent for all treated localities. Even with a heterogeneous estimation approach **no significant change in the number of rooms** as a proxy of timber products can be detected.
- - -
So all in all, the results from this exercise indicate that **treatment** with the program leads to a **homogeneous increase in the consumption of beef and milk** and to a **heterogeneous increase in the local production of ineligible neighboring households** but **not to a considerable increase in production among the treated households**.  
The authors conclude from these results the following functional chain: *(paper, p.432)*

![Exercise 3_4 Functional chain](functional_chain_3_4_kleiner.png)

At this point it is time to define the term **ecological footprint**. Wackernagel and Rees *(1996, pp.9-12)* explain that the ecological footprint of a subject measures the land area needed to support this subject's lifestyle in the long run: the area needed to supply his/her consumption of food, drinks, energy etc. and the area needed to dispose his/her waste.  
So, if you wondered why production is not mentioned in the flow chart: It is hidden behind the box "Increased ecological footprint of land use".  
*If you are interested in estimating your total ecological footprint, have a look at the following websites: <http://www.fussabdruck.de/>, <http://www.mein-fussabdruck.at/> (both sites are in German) or <http://www.footprintnetwork.org/en/index.php/GFN/page/calculators/> (different languages)*

## Exercise 3.5 -- The role of infrastructure - Deforestation
At the end of the last exercise we have stated a functional chain linking treatment and deforestation. This approach, which states a demand-driven increase in deforestation, seems to be plausible in so far as we could find an increase in the consumption of land-intensive goods (see *Exercises 3.3, 3.4*) and as the size of this increase supports the hypothesis that consumption is a major mechanism linking treatment and deforestation (see plausibility check in *Exercise 3.3*). However, as the corresponding local production increase is heterogeneous (see *Exercise 3.4*), we have to check furthermore whether we can also find **heterogeneity with respect to road density in the treatment effect on local deforestation**. If this was not the case, we would have to give up the current explanation.

Alix-Garcia et al. for the same reason test for heterogeneity in the deforestation effect with respect to road density and use two approaches for this purpose. *(Cf. paper, section V.A, table 9)* In the following we are going to reproduce the simpler one as it is very intuitive and the results are more easily interpretable: We will use the most elaborate RD Design approach and run the according regression for the percentage of area deforested three times - once for the subset of data containing the observations with the **lowest**, once for a subset with the **highest road densities** and once for a subset with road densities **in between**. The basic data set is again *"data_deforestation.rds"* here because we are interested in deforestation. *(Cf. paper, section V.A, table 9 columns (1)-(6))*  
Please note that the results from the alternative way to test for heterogeneity here *(paper, table 9, columns (7) and (8))* are in parts not as convincing as the results from the approach regarded here but still consistent with them.

### a) Data preparation - defining the subsets
To do so, we first have to define the three subsets of the data set for which we will run the regressions later. We will do this in the following way:  
* Sorting the data set by the size of road density in ascending order.  
* Adding the column *group* to the data set which contains a $0$ for the first third of the observations (lowest road densities), a $1$ for the second third (medium road densities) and a $2$ for the last third of observations (highest road densities).

It is your turn now to implement the first step. In the data set *"data_deforestation.rds"*, which is loaded first, the variable describing road density is called *dens_road* and denotes the length (in km) of roads in a radius of $10$ km around the locality. *(Cf. paper, p.424)* Sorting in ascending order can be realized by the command *arrange* from the package *'dplyr'*. You can find information on it in the *'info' box*. In the following code chunk, please sort the data set *data_deforest* by *dens_road* and save the resulting data set in *data_deforest_sort*. The head of both data sets is shown such that you can compare them.
#< info "Data manipulation with 'dplyr' - arrange"
The command *arrange(data, column)* sorts the data set *data* by the column given as argument. The default order of sorting is ascending. If one indicates *desc(column)* instead of *column*, the descending order is chosen for sorting.
#>

```{r "3.5 a) 1"}
#< task
#Read in the data and save it as data_deforest
data_deforest=readRDS("data_deforestation.rds")
#Load package 'dplyr'
library(dplyr)
#Sort data_deforest by dens_road in ascending order
#and save it in data_deforest_sort

#>
data_deforest_sort = arrange(data_deforest, dens_road)
#< task
#Show the head of each data frame
head(data_deforest)
head(data_deforest_sort)
#>
```
The implementation of the second part is described step by step in the next code chunk. A part of the code is already given such that you only have to complete and then uncomment it. Some missing commands are very basic and have not been introduced in this problem set so far. If you don't know them, just have a look at the *'info' box* below. The nested for-loops are a bit tricky to complete. The *'hint' button* might help you. If you are not interested in this kind of programming, you can also just use the *'solution' button* here. Finally, I recommend to you having a look at the data set in the *'Data Explorer'* because the command *head* is not sufficient here to show the important feature of the created data set.

#< info "Some helpful commands"
Sometimes it is helpful to find out how many observations there are in a data frame or vector or how many variables are considered in a data frame. The following commands are very basic:
* *nrow(data)* gives you the number of rows of *data* which can be a data frame or an array  
* *ncol(data)* is the analogous command for the number of columns  
* *length(x)* gives you the number of elements of a vector (among others)

Sometimes we need an "empty" vector or data frame which we are going to fill afterwards step by step. In R any vector or data frame has to contain something when it is defined. One possibility to do so is the command *rep(x,n)* which creates a vector of *n* elements, each of them equal to *x*.
#>
```{r "3.5 a) 2"}
#< task
#Extract the number of rows in data_deforest_sort and save it in the variable N
#>
N = nrow(data_deforest_sort)
#< task
#Use the command rep(x, times) to create the variable group 
#consisting of N zeros
#>
group = rep(0, N)
#< task
#The following two nested for loops run through all observations
#whereby the index i denotes the group of road density
#Goal:
# group[r] indicates the group of road density (0,1,2) the r'th row of 
# data_deforest_sort belongs to
#for (i in 0:2){
#  for (j in 1:(N/3)){
#    group[...] = ...
#  }
#}
#>
for (i in 0:2){
  for (j in 1:(N/3)){
    group[i*(N/3) +j] = i
  }
}
#< hint
display("Concerning the left-hand side of the command: group can take on three values: 0, 1 and 2 - which are exactly the values i can take on. Concerning the general index of group in the command: For which of the indices group is equal to 0, 1 or 2? Remember that group[r] indicates to which group of road density (low, medium, high) the row r in the SORTED data frame data_deforest_sort belongs.")
#>
#< task
#Use the command mutate to add the variable group as 
#new column group to data_deforest_sort
#>
data_deforest_sort = mutate(data_deforest_sort, group=group)
```
Now, we have all the ingredients to run the three regressions on the subsets of *data_deforest* - one for each value of the column *group*.

### b) Running the regressions
As already mentioned, we will use the **most elaborate regression approach** from *Exercise 2.7* for this purpose: An **IV Tobit estimation of the fuzzy RD design** with the **excluded instruments** $E_i, E_i \times I_i, M_i, M_i \times I_i$. More precisely, we will use the **fourth-order polynomial specification** which means that in the set of covariates the marginality index is included up to the power of four. Apart from that, the same covariates we used in all regressions of the RD Design shall be used here as well.  
The next code chunk uses again the function *Newey_SB_ivtobit* to estimate the coefficients and the marginal effects of treatment. Click on the *'check' button* to run the code and to see the estimates of the coefficients. This might take some seconds.  
*This corresponds to table 9, columns (1), (3) and (5) on p.431 in the paper.*
```{r "3.5 b) 1", results="asis"}
#< task
#Load packages 'AER', 'car'
library(AER)
library(car)
#Define the string of exogenous explanatory variables
X1str = c("pov_ind_95", "pov_ind_2", "pov_ind_3", "pov_ind_4", "forest_prior",
          "ln_area_polyg", "deg_slope", "ln_pop",
          "ecoreg_control_1", "ecoreg_control_2", "ecoreg_control_3")
#Define the string of excluded instruments
X2str = c("eligible", "eligible_povind95", "marginal", "marginal_povind95")
#Estimate fourth-order polynomial specification for low road densities
elab_low = 
  Newey_SB_ivtobit(data = filter(data_deforest_sort, group==0),
                      ystr = "pct_deforested", Ystr="treat",
                      X1str = X1str, X2str = X2str)
#Estimate fourth-order polynomial specification for medium road densities
elab_medium = 
  Newey_SB_ivtobit(data = filter(data_deforest_sort, group==1),
                      ystr = "pct_deforested", Ystr="treat",
                      X1str = X1str, X2str = X2str)
#Estimate fourth-order polynomial specification for high road densities
elab_high = 
  Newey_SB_ivtobit(data = filter(data_deforest_sort, group==2),
                      ystr = "pct_deforested", Ystr="treat",
                      X1str = X1str, X2str = X2str)

#Load package 'regtools'
library(regtools)
#Show the regression results
results.df = convert(list("low" = elab_low[[1]],
                          "medium" = elab_medium[[1]],
                          "high" = elab_high[[1]]))
stargazer(results.df, type="html", summary=FALSE)
#>
```
As we are only interested in whether the treatment effect differs for different levels of road density and not in the coefficients of the covariates, please focus on the first row of the presented results. Actually, the results are as expected given our theoretical, intuitive thoughts and the results from the earlier exercises: A **significant treatment coefficient** can only be measured for those localities with the **lowest road densities**.
#< info "Remark on the results"
Again, the coefficient estimates are not identical with those indicated in the paper. *(Cf. paper, table 9)* However, they are again consistent with those obtained by the STATA command *ivtobit* run with the method *twostep*.
#>
Remember that the treatment coefficient from above is not equal to the marginal effect of treatment on the dependent variable - marginal effects have to be calculated separately. Again we are interested in the marginal effect of treatment on the **probability of deforestation** as well as on the **expected percentage of area deforested given positive deforestation**. The according estimates are already contained in the list returned by *Newey_SB_ivtobit* and the next code chunk shows them. Just click on the *'check' button*.  
*This corresponds to table 9, columns (1), (3) and (5) on p.431 in the paper.*
```{r "3.5 b) 2", results="asis"}
#< task
#Load package 'stargazer'
library(stargazer)
#Show the estimated marginal effects of treat
margEff.df = convert(list("low" = elab_low[[2]],
                          "medium" = elab_medium[[2]],
                          "high" = elab_high[[2]]))
stargazer(margEff.df, type="html", summary=FALSE)
#>
```
According to the estimates, in isolated localities treatment increases the probability of deforestation by almost $8$ percentage points and the percentage of area deforested by about $0.25$ percentage points among deforesters while in localities with medium or high road density no significant marginal effect of treatment on deforestation can be measured. Recall from *Exercise 2* that the marginal effect estimates from the several specifications of the regressions on the whole data set *data_deforest* lie between $1.8$ and $3.7$ percentage points for the probability and between $0.09$ and $0.18$ percentage points for the size of deforestation. So with respect to both variables the **isolated localities** show a **much higher treatment effect on deforestation than the average locality**.
#< info "Remark on the results"
First, please note that the marginal effects also deviate a bit from those in the paper but are still explainable by the different calculation procedure based on a different input. *(Cf. paper, table 9)*   
Second, please note that in our calculations, both marginal effects of treatment among the isolated localities are only significant on the $10 \%$ level. This is generally a very weak level of significance. In the paper, higher significance levels are denoted. Actually, with the Stata code given by the authors, I cannot replicate the $5 \%$ significance level for the effect on $E[y|y>0]$. 
#>
As robustness test, the authors don't only use IV Tobit in order to estimate the elaborate regression for all three groups of road density, but also IV OLS. The regressions lead to the same results concerning the significance of the treatment effect which supports the results obtained through the IV Tobit estimation reproduced in this exercise. *(Cf. paper, section V.A; table 9, columns (2), (4), (6))* 

### c) Excursus: Baseline deforestation among isolated localities
It might be interesting to not only differentiate the marginal effects of treatment but also the **baseline probability and amount of deforestation** for different levels of road density. Remember that in *Exercise 2.7* we used the mean of the variable *d* among the noneligible localities (*eligible=0*) as baseline probability of deforestation and the mean of the variable *pct_deforested* among noneligible localities with positive deforestation (*d=1*) as baseline amount of deforestation given there is deforestation. In the next code chunk, please calculate both baselines for the low road density group. A template of the code is already given such that you only have to fill in the missing part and uncomment the code.

```{r 3.5 c) 1}
#< task
#Please calculate the baseline probability of deforestation
#for the low road density group
#mean(subset(data_deforest_sort, ...)$d)
#>
mean(subset(data_deforest_sort, (group==0 & eligible==0))$d)
#< task
#Please calculate the baseline amount of deforestation given there is
#deforestation for the low road density group
#mean(subset(data_deforest_sort, ...)$pct_deforested)
#>
mean(subset(data_deforest_sort, (group==0 & eligible==0&d==1))$pct_deforested)
```
As you can see, the baseline probability of deforestation among noneligible, *isolated* localities is a bit higher but still close to the one among all noneligible localities ($0.048$). The baseline amount of deforestation given there is deforestation, however, in the *isolated* localities is only about one half of the baseline in the whole set of noneligible localities ($0.597 \%$). So one could say that in response to treatment the deforestation level of very isolated localities approaches the one of better connected localities.

- - -
All in all, we can state that not only the **spillover effect on production** (and thus the whole effect of the program on the local production), but also the **treatment effect on deforestation** are **heterogeneous** with respect to infrastructure quantity.  
This is consistent with the idea of consumption (and the corresponding production) being the major link between treatment and the increase in deforestation. We can therefore speak of a **"market-mediated shock or impact"**.
- - -
However, we are still not at the end of our journey. We have found out that the increase in consumption has to be met by other households than the treated ones - but the extent to which we can observe a spillover effect on households in treated localities varies with road density. So two questions arise:  
* What is the true, total impact of the program on deforestation?  
* Where does the ecological footprint of the program finally land?

To be able to - at least approximately - answer these questions, we will have a closer look at the theoretical background of the estimation of market-mediated shocks which has so far only been introduced in an intuitive way and which is - as you will notice - supported by the results we have obtained so far.

## Exercise [Q6] -- QUIZ on Exercises 3.4 and 3.5
You have passed through the greatest part of *Exercise 3* now. The last two exercises used econometric methods and R commands you had already got to know earlier. The focus was clearly on the economic  chain here. In the following quiz you can check whether you have really understood the different steps. Good luck!
- - -
#< quiz "Quiz Number 6"
parts:
  - question: 1. We have used two regression approaches for estimating the treatment (and spillover) effect on production. The first one assumes a homogeneous effect with respect to road density, the second one allows for heterogeneity. The desired result has been... Please complete.
    choices:
        - to get similar results for both approaches meaning a significant homogeneous effect or none at all.
        - to estimate a significant effect at least for the more isolated localities*
    multiple: FALSE
    success: Great! This is correct! For the increase in consumption being the main reason for the treatment effect on deforestation, there must be some increase in the according production of the same market - wherever this might exactly be.
    failure: Try again. It might also help you to have a look at the introductory part of *Exercise 3.4* again.
  - question: 2. Imagine you want to estimate "y = a + bx + cz + d(x z)" by OLS. The shortest way to write the formula input for *lm* is *y ~ x ... z*. What has to be written in "..."?
    choices:
        - a double dot
        - a star*
    multiple: FALSE
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 3.4 a)* again.
  - question: 3. The treatment effect on deforestation should be homogeneous with respect to road density in order to be consistent with the theory of a consumption-driven mechanism linking treatment and deforestation.
    choices:
        - Wrong because the local increase in production is heterogeneous.*
        - Correct because the treatment effect on consumption is homogeneous.
    multiple: FALSE
    success: Great! This is correct! The location where deforestation changes as a result of the program does not have to be equal to the location where consumption is increased but where the according production is increased.
    failure: Try again. It might also help you to have a look at the introductory part of *Exercise 3.5* again.
  - question: 4. Which command from the package 'dplyr' sorts a data frame by one column? Please don't write down the arguments or brackets.
    answer: arrange
    success: Great! This is correct!
    failure: Try again. It might also help you to have a look at *Exercise 3.5 a)* again.
#>
#< award "Good thinking!"
As you have demonstrated, you are familiar with the economic story
in this exercise!  
You have understood what we need to check in order
to find out the microeconomic explanation for the treatment effect on
deforestation and you have understood - in an intuitive way - the role
infrastructure plays here.  
The theoretical input in the next exercise will complement this.
#>

## Exercise 3.6 -- The role of infrastructure - Theoretical background
The idea of studying not only the production effect among treated households but also among their neighbors in the same locality as well as the idea of allowing for heterogeneity with respect to infrastructure both in the production and the deforestation effect are quite intuitive. Nevertheless, Alix-Garcia et al. develop a **theoretical framework** describing the background of this idea and predicting the according results. It is dedicated to the **effect of market-mediated shocks** in general and helps to draw conclusions from the findings in this paper:  

### a) Theoretical framework
*This part of the exercise is based on sections II.C, V.B, V.C and VI in the paper.*  
A common feature of market-mediated influences is the fact that the location of a change in consumption and the location of the corresponding change in production don't have to coincide. This is because as products are traded within markets, an increase in demand can be sourced in theory by every subject in this market - the **effect can spill over**, at least in parts.  
A very important determinant of the market size - and thus of the extent to which the treatment effect can spill over - is **infrastructure:** The better a treated locality is connected to surrounding localities, the more subjects source its local increase in consumption and thus the smaller the measured production increase in the originally treated locality is. Theoretically this could even lead to the fact that no effect can be measured in the originally treated locality. This means that after being mediated by the market the **treatment effect appears heterogeneous** across the treated localities with respect to infrastructure even though the underlying true treatment effect is homogeneous. The logic depicted here is not new but drawn in analogy to studies on the estimation of how rainfall shocks influence prices.

To **quantify** the measured effect in a local experiment like in their paper, the authors set up the following **model:**
- - -
* The market consists of $\eta$ members
* $\sigma \cdot \eta$ members in this market are treated
* Per member treated demand increases by one unit
* As response to its own treatment, the outcome $y$ of a treated member increases by $\frac{\tau}{\eta}$ (direct effect of treatment)
* Additionally, the outcome $y$ of every unit (if treated or not) increases by $\frac{\tau}{\eta}$ per treated member in the same market,  
so in total by $\frac{\tau}{\eta} \cdot ( \sigma \eta ) = \sigma \tau$ (expected spillover effect)

In this model, the **average treatment effect** is just defined as the difference in the expected outcome changes between treated and untreated members. The average treated member shows an expected increase in outcome of $E[y|treated] = \frac{\tau}{\eta} + \sigma \tau$, while the average untreated member shows an expected increase in outcome of $E[y|untreated] = \sigma \tau$. So finally the average treatment effect results in
$$E[y|treated] - E[y|untreated] = \frac{\tau}{\eta}$$
- - -
Given that the true total treatment effect is $\tau$, according to the model the **average treatment effect underestimates the effect**. What is actually measured here, is only the part of the increased demand which is sourced by the treated member directly. So it ignores the part of the increased demand sourced by the other members in the market.  
As the direct effect of treatment - so the measured average treatment effect - is **antiproportional to the number of members** $\eta$ in the market the treatment effect appears smaller the bigger the market is.  

In the following code chunk, the functional relation between the measured average treatment effect $\frac{\tau}{\eta}$ and the market size $\eta$ is visualized for three different sizes of the true total treatment effect $\tau$: $0.1$, $0.9$ and $10.0$. Just click on the *'check' button* to see the according graphics.
```{r "3.6 1", fig.width=10}
#< task
#Load package 'dplyr'
library(dplyr)
#Creating a data set with values for eta, tau and the avg. treatment effect
data_model = data.frame(eta = rep(seq(1:100), times=3), 
                        tau = rep(c(0.1, 0.9, 10),each=100)) %>%
             mutate(ate=tau/eta) %>%
             mutate(tau = as.character(tau))
#Show the first six rows of data_model
head(data_model)

#Load package 'ggplot2'
library(ggplot2)
#Creating a plot showing the avg. treatment effect over eta for each tau
p = ggplot() + 
  geom_point(data=data_model, 
             aes(x=eta, y=ate, group = tau, color=tau)) + 
  xlab("market size [numbers of members]") + 
  ylab("avg. treatment effect")
#Creating the same plot as log-log-plot
p_ln = ggplot() + 
  geom_point(data=data_model, 
             aes(x=log(eta), y=log(ate), group = tau, color=tau)) + 
  xlab("log(market size [numbers of members])") + 
  ylab("log(avg. treatment effect)")


#Load package 'gridExtra'
library(gridExtra)
#Showing both plots
grid.arrange(p,p_ln, nrow=1, ncol=2)
#>
```
On the left side you can see how quickly the measured average treatment effect is declining for rising market size - more market members $\eta$. For $\eta = 1$, which means that the market only consists of one single member, the measured average treatment effect just equals the true total treatment effect $\tau$. You can see this from the definition above as well: For a market of one member there is no spillover effect and the whole increase in demand is sourced by the treated member alone. For two members the effect is only a half of the effect measured for one member/the true total treatment effect. On the right-hand side you can see a log-log plot of the same function. This shows linear relations for all three parameters $\tau$ which only vary in their starting point for $\eta=1$ $\left(log\left(\eta\right)=0\right)$.

Now what about the **role of infrastructure** in this model?  
The more infrastructure, the more local markets are connected to each other. So actually, infrastructure is a great determinant of the number of market members $\eta$ and this way influences as well the measured average treatment effect. More precisely, very good infrastructure leads to $\eta$ being very large such that the average treatment effect vanishes locally. If in contrast infrastructure is very bad meaning that the locality is isolated or autonomous, $\eta$ gets very small and the measured average treatment effect approaches the true effect $\tau$. So the model indeed shows quantitatively what was explained qualitatively in the beginning of the theoretical framework.

- - -
With this theoretical framework quantified by the model above in mind how would you answer the following questions:
#< quiz "ModelCheck"
parts: 
  - question: 1. According to the model above, is there a chance of approximately measuring the true total treatment effect?
    choices:
      - yes* 
      - no 
    multiple: FALSE
    success: Great! This is correct! 
    failure: Try again. 
  - question: If you see a chance of measuring the effect approximately correctly, for which level of infrastructure does the model predict the measured average treatment effect to be closest to the true total treatment effect?
    choices:
      - I do not see any chance of measuring the effect approximately correctly.
      - low*
      - medium
      - high
    multiple: FALSE
    success: Great! This is correct!
    failure: Try again.
#>
- - -

### b) Theoretical predictions - and our results
Returning now to the **subject of this problem set** - the effect of treatment with the poverty alleviation program Oportunidades - which points in the analysis up to now are consistent with this theoretical model and which further conclusion can we draw from the model?  
In the following, please indicate whether you think that our results are consistent or not with the following predictions. As this shall invite you to rethink the meaning of the results we have obtained but not be a test as such, you will be given the sample solution after having clicked on the *'check' button* - no matter if your answer is correct or not.

#< quiz "Prediction 1"
question: 1. Prediction. The location of the consumption change in response to treatment and the location of the according production change don't have to coincide. Are the results consistent with the prediction?
sc: 
  - yes* 
  - no 
success: Correct! In fact, we cannot find a considerable production change among treated households and the untreated households in the same locality only show a change in production if their infrastructure is really bad. This is consistent with the prediction. 
failure: Unfortunately, I don't agree with you here. In fact, we cannot find a considerable production change among treated households and the untreated households in the same locality only show a change in production if their infrastructure is really bad. This is consistent with the prediction. 
#>
<br>
#< quiz "Prediction 2"
question: 2. Prediction. The treatment effect on production and thus on deforestation can be heterogeneous with respect to road density even though the consumption increase is homogeneous. Are the results consistent with the prediction?
sc:
  - yes*
  - no
success: Correct! As we have seen in the last exercises, the estimated consumption effect does not vary with the quantity of infrastructure. However, we have also seen that the spillover effect on production decreases significantly in road density such that it is only significant for lower levels of road density. This last aspect holds as well for the deforestation as we have seen in *Exercise 3.5*. So these observations are consistent with the model as well.
failure: Unfortunately, I don't agree with you here. As we have seen in the last exercises, the estimated consumption effect does not vary with the quantity of infrastructure. However, we have also seen that the spillover effect on production decreases significantly in road density such that it is only significant for lower levels of road density. This last aspect holds as well for the deforestation as we have seen in Exercise 3.5. So these observations are consistent with the model as well.
#>
<br>
#< quiz "Prediction 3"
question: 3. Prediction. Even though the local experiment we study here is randomized, we do not measure the whole treatment effect but only the part which does not spill over. This error goes to zero for completely isolated localities. Are the results consistent with the prediction?
sc:
  - yes*
  - no
success: Correct! Actually, in our studies the largest (and often the only significant) treatment effects on production and deforestation can be observed if infrastructure is poorest which is in line with the statement of the model. According to the model we would therefore expect that this (highest) effect comes closest to the true, total effect even though we might still underestimate it.
failure: Unfortunately, I don't agree with you here. Actually, in our studies the largest (and often the only significant) treatment effects on production and deforestation can be observed if infrastructure is poorest which is in line with the statement of the model. According to the model we would therefore expect that this (highest) effect comes closest to the true, total effect even though we might still underestimate it.
#>
<br><br>

### c) Spatial contour of the effect
After all, one question is still left: Where is the area which belongs to the increase of the ecological footprint we have estimated actually located?

Holding in mind the model from this exercise and the results from the analyzes up to now we could imagine the following: It is not located (at least not for a considerable part) among the treated households themselves. The worse the infrastructure, the greater the part of the area which is located at least in the same locality - but among the untreated households. Thus we can think of the rest of the area being located in the environment of the treated localities. The results until now are consistent with this theory - we could see them as a kind of indirect support of it. The authors' **analysis of the spatial contour** additionally gives some **direct support** of the "hypothesis that production is sourced from surrounding markets" *(paper, p.431)*.  
Unfortunately, I am not able to reproduce the results of this study with the data and code provided by the authors. This is why the following is based on the study descriptions, results and interpretations given in the paper.

For their **spatial analysis** Alix-Garcia et al. divide the whole country into a grid with fields of $10$ times $10$ km and calculate the treatment saturation for each ring of width $10$ km around the locality. *For the definition of treatment saturation and a visualization of the rings, you can have a look at the 'info' box.*
The question of interest is **whether and how the treatment saturations** in the different rings around a locality **influence the probability of deforestation** in the locality itself. *(Cf. paper, section V.B)* 
#< info "Treatment saturation in rings"
The subset of localities which are used for this study have a marginality index between $-2$ and $-0.2$ (restricted sample). This way, the treatment intensities are quasi-random even though treatment might be endogenous regarding the entire sample. This is a similar idea to the one in the regression discontinuity design.  
The treatment saturation is calculated as the ratio of treated localities among all studied localities. For each locality this ratio is calculated for the grid field of the locality itself as well as for each surrounding ring every $10$ km until a distance of $40$ to $50$ km.
The graphics below shows for one locality (figured as circle) the rings for which the saturations are calculated. The numbers in the graphics denote the distance between the line just above it and the locality.  
*(Cf. paper, section V.B)*

![Spatial grid](Exercise_3_7_spatial_grid_small.png)
#>
The estimation strategy used by the authors as well as the detailed regression results and their analysis shall not be in the focus of this sub-exercise. However, if you are interested, I have provided all this to you in the next *'info' box*.
#< info "Estimation strategy and results"
The authors use a Linear Probability Model which they estimate by OLS. The according regression equation is the following *(paper, p.431)*:
$$d_i = \alpha + \left(\sum_{k=0,10,20,30,40}{\beta_k s_{ik}  + \theta_k s_{ik} c_i}\right) + \Gamma \vec{X_i} + \epsilon_i$$
Here, $d_i$ is a dummy variable indicating positive deforestation in grid field $i$, $s_{ik}$ the saturations defined above, $c_i$ stands for the road density within $50$ km around the studied field $i$ and $\vec{X_i}$ for some covariates.  
*(Cf. paper, section V.B)*

The following table is a replicate of table 10 in the paper.

<style>
th, td {
    padding: 10px;
}
</style>
<table style="width:100%">
  <tr>
    <th>Variables</th>
    <th>homogeneous regression</th>
    <th>heterogeneous regression c</th>
    <th>heterogeneous regression dm</th>
  </tr>
  <tr>
    <td>own saturation</td>
    <td>0.017 (0.018)</td>
    <td>0.192 (0.044)***</td>
    <td>-0.006 (0.019)</td>
  </tr>
  <tr>
    <td>sat. ring 10-20</td>
    <td>0.025 (0.019)</td>
    <td>0.120 (0.047)**</td>
    <td>-0.027 (0.033)</td>
  </tr>
  <tr>
    <td>sat. ring 20-30</td>
    <td>0.038 (0.023)*</td>
    <td>-0.070 (0.067)</td>
    <td>0.067 (0.056)</td>
  </tr>
  <tr>
    <td>sat. ring 30-40</td>
    <td>-0.006 (0.028)</td>
    <td>-0.154 (0.079)*</td>
    <td>0.066 (0.076)</td>
  </tr>
  <tr>
    <td>sat. ring 40-50</td>
    <td>0.012 (0.031)</td>
    <td>-0.014 (0.080)</td>
    <td>-0.068 (0.075)</td>
  </tr>
  <tr>
    <td>ln(road density)</td>
    <td>-0.027 (0.016)*</td>
    <td>-0.081 (0.039)**</td>
    <td></td>
  </tr>
  <tr>
    <td>baseline forest</td>
    <td>0.001 (0.0002)***</td>
    <td>0.001 (0.0002)***</td>
    <td>0.001 (0.0002)***</td>
  </tr>
  <tr>
    <td>c x sat. own</td>
    <td></td>
    <td>-0.090 (0.020)***</td>
    <td></td>
  </tr>
  <tr>
    <td>c x sat. 10-20</td>
    <td></td>
    <td>-0.063 (0.029)**</td>
    <td></td>
  </tr>
  <tr>
    <td>c x sat. 20-30</td>
    <td></td>
    <td>0.068 (0.045)</td>
    <td></td>
  </tr>
  <tr>
    <td>c x sat. 30-40</td>
    <td></td>
    <td>0.099 (0.051)*</td>
    <td></td>
  </tr>
  <tr>
    <td>c x sat. 40-50</td>
    <td></td>
    <td>0.013 (0.054)</td>
    <td></td>
  </tr>
  <tr>
    <td>dm</td>
    <td></td>
    <td></td>
    <td>-0.061 (0.044)</td>
  </tr>
  <tr>
    <td>dm x sat. own</td>
    <td></td>
    <td></td>
    <td>0.058 (0.020)***</td>
  </tr>
  <tr>
    <td>dm x sat. 10-20</td>
    <td></td>
    <td></td>
    <td>0.069 (0.032)**</td>
  </tr>
  <tr>
    <td>dm x sat. 20-30</td>
    <td></td>
    <td></td>
    <td>-0.043 (0.058)</td>
  </tr>
  <tr>
    <td>dm x sat. 30-40</td>
    <td></td>
    <td></td>
    <td>-0.093 (0.079)</td>
  </tr>
  <tr>
    <td>dm x sat. 40-50</td>
    <td></td>
    <td></td>
    <td>0.095 (0.078)</td>
  </tr>
</table>
<br>
The second regression is based on the equation given above. It allows for heterogeneity in the effects of the changes in the different saturations with respect to road density because it includes the products $s_{ik} \cdot c_i$. The first column in contrast does not include those products and thus assumes homogeneous effects. The third column uses the dummy variable *dm* which indicates whether the road density is smaller than the median of road density in the sample. *(Cf. paper, section V.C)*  
As you can see from the results, the homogeneous approach in column $(1)$ does not show convincing significant effects of treatment saturations on the probability of deforestation.  
The more differentiated second and third column however show significant effects:  
In column $(2)$ the coefficients on $s_{i0}$ and $s_{i10}$ are significant and positive while the coefficients on $s_{i0} \cdot c_i$ and $s_{i10} \cdot c_i$ are significant and negative.  
This is consistent with the results in column $(3)$: The coefficients on $s_{ik} \cdot dm_i$ are significant for $k=0$ and $k=10$.
#>
According to the results from this study, the treatment intensity in the own grid cell as well as the treatment intensity in the next ring (in a distance from $10$ to $20$ km) increase the probability of deforestation significantly. These effects depend strongly on road density, though: The better the infrastructure, the smaller the effects or, when comparing localities with a road density smaller than the median road density in the sample with the rest, such a significant effect appears only for the low road density group.  
At the same time these results indicate that **treatment of some locality influences deforestation in an area up to about $20$ km away.** Taking into account the different estimates depending on road density, we can state that the spatial contour of the *observed* effect is thus about $20$ times $20$ kilometers large if road density is very bad and very flat if road density is better.  
*(Cf. paper, section V.C and table 10)*
- - -
The study thus suggests the following points:
* The **probability of deforestation does not only depend on treatment in the own locality**, but is influenced significantly by the treatment intensity of its environment within a distance of several kilometers.  
* This also means that **treatment** of some locality does **not only affect deforestation in the same locality** but also in an environment up to several kilometers away. The land area from the increase in the ecological footprint due to the program should therefore be located (at least) in this environment.  
* The **relevant spatial range** from which a place can feel treatment changes and thus the consequences of the associated increase of the ecological footprint seems to be influenced by **infrastructure quantity**.

- - -

Concerning the model described in part *a)* of this exercise  and its implied predictions in part *b)* the results are consistent with the model and its predictions and furthermore support the first prediction even more directly and obviously than the results obtained earlier in this problem set.

## Exercise 4 -- Summary and Conclusion

### Congratulations!
You have mastered a long and intense journey through the problem set "Ecological Footprint of Poverty Alleviation" passing economic hurdles, looking for R treasures and of course running through an interesting economic story. Now just lie back and have a look at what you have learned and which conclusions we can draw from this.


### a) Summary of the results and insights
The first big exercise (*Exercise 2*) was dedicated to the question whether Oportunidades influenced deforestation - and if so, how. We have obtained the following result:  
* **Significant and positive effect of treatment with the program on local deforestation**:  
The regression results from various approaches indicate that treatment of a locality with Oportunidades is associated with an *increase* of both the *probability* of local deforestation and of the *percentage of area deforested*.

<br>
In the second big exercise (*Exercise 3*) we took a closer look at this result to find out which *mechanism* could be a plausible link between treatment and deforestation and to find out about the *role of infrastructure* in this context. Here, we have measured the following effects:  
* **Significant increase of the consumption of land-intensive agricultural products** as a response to the program:  
The regression results indicate that treatment of a household leads to an increased frequency of the consumption of beef and milk in this household. An increase of this type of consumption implies - ceteris paribus - an *increase* of the **ecological footprint** of those households, so of the land area needed to sustainably support the lifestyle of those households. A plausibility check has shown that this effect should be large enough to explain the result from *Exercise 2*.  
* **Significant increase of the corresponding agricultural production** among wealthier neighboring households:  
We have measured a significant and positive effect on the total hectares and on the number of cows among the wealthier, ineligible households as a response to the treatment of their neighbors.  
* The results indicate a **homogenous increase of consumption** in the road density and a **heterogeneous increase of production and deforestation**.

<br>
All in all, the results are consistent with the following **economic story:**

- - -
Treatment with the program increases the consumption of land-intensive agricultural products. This effect is mediated through a market of which the size is determined (at least in parts) by road density. The increase in consumption is thus met by an increase in production which involves by need deforestation and which is spread over the market. The better the infrastructure, the more localities can source the local consumption increase and the smaller the production (and thus deforestation) increase in the locality originally treated. Eventually, the local effect becomes immeasurably small even though it is still there - but spread all over the market.

- - -

The authors conclude from this that the deforestation effect measured in their paper - and thus in the problem set - might underestimate the effect and that the effect measured among the most isolated localities might be the most reliable even though it is the highest. *(paper, p.434)*


### b) Implications from the results
First of all, one should be careful in generalizing the results because of two aspects:  
* Oportunidades is a *conditional cash transfer program*. So the results should not be generalized to all poverty alleviation programs providing financial support but rather be restricted to CCT programs.  
* Using the RD Design for the analysis of the deforestation effect means that we have only measured a *local* average treatment effect. So the results apply only to very poor people.

*(Cf. paper, p.433)*

You might be disappointed about the results from the paper indicating that CCT programs like Oportunidades might lead to environmental harm. On the one hand, Alix-Garcia et al. recommend to be aware of this problem when running such a program and to use according measures to prevent poverty alleviation to go along with a degradation of the environment. On the other hand, the authors qualify their results: The paper only takes a look at a very small aspect of the program's impacts and does not consider total welfare or a long-term development of the clearing behavior. *(Cf. paper, pp.433-434)*  
As the results indicate that the treated households themselves do not supply great parts of their increase in consumption, I think it would not be sufficient to apply ecological measures to the treated households alone but rather to everybody who could be affected.  
In general, I doubt that deforestation due to an increase in the consumption of land-intensive agricultural products will increase beyond measure. At the same time I think there is a minimum ecological footprint associated with a healthy and worthy living lifestyle. This way it might rather be industrial nations whose consumption is often far beyond any lower limit of a worthy living lifestyle who should decrease their consumption and prevent further deforestation.

### c) Outlook
To extend the studies in the paper, it would be interesting to get information on where the treated households bought their food - especially beef and milk. With the help of such data one could collect further information on the validity of the adaption of the model from rainfall shocks to the context of the paper.

For future researches, it might be a good complement to the paper to study long-term effects of the program on deforestation. Such studies could help to overcome the issue of not studying the total welfare. It would be interesting to see whether consumption increases further or reaches some saturation level. Further, one could study whether deforestation in the long term increases in the same way as consumption or if people develop more efficient agricultural techniques which reduces the need of land area as already depicted in the World Development Report from 1992 *(Cf. World Bank (1992))*.  
Even though it might be difficult to conduct such studies based on data from the past, like on Oportunidades or on Progresa, one could also use data from the present as the program (named Prospera) still exists today. The existence of the program - albeit with adapted rules - also shows that even about 20 years after the initiation of the program it is still a current issue.

### d) ... Finally, don't forget ...
 ... that on your economic journey you have passed lots of econometric issues - from the classical linear regression model and the linear probability model over the Tobit and IV Tobit model to different estimation methods, causality problems, the interpretation and comparison of results and many details linked to those issues. I hope you can carry with you some new aspects or have become aware again of important details in issues already known to you.  
I won't list all the R commands you have encountered on your journey because I think if you have made use of the interactive character of this problem set, you will remember them roughly. In R, this is usually already enough because the R help and the internet with various forums will help you with the detailed syntax. The key is to know for which applications there are useful commands - and I hope you could discover some of them in this problem set.

### Thank you...
very much for going on this interactive economic, econometric and programming journey! And if you are interested in finding out more details, you can find the list of the sources I used in this problem set in the section *Bib*.


## Exercise Bib -- Bibliography
## Books and Papers:
* Alix-Garcia, J. & C. McIntosh & K. R. E. Sims & J.R. Welch (2013): The Ecological Footprint of Poverty Alleviation: Evidence from Mexico's Oportunidades Program. *The Review of Economics and Statistics, 95*(2), pp. 417-435.

* Angelsen, A. & D. Kaimowitz (1999): Rethinking the Causes of Deforestation: Lessons from Economic Models. *The World Bank Research Observer, 14*(1), pp. 73-98.

* Angrist, J. D. & J. Pischke (2015): *Mastering 'metrics: the path from cause to effect*. Princeton, NJ: Princeton University Press.

* Barbier, E. B. & J. C. Burgess (2001): The Economics of Tropical Deforestation. *Journal of Economic Surveys, 15*(3).

* Blundell, R. & A. Duncan (1998), Kernel Regression in Empirical Microeconomics. *The Journal of Human Resources, 33*(1), p. 62-87. <http://www.jstor.org/stable/146315>. 

* Deininger, K. & B. Minten (1999): Poverty, Policies, and Deforestation: The Case of Mexico. *Economic Development and Cultural Change, 47*(2), pp. 313-344.

* Deininger, K. & B. Minten (2002): Determinants of Deforestation and the Economics of Protection: An Application to Mexico. *Journal of Agricultural Economics 84*(4), pp. 943-960.

* Foster, A. D. & M. R. Rosenzweig (2003): Economic Growth and the Rise of Forests. *The Quarterly Journal of Economics, 118*(2), pp. 601-637. <http://www.jstor.org/stable/25053915>.

* Greene, W. (1999): Marginal effects in the censored regression model. *Economics Letters, 64*(1), pp. 43-49. <http://pages.stern.nyu.edu/~wgreene/censored.doc>.

* Greene, W. H. (2008): *Econometric Analysis*. Edition 6, Upper Saddle River, NJ: Pearson Education.

* Hackl, P. (2013): *Einfhrung in die konometrie*. Edition 2, Munich: Pearson Deutschland.

* Hrdle, W. & O. Linton (1994): Applied nonparametric methods. In: *Handbook of Econometrics*, Edition 4, Chapter 38, Elsevier, pp. 2295-2339.

* Imbens, G. W. & T. Lemieux (2008): Regression discontinuity designs: A guide to practice. *Journal of Econometrics 142*(2), pp. 615-635.

* Jacob, B. A. & L. Lefgren (2004): Remedial Education and Student Achievement: A Regression-Discontinuity Analysis. The Review of Economics and Statistics, 86(1), pp. 226-244.

* Kennedy, P. (2008): *A Guide to Econometrics*. Edition 6, Malden, MA: Blackwell Publishing.

* Newey, W. K. (1987): Efficient Estimation of Limited Dependent Variable Models with Endogenous Explanatory Variables. *Journal of Econometrics 36*(3), pp. 231-250.

* Rhynsburger, D. (1973): Analytic Delineation of Thiessen Polygons. *Geographical Analysis, 5*(2), pp. 133-144.

* Shortle, J. & D. Abler (1999): Agriculture and the Environment. In: Van den Bergh, J. C.J.M., *Handbook of Environmental and Resource Economics*. Chapter 11, Cheltenham: Edward Elgar Publishing, pp. 159-176.

* Skoufias, E. (2005): *PROGRESA and Its Impacts on the Welfare of Rural Households in Mexico*. Washington, D.C.: International Food Policy Research Institute.

* Son, H. H. (2011): *Equity and Well-Being: Measurement and Policy Practice*. Oxon: Routledge. <http://www.adb.org/sites/default/files/publication/29288/equity-well-being.pdf>.

* Wackernagel, M. & W. Rees (1996): *Our ecological footprint: Reducing Human Impact on the Earth*. Gabriola Island, BC: New Society Publishers.

* Wooldridge, J. M. (2002): *Econometric Analysis of Cross Section and Panel Data*. Cambridge, MA: MIT Press.

* Wooldridge, J. M. (2010): *Econometric Analysis of Cross Section and Panel Data*. Edition 2, Cambridge, MA: MIT Press.

* World Bank (1992): *World Development Report 1992: Development and the Environment*. New York: Oxford University Press. https://openknowledge.worldbank.org/handle/10986/5975.

* Zwane, A. P. (2007): Does poverty constrain deforestation? Econometric evidence from Peru. *Journal of Development Economics, 84*(1), pp. 330-349.

## Websites:

* CRAN Reference manual for package 'car': <https://cran.r-project.org/web/packages/car/car.pdf>, retrieved September 22, 2016.

* Program's website: <https://www.prospera.gob.mx/swb/swb/PROSPERA2015/>, retrieved April 29, 2016.

* Stata manual, command ivtobit (2016): <http://www.stata.com/manuals13/rivtobit.pdf>, retrieved September 1, 2016.

* Stata manual, command ivprobit (2016): <http://www.stata.com/manuals13/rivprobit.pdf>, retrieved September 1, 2016.

* Stata manual, command regress (2016): <http://www.stata.com/manuals13/rregress.pdf>, retrieved October 19, 2016

* Stata manual, Chapter 20 (2016): <http://www.stata.com/manuals13/u20.pdf>, retrieved October 19, 2016

* NASA Earth Observatory (2016): Weier, J. & D. Herring (2000): Measuring Vegetation (NDVI & EVI), <http://earthobservatory.nasa.gov/Features/MeasuringVegetation/measuring_vegetation_1.php>, retrieved September 1, 2016.

## R and R Packages:
* 'AER': Kleiber, C. & A. Zeileis (2008): *Applied Econometrics with R*. New York, NY: Springer-Verlag. <http://CRAN.R-project.org/package=AER>.

* 'car': Fox, J. & S. Weisberg (2011): *An R Companion to Applied Regression*. Edition 2, Thousand Oaks, CA: Sage Publications. <http://socserv.socsci.mcmaster.ca/jfox/Books/Companion>.

* 'censReg': Henningsen, A. (2013): *censReg: Censored Regression (Tobit) Models*. R package version 0.5-20. <https://CRAN.R-project.org/package=censReg>.

* 'deldir': Turner, R. (2016): *deldir: Delaunay Triangulation and Dirichlet (Voronoi) Tessellation*. R package version 0.1-12. <https://CRAN.R-project.org/package=deldir>.

* 'dplyr': Wickham, H. & R. Francois (2016): *dplyr: A Grammar of Data Manipulation*. R package version 0.5.0. <https://CRAN.R-project.org/package=dplyr>.

* 'dplyrExtras': Kranz, S. (2016): *dplyrExtras: extra functionality for dplyr like mutate_rows for mutation of a subset of rows*. R package version 0.1.3. <https://github.com/skranz/dplyrExtras>

* 'ggplot2': Wickham, H. (2009): *ggplot2: Elegant Graphics for Data Analysis*. New York, NY: Springer-Verlag.

* 'gridExtra': Auguie, B. (2016): *gridExtra: Miscellaneous Functions for "Grid" Graphics*. R package version 2.2.1. <https://CRAN.R-project.org/package=gridExtra>.

* 'lfe': Gaure, S. (2013): lfe: Linear group fixed effects. *The R Journal, 5*(2), pp.104-117.

* 'lmtest': Zeileis, A. & T. Hothorn (2002): Diagnostic Checking in Regression Relationships. *R News, 2*(3), pp.7-10. <http://CRAN.R-project.org/doc/Rnews/>.

* 'locfit': Loader, C. (2013): *locfit: Local Regression, Likelihood and Density Estimation*. R package
  version 1.5-9.1. <https://CRAN.R-project.org/package=locfit>.
  
* 'np': Hayfield, T. & J. S. Racine (2008): Nonparametric Econometrics: The np Package. *Journal of
Statistical Software 27*(5). <http://www.jstatsoft.org/v27/i05/>.

* R Core Team (2016): *R: A language and environment for statistical computing*. R Foundation for Statistical Computing, Vienna, Austria. <https://www.R-project.org/>.

* 'regtools': Kranz, S. (2016): *regtools: Some tools for regressions and presentation of regressions results*. R package version 0.2. <https://github.com/skranz/regtools>.

* 'reshape2': Wickham, H. (2007): Reshaping Data with the reshape Package. *Journal of Statistical Software, 21*(12), pp.1-20. <http://www.jstatsoft.org/v21/i12/>.

* 'RTutor': Kranz, S. (2015): *RTutor: R problem sets with automatic test of solution and hints*. R package version 2015.12.16. <https://github.com/skranz/RTutor>.

* 'sandwich': Zeileis, A. (2004): Econometric Computing with HC and HAC Covariance Matrix Estimators. *Journal of Statistical Software, 11*(10), pp.1-17. <http://www.jstatsoft.org/v11/i10/>.

* 'stargazer': Hlavac, M. (2015): *stargazer: Well-Formatted Regression and Summary Statistics Tables*. R package version 5.2. <http://CRAN.R-project.org/package=stargazer>.

* 'yaml': Stephens, J. (2014): *yaml: Methods to convert R data to YAML and back*. R package version 2.1.13.
<https://CRAN.R-project.org/package=yaml>.

## Data and Code
* original data sets and available STATA code files (used type of data files: filename extension ".dta"):  
Alix-Garcia, J. & C. McIntosh & K. R. E. Sims & J.R. Welch (2013): *Replication data for: The Ecological Footprint of Poverty Alleviation: Evidence from Mexico's Oportunidades Program*. V2, Harvard Dataverse. <http://hdl.handle.net/1902.1/22101>



## Exercise A -- Appendix
### A 1 Newey two-step estimator
There are several commands in R to estimate Tobit regressions. However, up to date there seems to be no command estimating a Tobit regression with one or more endogenous variables (called "IV Tobit" in the problem set). In theory, there are several estimators for the coefficients of such a model. As I want to reproduce the results from the paper in the problem set, I prefer using an estimator which comes quite close to it but at the same time does not require too much computational time. As explained in the problem set I therefore choose the so-called **Newey two-step estimator**. Apart from the already mentioned advantages, it does not require starting values which also speaks in favor of it and against the maximum-likelihood estimator. As sources for the implementation I mainly used the original paper Newey (1987) where the estimator has been introduced but also the STATA help pages on the ivtobit and ivprobit command.

In order to make the description of my implementation steps clear I will sum up the main points from the original paper by **Newey (1987)** here:  
The underlying model is as follows:
$$
\begin{aligned}
	y_t &= max\left(0, y_t^*\right) \quad \text{with }t=1,...,n & &\qquad \text{(1)} \\\
	\quad y_t^* &= \overrightarrow{\beta_0}^{\prime} \overrightarrow{Y_t} + \overrightarrow{\gamma_0}^{\prime}\overrightarrow{X_{1t}} + u_t \quad \text{with }t=1,...,n  & &\qquad \text{(2)} \\\
	\quad \overrightarrow{Y_t} &= \overrightarrow{\pi_1}^{\prime} \overrightarrow{X_{1t}} + \overrightarrow{\pi_2}^{\prime}\overrightarrow{X_{2t}} + \overrightarrow{V_t} \quad \text{with }t=1,...,n  & &\qquad \text{(3)} 
\end{aligned}
$$
Here, $y_t$ is the dependent variable, $y_t^*$ the latent variable, $\overrightarrow{Y_t}$ a vector of endogenous variables, $\overrightarrow{X_{1t}}$ is a vector of exogenous explanatory variables and $\overrightarrow{X_{2t}}$ a vector of excluded instruments. For the error terms $u_t$ and $\overrightarrow{V_t}$ it is assumed that they are multivariate normally distributed with expected value $0$ and covariance matrix $\Sigma$ conditional on $\overrightarrow{X_{1t}}$ and $\overrightarrow{X_{2t}}$.  
The equation for the latent variable $y_t^*$ can be written in a reduced form:
$$\begin{aligned}y_t^* = \overrightarrow{\alpha_0}^{\prime}\overrightarrow{X_{1t}} +\overrightarrow{\beta_0}^{\prime} \overrightarrow{V_t} + u_t \quad \text{with }t=1,...,n  & &\qquad \text{(4)}
	\label{eq: reducedFormNeweyAlpha}\end{aligned}$$
The coefficients $\alpha$ and $\delta$ are related through the following equation
$$\begin{aligned}\alpha_0 = D\left(\overrightarrow{\pi_0}\right)\delta_0 \label{eq: linkalphadelta} & &\qquad \text{(5)}\end{aligned}$$
where
$$\begin{aligned}D = \left(\overrightarrow{\pi}, I\right) \label{eq: Ddefinition} & &\qquad \text{(6)}\end{aligned}$$
and  
$$I=\left(
\begin{array}
{rrrr}
    1      & 0      & \dots  & 0 \\\
    0      & 1      & \dots  & 0 \\\
    \vdots & \vdots & \ddots & \vdots \\\
    0      & \dots  & \dots  & 1 \\\
    0      & \dots  & \dots  & 0 \\\
    \vdots & \vdots & \ddots & \vdots \\\
    0      & 0      & \dots  & 0
\end{array}\right) \qquad \text{(7)}
$$
The identity matrix in the upper part of $I$ has as many rows (and columns) as there are variables in $X_1$; In the lower part of $I$ there is one row of zeros for each excluded instrument.  
The **idea** of the Newey two-step estimator is to use a two-stage estimator for $\alpha$ and put it into an Amemiya Generalized Least Squares estimator (AGLS estimator). The latter is a special type of minimum-distance estimator which uses a consistent estimator $\widehat{\Omega}$ of the asymptotic covariance matrix of $\sqrt{n}(\widehat{\alpha}-\widehat{D}\delta_0)$ which measures the distance between $\widehat{\alpha}$ and $\widehat{D}\delta$. Note that $n$ is the number of observations here. The resulting Newey two-step estimator is then efficient relative to the two-stage estimator $\widehat{\delta}$ which belongs to the used two-stage estimator $\widehat{\alpha}$.  
The two final formulas used to finally calculate the estimator are 
$$\begin{aligned}
	\widehat{\delta_{Al}} &= \left(\widehat{D}^{\prime}\widehat{\Omega}^{-1}\widehat{D}\right)^{-1}\widehat{D}^{\prime}\widehat{\Omega}^{-1} \widehat{\vec{\alpha}}  & &\qquad \text{(8)} \label{eq: deltaAlHat}\\\
	\widehat{\Omega_l} &= \left(\widehat{J}^{-1}\right)_{\alpha\alpha} + \widehat{\mathcal{V}}^2 \left(X^{\prime}X/n\right)^{-1}  & &\qquad \text{(9)}\label{eq: OmegalHat}
\end{aligned}$$
In the case of the endogenous *Tobit* model the two-stage instrumental variables estimator by Smith and Blundell can be (and - in my implementation - actually is) used as base for the Newey two-step estimator.

*(Cf. Newey (1987))*

The following points describe in detail how I obtain the different input parameters according to the description in Newey (1987). I should remark here that I focus on cases with only one endogenous explanatory variable, so $Y_t$ is a scalar. **My implementation** is analogous to the estimation steps on page 12 in the STATA ivprobit help file - adapted to the IV Tobit framework. *(Cf. STATA ivtobit help site, p.10)*

* $\widehat{D}$: I calculate $\widehat{D}$ according to (6) with $\widehat{\vec{\pi}}$ from an OLS regression of (3).  
I call this regression (I) and the estimates of $\vec{\pi}$ $\widehat{\overrightarrow{\pi_l}}$.
* $\widehat{\vec{\alpha}}$: I use the estimates of $\vec{\alpha}$ from a Tobit regression of $y_t^* = \vec{\alpha}^{\prime} \overrightarrow{X_t} + \lambda \widehat{V_t} + e_t$ with $\widehat{V_t}$ the residuals from the OLS regression (I).  
I call this regression (II) and the estimates of $\vec{\alpha}$ $\widehat{\overrightarrow{\alpha_l}}$ .
* $\left(\widehat{J}^{-1}\right)_{\alpha\alpha}$: I use the covariance matrix of the estimated coefficients $\widehat{\alpha_l}$ from regression (II) directly as $\left(\widehat{J}^{-1}\right)_{\alpha\alpha}$.
* $\widehat{\mathcal{V}}^2 \left(X^{\prime}X/n\right)^{-1}$: To finally obtain this last component I first run a Tobit regression on $y_t^* = \beta Y_t + \vec{\gamma}^{\prime} \overrightarrow{X_{1t}} + \rho \widehat{V_t} + e_t$ which I call (III). I take the estimate of $\beta$ (called $\widehat{\beta_l}$) from (III) and the estimate of $\gamma$ (called $\widehat{\gamma_l}$) from (II) and run an OLS regression of $Y_t \left(\widehat{\lambda_l}-\widehat{\beta_l}\right)$ on $\overrightarrow{X_t}$ which I call (IV). The covariance matrix of the coefficient estimates from this regression gives $\widehat{\mathcal{V}}^2 \left(X^{\prime}X/n\right)^{-1}$.

In the commented code the different steps can be identified by the names and regression numbers defined above.

Finally, Newey (1987) provides a formula for the according asymptotic covariance matrix which is used in the implementation as well:
$$\begin{aligned}\left(\widehat{D}^{\prime} \widehat{\Omega_l}^{-1} \widehat{D}\right)^{-1}  & &\qquad \text{(10)}\end{aligned}$$

### A 2 Marginal Effects in IV Tobit

#### A 2.1 The model
Consider again the equations defining the IV Tobit model from above but this time only with one endogenous variable ($V$ a scalar) because this is the case considered in my implementation:
$$\begin{aligned}
	y &= max\left(0, y^*\right) & &\qquad \text{(11)}\label{eq: ytobit}\\\
	y^* &= \beta_0^{\prime}Y_t + \overrightarrow{\gamma_0}^{\prime}\overrightarrow{X_{1}} + u & &\qquad \text{(12)} \label{eq: ystartobit}\\\
	Y &= \overrightarrow{\pi_1}^{\prime} \overrightarrow{X_{1}} + \overrightarrow{\pi_2}^{\prime}\overrightarrow{X_{2}} + V & &\qquad \text{(13)} \label{eq: Ytobit}
\end{aligned}$$
For the procedure by Smith and Blundell it is assumed that $u$ and $V$ are bivariate normally distributed with zero mean and variance-covariance matrix $\Sigma$ independent of $\overrightarrow{X_{1}}$ and $\overrightarrow{X_{2}}$. So it holds that those error terms are related in the following way:
$$\begin{aligned}
	u &= \rho V + \epsilon \quad \text{with} \quad \epsilon \sim \mathcal{N}\left(0,\tau_1^2\right) & &\qquad \text{(14)}	\label{eq: errorterms}\\\
	\rho &= \frac{Cov(u,V)}{\tau_2^2} & &\qquad \text{(15)}
\end{aligned}$$
with $Var(u)=\sigma_u^2$, $Var(V)=\tau_2^2$. *Note that this assumption includes the case from section A 2, so of $(u,V)$ being multivariate normally distributed conditional on* $\overrightarrow{X_{1}}$ *and* $\overrightarrow{X_{2}}$ *which is used in Newey (1987).*  
Assume that $V$ and $\epsilon$ are independent which means that $\epsilon$ is also independent of $\overrightarrow{X_{1}}$ and $\overrightarrow{X_{2}}$.  
The explanatory variable $Y$ is endogenous if it is correlated with the error term $u$, which is the case if the error terms $u$ and $V$ are correlated. Using (14) one can rewrite (12) as follows:
$$\begin{aligned}
	y^* &= \beta_0^{\prime} Y + \overrightarrow{\gamma_0}^{\prime}\overrightarrow{X_1} + \rho V + \epsilon \quad \text{with} \quad \epsilon \left.\right| \overrightarrow{X_1},\overrightarrow{X_2},V \sim \mathcal{N}\left(0,\tau_1^2\right) & &\qquad \text{(16)}\label{eq: ystartobitexpanded}
\end{aligned}$$
This model as a whole shall be used in the following considerations.  
*Remark:* This model (and thus also this whole chapter on marginal effects) is originally dedicated to *continuous* endogenous variables. I use it here - even though I apply the results to a binary endogenous variable - to come close to the results Alix-Garcia et al. get by using the STATA command *ivtobit* (and the according command for the marginal effects) which is based on the same model.  
The following **procedure** by Smith and Blundell estimates $\beta_0$ and $\overrightarrow{\gamma_0}$ consistently: First, equation (13) is estimated by OLS. The residuals $\widehat{V}$ are then used in the second step where a Tobit model with the latent variable $y^*$ given by equation (16) is estimated. For the calculation of the marginal effects further estimates are taken from this procedure: the standard error $\widehat{\tau_2}$ of $V$ can be obtained from the first step; $\widehat{\rho}$ and the standard error $\widehat{\tau_1}$ of $\epsilon$ can be obtained from the second step.  
*(Cf. Wooldridge (2002, section 16.6.2))*
<br>
<br>
#### A 2.2 Marginal Effects on P(y>0)
Considering $P(y>0|\overrightarrow{X_1}, Y)$ with $y$, $\overrightarrow{X_1}$ and $Y$ from the Tobit model in (11)-(13) is equivalent to considering $E[z|\overrightarrow{X_1}, Y]$ with 
$$\begin{aligned}
	z&=I\{y^*>0\} & &\qquad \text{(17)}
	\label{eq: zprobit}
\end{aligned}$$
where $I$ denotes the indicator function.  
The model described in section A 2.1 with (17) substituted for (11) forms a **probit model** with endogenous variable $Y$ as used by Rivers and Vuong. *(Cf. Wooldridge (2002, Chapter 15.7.2))*
#< info "Remark on suitability of the model"
The same comment on the suitability of this model as in section A 2.1 holds here: $Y\left.\right|\overrightarrow{X_1},\overrightarrow{X_2}$ should not be a discrete variable, but have properties of a normal random variable. *(Cf. Wooldridge (2002, p. 472))*
#>
This way, one can just use the results from Wooldridge *(2002, Chapter 15.7.2)* in order to get a formula for $P(y>0|\overrightarrow{x_1},Y)$. In the following, the main steps shall be rewritten in the notation from above:
$$\begin{aligned}
	P(y>0\left.\right|\overrightarrow{x_1}, Y) &= E[z\left.\right|\overrightarrow{x_1},Y]\\\
	&= E_u[E[z|\overrightarrow{x_1},Y,u]]\\\
	&= E_v\left[ \Phi\left( \frac{\vec{\gamma}'\overrightarrow{x_1} + \beta Y + \theta v}{\tau_1} \right) \right]\\\
	&= \Phi\left( \frac{\vec{\gamma}'\overrightarrow{x_1} + \beta Y}{\sqrt{\tau_1^2 + \theta^2\tau_2^2}} \right)  & &\qquad \text{(18)}
	\label{eq: margP}
\end{aligned}$$
Here, $\Phi$ denotes the normal distribution function.  
The **marginal effects** of an explanatory variable $m$ on $P(y>0 \left. \right| \overrightarrow{x_1},Y)$ can be calculated using differences of this formula with $m=1$ and $m=0$ holding all other variables constant if $m$ is a binary variable, or using the partial derivative of this formula with respect to $m$ at some constant value of all variables. A suitable choice for constant values could be the variables' means.
<br>
<br>
#### A 2.3 Formula used for derivation of $E[y|y>0,\vec{x_1},Y]$
For the derivation of the formula for the marginal effect on $E[y|y>0, \overrightarrow{x_1},Y]$ in the model above, the following identity is used: 
$$\begin{aligned}
	E_q[E[z\left.\right|z>0,\vec{x}^0,q]] &= E_w[E[z\left.\right|z>0,\vec{x}^0,\vec{w}]] & &\qquad \text{(19)}
	\label{eq: EwEqpos}
\end{aligned}$$
Note that $\vec{x}^0$ represents fixed values of $\vec{x}$ here. This identity can be derived in analogy to Wooldridge *(2002, section 2.2.5)*: It is based on the following **assumptions:**
$$\begin{aligned}
	&\text{[A]} \quad D\left(q\left.\right|\vec{x},\vec{w}\right) = D\left(q\left.\right|\vec{w}\right) \text{, with } D(.|.) \text{ conditional distribution}\notag	\label{eq: ass_a}\\\
	&\text{[B]} \quad E[z\left.\right|z>0,\vec{x},q,\vec{w}] = E[z\left.\right|z>0,\vec{x},q]\notag\\\
	&\text{[C]} \quad \text{$q$ is a continuous random variable}\notag	\label{eq: ass_c} 
\end{aligned}$$
Here, $D(.|.)$ stands for the conditional distribution,  $q$ is an unobservable random variable, $\vec{x}$ and $\vec{w}$ are observable explanatory variables and $z$ is the response variable. These assumptions are needed to prove (19).  
*Proof:*
$$\begin{aligned}
	E[z\left.\right|z>0,\vec{x}, \vec{w}] &\overset{\text{lie}}{=} E_q[E[z\left.\right|z>0,\vec{x}, \vec{w}, q]\left.\right|\vec{x}, \vec{w}]\\\
	&\overset{\text{[B]}}{=} E_q[E[z\left.\right|z>0,\vec{x},q]\left.\right|\vec{x}, \vec{w}]\\\
	&\overset{\text{[A]}}{=} E_q[E[z\left.\right|z>0, \vec{x}, q]\left.\right|\vec{w}] & &\qquad \text{(20)} \label{eq: proofpos1}\\\
	E_w[E[z\left.\right|z>0,\vec{x}^0,\vec{w}]] &\overset{\text{(20)}}{=} E_w[E_q[E[z\left.\right|z>0,\vec{x}^0,q]\left.\right|\vec{w}]]\\\
	&\overset{\text{lie}}{=}E_q[E[z\left.\right|z>0,\vec{x}^0,q]]
\end{aligned}$$
The comment *lie* stands for the "law of iterated expectations":	$E[y] = E_x[E[y\left.\right|x]]$ with random variables $x$ and $y$.
<br>
<br>
#### A 2.4 Marginal Effects on $E[y\left.\right|y>0,\vec{X_1},Y]$
In order to derive the conditional expectation of $y$ in (11) given this response is positive, one can use (19). To do so, the variables from section A 2.3 can be identified with the variables in the Tobit model in the following way: $q$ is identified with $u$, $\vec{x}$ with $(\overrightarrow{X_1},Y)$ and $\vec{w}$ with $V$. In this case, $z$ is just the response variable $y$ in (11). The assumptions [A], [B] and [C], which are needed to be able to use (19), take on the following form:
$$\begin{aligned}
	&\text{[A']} \quad D\left(u\left.\right|\overrightarrow{X_1},Y,V\right) = D\left(u\left.\right|V\right) \text{, with } D(.|.) \text{ conditional distribution}\\\
	&\text{[B']} \quad E[y\left.\right|y>0,\overrightarrow{X_1},Y,u,V] = E[y\left.\right|y>0,\overrightarrow{X_1},Y,u]\\\
	&\text{[C']} \quad \text{$u$ is a continuous random variable}
\end{aligned}$$
Condition [A'] is fulfilled because $u\left.\right|\overrightarrow{X_1},Y,V$ is normally distributed with zero mean and variance independent of $\overrightarrow{X_1}$ and $Y$. So the distribution of $u$ given only $V$ is just the same. Condition [B'] is fulfilled because $z$ is determined by $(\overrightarrow{X_1},Y,u)$ such that $V$ is redundant in the expectation value in condition [B']. Condition [C'] is self-explaining.  
The conditional expectation of the response variable $y$ in the Tobit model with endogenous variable $Y$ can be calculated using the following formula:
$$\begin{aligned}
	E[y\left.\right|y>0, \overrightarrow{X_1},Y] &= E_V\left[\vec{\gamma_0}'\overrightarrow{X_1} + \beta_0 Y + \rho V + \tau_1 \frac{\phi\left(\left(\vec{\gamma_0}'\overrightarrow{X_1} + \beta_0 Y + \rho V\right)/\tau_1\right)}{\Phi\left(\left(\vec{\gamma_0}'\overrightarrow{X_1} + \beta Y + \rho V\right)/\tau_1\right)}\right] & &\qquad \text{(21)}
	\label{eq: margE}
\end{aligned}$$
*Proof:*
$$\begin{aligned}
	E[y\left.\right|y>0, \overrightarrow{X_1},Y] &\overset{\text{lie}}{=} E_u[E[y\left.\right|y>0,\overrightarrow{X_1},Y, u]]\\\
	&\overset{\text{(19)}}{=} E_V[E[y\left.\right|y>0,\overrightarrow{X_1}, Y,V]]\\\
	&\overset{\text{(16)}}{=} E_V\left[E\left[\vec{\gamma_0}'\overrightarrow{X_1} + \beta_0 Y + \rho V + \epsilon \left.\right| \frac{\epsilon}{\tau_1}>-\frac{\vec{\gamma_0}'\overrightarrow{X_1} + \beta_0 Y + \rho V}{\tau_1},\overrightarrow{X_1}, Y,V\right]\right]\\\
	&\overset{\text{(16)}}{=} E_V\left[\vec{\gamma_0}'\overrightarrow{X_1} + \beta_0 Y + \rho V + \tau_1 E\left[\frac{\epsilon}{\tau_1} \left.\right| \frac{\epsilon}{\tau_1}>-\frac{\vec{\gamma_0}'\overrightarrow{X_1} + \beta_0 Y + \rho V}{\tau_1},\overrightarrow{X_1}, Y,V\right]\right]\\\
	&\overset{\text{**}}{=} E_V\left[\vec{\gamma_0}'\overrightarrow{X_1} + \beta_0 Y + \rho V + \tau_1 \frac{\phi\left(\left(\vec{\gamma_0}'\overrightarrow{X_1} + \beta_0 Y + \rho V\right)/\tau_1\right)}{\Phi\left(\left(\vec{\gamma_0}'\overrightarrow{X_1} + \beta_0 Y + \rho V\right)/\tau_1\right)}\right]
\end{aligned}$$
Here, $\phi$ denotes the normal distribution function and $\Phi$ the normal density function.  
** *As* $(\epsilon/\tau_1)\left.\right|\overrightarrow{X_1},Y,V$ *is standard normally distributed the following identity for a standard normally distributed random variable* $r$ *and some constant* $k$ *is used (Wooldridge (2002), p.521):*
$$\begin{aligned}
	E[r|r>k] &= \frac{\phi(k)}{1-\Phi(k)} & &\qquad \text{(22)}
\end{aligned}$$
It is not necessary to compute this expectation further analytically. Given estimates $\widehat{\overrightarrow{\gamma_0}}$, $\widehat{\beta_0}$, $\widehat{\tau_1}$, $\widehat{\rho}$ and $\hat{V}$ one can use the empirical mean over all estimated error terms $\hat{V}$ as a consistent estimator for the expectation with respect to $\hat{V}$.
The **marginal effects** of an explanatory variable $m$ on $E[y|y>0, \overrightarrow{X_1},Y]$ can be calculated using differences of this formula with $m=1$ and $m=0$ holding all other variables constant if $m$ is a binary variable, or using the partial derivative of this formula with respect to $m$ at some constant value of all variables. A suitable choice for constant values could be the variables' means.  
The same note on discrete variables as in section A 2.2 holds.  
*This subsection was written in analogy to Wooldridge (2002, Chapter 16.6.2 and Chapter 15.7.2).*  

In the second edition Wooldridge provides another possibility to estimate marginal effects in the IV Tobit model by showing how one can estimate the standard error $\sigma_u$ of the error term $u_t$ in equation (12). *(Cf. Wooldridge (2010, Chapter 17.5.2))*

### A 3 Some remarks on the implementation
I decided to implement the Newey two-step estimator to replicate (or at least come close to) the results from the STATA ivtobit estimations in the paper. The reasons for this were already explained in the according *'info' box* "Newey\_SB\_ivtobit - Code and Introduction into Newey two-step estimator and Smith-Blundell procedure". However, as a base for the marginal effects I use the Smith-Blundell procedure. In the following I will explain why.

My implementation of the Newey two-step estimator only returns an estimate of the coefficients $\beta_0$ and $\overrightarrow{\gamma_0}$ in equation (2) and the corresponding variance-covariance matrix. For the calculation of the marginal effects according to the aforementioned formulas, the standard errors $\widehat{\tau_1}$ and $\widehat{\tau_2}$ are needed. As I calculate the standard errors of the marginal effects through the delta method, I further need the variance-covariance matrix of all used estimates. The procedure by Smith and Blundell provides estimates of all needed parameters and a variance-covariance matrix of all estimates except $\widehat{\tau_2}$ (which is the sole estimate obtained from the first step). Further, I assume that the standard error of the variance $\widehat{\tau_2^2}$ of the residuals $\hat{V}$ of the first step OLS regression as well as the covariance between $\widehat{\tau_2^2}$ and the other parameters are negligibly small. This variance-covariance matrix is not exact due to this assumption and due to the fact that it is not taken into account that the residuals $\hat{V}$ instead of $V$ are used in the second step. *(Cf. Wooldridge (2002, p.532))* The alternative would have been to combine the Newey estimates and their variance-covariance matrix with estimates for $\tau_1$ and $\tau_2$ from the Smith-Blundell procedure. However, this would lead to the problem that one does not only have to assume values of the covariances between $\widehat{\tau_2^2}$ and the other parameters, but also values of the covariances between $\widehat{\tau_1}$ and the other parameters. A comparison of the results of the marginal effects in the paper to the results using my implementation supports my decision.

One might wonder why I use the Newey two-step estimator to estimate the coefficients if I cannot use them for calculating the marginal effects later. This is because Newey (1987) has shown that the Newey two-step estimator is "asymptotically efficient" relative to the estimator from the procedure by Smith and Blundell. *(Newey (1987, p.243))* Besides, the implementations of the two estimators complement each other as the estimation procedure to obtain the Newey two-step estimator contains the two regressions of the procedure by Smith and Blundell.
